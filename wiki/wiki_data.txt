 == Supervised learningEdit == AODE Artificial neural network Backpropagation Autoencoders Hopfield networks Boltzmann machines Restricted Boltzmann Machines Spiking neural networks Bayesian statistics Bayesian network Bayesian knowledge base Case-based reasoning Gaussian process regression Gene expression programming Group method of data handling (GMDH) Inductive logic programming Instance-based learning Lazy learning Learning Automata Learning Vector Quantization Logistic Model Tree Minimum message length (decision trees, decision graphs, etc.) Nearest Neighbor Algorithm Analogical modeling Probably approximately correct learning (PAC) learning Ripple down rules, a knowledge acquisition methodology Symbolic machine learning algorithms Support vector machines Random Forests Ensembles of classifiers Bootstrap aggregating (bagging) Boosting (meta-algorithm) Ordinal classification Information fuzzy networks (IFN) Conditional Random Field ANOVA Linear classifiers Fisher's linear discriminant Logistic regression Multinomial logistic regression Naive Bayes classifier Perceptron Support vector machines Quadratic classifiers k-nearest neighbor Boosting Decision trees C4.5 Random forests ID3 CART SLIQ SPRINT Bayesian networks Naive Bayes Hidden Markov models == Unsupervised learningEdit == Expectation-maximization algorithm Vector Quantization Generative topographic map Information bottleneck method === Artificial neural networkEdit === Self-organizing map === Association rule learningEdit === Apriori algorithm Eclat algorithm FP-growth algorithm === Hierarchical clusteringEdit === Single-linkage clustering Conceptual clustering === Cluster analysisEdit === K-means algorithm Fuzzy clustering DBSCAN OPTICS algorithm === Outlier DetectionEdit === Local Outlier Factor == Semi-supervised learningEdit == == Reinforcement learningEdit == Temporal difference learning Q-learning Learning Automata SARSA == Deep learningEdit == Deep belief networks Deep Boltzmann machines Deep Convolutional neural networks Deep Recurrent neural networks Hierarchical temporal memory == OthersEdit == Data Pre-processing List of artificial intelligence projects 
The Journal of Machine Learning Research (usually abbreviated JMLR), is a scientific journal focusing on machine learning, a subfield of artificial intelligence. It was founded in 2000. The journal was founded as an open-access alternative to the journal Machine Learning. In 2001, forty editors of Machine Learning resigned in order to support JMLR, saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet. Print editions of JMLR were published by MIT Press until 2004, and by Microtome Publishing thereafter. Since Summer 2007 JMLR is also publishing Machine Learning Open Source Software . == Notes == == References == "Top journals in computer science". Times Higher Education. 14 May 2009. Retrieved 22 August 2009. == External links == Journal of Machine Learning Research website 
Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, is a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC. In June 2013 the private equity firm Peterson Partners acquired Angoss for $8.4 million. == Software == KnowledgeREADER is an integrated customer intelligence product combining visual text discovery and predictive analytics for customer experience management. KnowledgeSEEKER is a data mining product. Its features include data profiling, data visualization and decision tree analysis. It was first released in 1990. KnowledgeSTUDIO is a data mining and predictive analytics suite for the model development and deployment cycle. Its features include data profiling, data visualization, decision tree analysis, predictive modeling, implementation, scoring, validation, monitoring and scorecard development. KnowledgeEXCELERATOR is a visual data discovery software and prediction tool for business analysts and knowledge workers. StrategyBUILDER is an add-on module for KnowledgeSEEKER and KnowledgeSTUDIO and is a product to design, verify, and deploy predictive and business rules. == Services == FundGUARD is software as a service for marketing, sales targeting and predictive leads for mutual funds and wealth management companies. ClaimGUARD is a fraud and abuse detection service. Cloud on demand Software is offered for KnowledgeSEEKER, KnowledgeSTUDIO and its text analytics module. KnowledgeSCORE for Salesforce.com customer relationship management is a forecasting and predictive sales analytics system for Salesforce users. == See also == List of statistical packages Predictive analytics == See also == FICO == References == == External links == Official website 
In data mining, anomaly detection (or outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions. In particular in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns. Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then testing the likelihood of a test instance to be generated by the learnt model. == Applications == Anomaly detection is applicable in a variety of domains, such as intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting Eco-system disturbances. It is often used in preprocessing to remove anomalous data from the dataset. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy. == Popular techniques == Several anomaly detection techniques have been proposed in literature. Some of the popular techniques are: Density-based techniques (k-nearest neighbor, local outlier factor, and many more variations of this concept). Subspace- and correlation-based outlier detection for high-dimensional data. One class support vector machines. Replicator neural networks. Cluster analysis-based outlier detection. Deviations from association rules and frequent itemsets. Fuzzy logic based outlier detection. Ensemble techniques, using feature bagging, score normalization and different sources of diversity. == Application to data security == Anomaly detection was proposed for Intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations. The counterpart of anomaly detection in intrusion detection is misuse detection. == Software == ELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them. == See also == Outliers in statistics Change detection Novelty detection == References == 
Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily in the areas of collaborative filtering, clustering and classification. Many of the implementations use the Apache Hadoop platform. Mahout also provides Java libraries for common maths operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; the number of implemented algorithms has grown quickly, but various algorithms are still missing. While Mahout's core algorithms for clustering, classification and batch based collaborative filtering are implemented on top of Apache Hadoop using the map/reduce paradigm, it does not restrict contributions to Hadoop-based implementations. Contributions that run on a single node or on a non-Hadoop cluster are also welcomed. For example, the 'Taste' collaborative-filtering recommender component of Mahout was originally a separate project and can run stand-alone without Hadoop. Integration with initiatives such as the Pregel-like Giraph are actively under discussion. == References == == External links == Official website Giraph - a Graph processing infrastructure that runs on Hadoop (see Pregel). Pregel - Google's internal graph processing platform, released details in ACM paper. Mahout Mailing List Archives - Mahout Mailing List Archives === Resources === A Spring based Java demo application that demonstrates a simple recommender using Apache Mahout Demo of travel recommendations using anonymous user-based recommender of Mahout 
Apache Spark is an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's multi-stage in-memory primitives provides performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms. Spark requires a cluster manager and a distributed storage system. For cluster management, Spark supports standalone (native Spark cluster), Hadoop YARN, or Apache Mesos. For distributed storage, Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS), Cassandra, OpenStack Swift, Amazon S3, Kudu, or a custom solution can be implemented. Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes, where distributed storage is not required and the local file system can be used instead; in such a scenario, Spark is run on a single machine with one executor per CPU core. Spark had in excess of 1000 contributors in 2015, making it not only the most active project in the Apache Software Foundation but one of the most active open source big data projects. == History == Spark was initially started by Matei Zaharia at UC Berkeley AMPLab in 2009, and open sourced in 2010 under a BSD license. In 2013, the project was donated to the Apache Software Foundation and switched its license to Apache 2.0. In February 2014, Spark became a Top-Level Apache Project. In November 2014, the engineering team at Databricks used Spark and set a new world record in large scale sorting. == Project components == The Spark project consists of multiple components. === Spark Core and Resilient Distributed Datasets === Spark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic I/O functionalities. The fundamental programming abstraction is called Resilient Distributed Datasets (RDDs), a logical collection of data partitioned across machines. RDDs can be created by referencing datasets in external storage systems, or by applying coarse-grained transformations (e.g. map, filter, reduce, join) on existing RDDs. An RDD is a collection of items distributed across many nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections. An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes. Users can create RDDs in two ways: by loading an external dataset, or by distributing a collection of objects in their driver program. RDDs offer two types of operations after creation: transformations and actions. Transformations construct a new RDD. Actions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system. The RDD abstraction is exposed through a language-integrated API in Java, Python, Scala, and R similar to local, in-process collections. This simplifies programming complexity because the way applications manipulate RDDs is similar to manipulating local collections of data. A Spark cluster is composed of one Driver JVM and one or many Executor JVMs. === Spark SQL === Spark SQL is a component on top of Spark Core that introduces a new data abstraction called DataFrames, which provides support for structured and semi-structured data. Spark SQL provides a domain-specific language to manipulate DataFrames in Scala, Java, or Python. It also provides SQL language support, with command-line interfaces and ODBC/JDBC server. Prior to version 1.3 of Spark, DataFrames were referred to as SchemaRDDs. === Spark Streaming === Spark Streaming leverages Spark Core's fast scheduling capability to perform streaming analytics. It ingests data in mini-batches and performs RDD transformations on those mini-batches of data. This design enables the same set of application code written for batch analytics to be used in streaming analytics, on a single engine. === MLlib Machine Learning Library === Spark MLlib is a distributed machine learning framework on top of Spark Core that, due in large part of the distributed memory-based Spark architecture, is as much as nine times as fast as the disk-based implementation used by Apache Mahout (according to benchmarks done by the MLlib developers against the Alternating Least Squares (ALS) implementations, and before Mahout itself gained a Spark interface), and scales better than Vowpal Wabbit. Many common machine learning and statistical algorithms have been implemented and are shipped with MLlib which simplifies large scale machine learning pipelines, including: summary statistics, correlations, stratified sampling, hypothesis testing, random data generation classification and regression: support vector machines, logistic regression, linear regression, decision trees, naive Bayes classification collaborative filtering techniques including alternating least squares (ALS) cluster analysis methods including k-means, and Latent Dirichlet Allocation (LDA) dimensionality reduction techniques such as singular value decomposition (SVD), and principal component analysis (PCA) feature extraction and transformation functions optimization primitives such as stochastic gradient descent, limited-memory BFGS (L-BFGS) === GraphX === GraphX is a distributed graph processing framework on top of Spark. It provides an API for expressing graph computation that can model the Pregel abstraction. It also provides an optimized runtime for this abstraction. Like Spark, GraphX initially started as a research project at UC Berkeley's AMPLab and Databricks, and was later donated to the Apache Software Foundation and the Spark project. == Features == APIs and libraries for Java, Scala, Python, R, and other languages. Scalability to over 8000 nodes in production. Ability to cache datasets in memory for interactive data analysis: extract a working set, cache it, query it repeatedly. Interactive command line interface (in Scala or Python) for low-latency, horizontally scalable, data exploration. Higher level library for stream processing, using Spark Streaming. Support for structured and relational query processing (SQL), through Spark SQL. Higher level libraries for machine learning and graph processing. == References == == External links == Official website Spark SQL Spark Streaming MLlib machine learning library GraphX graph processing library 
Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. The third edition of the book was released 11 December 2009. It is used in over 1100 universities worldwide and has been called "the most popular artificial intelligence textbook in the world". The book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography. == Chapters == Artificial Intelligence: A Modern Approach is divided into seven parts with a total of 27 chapters. The authors state that it is a large text which would take two semesters to cover all the chapters and projects. Part I: Artificial Intelligence - Sets the stage for the following sections by viewing AI systems as intelligent agents that can decide what actions to take and when to take them. Part II: Problem Solving - Focuses on methods for deciding what action to take when needing to think several steps ahead such as playing a game of chess. Part III: Knowledge and Reasoning - Discusses ways to represent knowledge about the intelligent agents' environment and how to reason logically with that knowledge. Part IV: Uncertain Knowledge and Reasoning - This section is analogous to Parts III, but deals with reasoning and decision-making in the presence of uncertainty in the environment. Part V: Learning - Describes ways for generating knowledge required by the decision-making components and introduces a new component: the artificial neural network Part VI: Communicating, Perceiving and Acting - Concentrates on ways an intelligent agent can perceive its environment whether by touch or vision. Part VII: Conclusions - Considers the past and future of AI by discussing what AI really is and why it has succeeded to some degree. Also the views of those philosophers who believe that AI can never succeed are given discussion. == Code == Programs in the book are presented in pseudo code with implementations in Java, Python, and Lisp available online. There are also unsupported implementations in Prolog, C++, and C#. == References == == External links == Artificial Intelligence: A Modern Approach, AIMA 3rd edition website 
In genetic algorithms, a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve. The set of all solutions is known as the population. The chromosome is often represented as a binary string, although a wide variety of other data structures are also used. == Chromosome design == The design of the chromosome and its parameters is by necessity specific to the problem to be solved. Traditionally, chromosomes are represented in binary as strings of 0s and 1s, however other encodings are also possible; almost any representation which allows the solution to be represented as a finite-length string can be used. Finding a suitable representation of the problem domain for a chromosome is an important consideration, as a good representation will make the search easier by limiting the search space; similarly, a poorer representation will allow a larger search space. The mutation operator and crossover operator employed by the genetic algorithm must also take into account the chromosome's design. === Example 1: binary representation === Suppose the problem is to find the integer value of between 0 and 255 that provides the maximal result for . The possible solutions for this problem are the integers from 0 to 255, which can all be represented as 8-digit binary strings. Thus, we might use an 8-digit binary string as our chromosome. If a given chromosome in the population represents the value 155, its chromosome would be 10011011. Note that this is not the type of problem that is normally solved by a genetic algorithm, since it can be trivially solved using numeric methods; it is only used to serve as a simple example. === Example 2: string representation === A more realistic problem we might wish to solve is the travelling salesman problem. In this problem, we seek an ordered list of cities that results in the shortest trip for the salesman to travel. Suppose there are six cities, which we'll call A, B, C, D, E, and F. A good design for our chromosome might be the ordered list we want to try. An example chromosome we might encounter in the population might be DFABEC. == Selection, crossover and mutation == In each generation of the genetic algorithm, two parent chromosomes are selected based on their fitness values; these chromosomes are used by the mutation and crossover operators to produce two offspring chromosomes for the new population. == References == 
Communications of the ACM is the monthly magazine of the Association for Computing Machinery (ACM). It was established in 1957, with Saul Rosen its first managing editor. It is sent to all ACM members. Articles are intended for readers with backgrounds in all areas of computer science and information systems. The focus is on the practical implications of advances in information technology and associated management issues; ACM also publishes a variety of more theoretical journals. The magazine straddles the boundary of a science magazine, trade magazine, and a scientific journal. While the content is subject to peer review, the articles published are often summaries of research that may also be published elsewhere. Material published must be accessible and relevant to a broad readership. From 1960 onward, CACM also published algorithms, expressed in ALGOL. The collection of algorithms later became known as the Collected Algorithms of the ACM. == Notable articles == Some notable articles are: The issue of what to call the then-fledgling field of computer science was raised by the editors of DATA-LINK in a letter to the editor in 1958. They called for giving the field a name "which is brief, definite, distinctive". The call was echoed by a wide range of suggestions, including comptology (Quentin Correll), hypology (P.A. Zaphyr), and datalogy (Peter Naur). C. A. R. Hoare's Quicksort. Martin Davis, George Logemann, and Donald Loveland described in 1962 the DPLL algorithm, containing the essential algorithm on which most modern SAT solvers are based. The "Revised report on the algorithm language ALGOL 60": A landmark paper in programming language design describing the results of the international ALGOL committee. Kristen Nygaard and Ole-Johan Dahl's original paper on Simula-67. Edsger W. Dijkstra's famous letter inveighing against the use of GOTO. The letter was reprinted in Jan 2008 in the 50th anniversary edition. Dijkstra's original paper on the THE operating system. This paper's appendix, arguably even more influential than its main body, introduced semaphore-based synchronization. Ronald L. Rivest, Adi Shamir, and Leonard M. Adleman's first public-key cryptosystem (RSA). == See also == Journal of the ACM == References == == External links == Official website ISSN 0001-0782 
In machine learning, a convolutional neural network (CNN, or ConvNet) is a type of feed-forward artificial neural network in which the connectivity pattern between its neurons is inspired by the organization of the animal visual cortex, whose individual neurons are arranged in such a way that they respond to overlapping regions tiling the visual field. Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons designed to use minimal amounts of preprocessing. They have wide applications in image and video recognition, recommender systems and natural language processing. == Overview == When used for image recognition, convolutional neural networks (CNNs) consist of multiple layers of small neuron collections which process portions of the input image, called receptive fields. The outputs of these collections are then tiled so that they overlap, to obtain a better representation of the original image; this is repeated for every such layer. Tiling allows CNNs to tolerate translation of the input image. Convolutional networks may include local or global pooling layers, which combine the outputs of neuron clusters. They also consist of various combinations of convolutional and fully connected layers, with pointwise nonlinearity applied at the end of or after each layer. To reduce the number of free parameters and improve generalisation, a convolution operation on small regions of input is introduced. One major advantage of convolutional networks is the use of shared weight in convolutional layers, which means that the same filter (weights bank) is used for each pixel in the layer; this both reduces memory footprint and improves performance. Some time delay neural networks also use a very similar architecture to convolutional neural networks, especially those for image recognition and/or classification tasks, since the tiling of neuron outputs can be done in timed stages, in a manner useful for analysis of images. Compared to other image classification algorithms, convolutional neural networks use relatively little pre-processing. This means that the network is responsible for learning the filters that in traditional algorithms were hand-engineered. The lack of dependence on prior knowledge and human effort in designing features is a major advantage for CNNs. == History == The design of convolutional neural networks follows the discovery of visual mechanisms in living organisms. Early 1968 work showed that the animal visual cortex contains complex arrangements of cells, responsible for detecting light in small, overlapping sub-regions of the visual field, called receptive fields. The paper identified two basic cell types: simple cells, which respond maximally to specific edge-like patterns within their receptive field, and complex cells, which have larger receptive fields and are locally invariant to the exact position of the pattern. These cells act as local filters over the input space. The neocognitron, a predecessor to convolutional networks, was introduced in a 1980 paper. In 1988 they were separately developed, with explicit parallel and trainable convolutions for temporal signals. Their design was improved in 1998, generalized in 2003, and simplified in the same year. The famous LeNet-5 network can classify digits successfully and is applied to recognising hand-written check (cheque) numbers. However, given more complex problems the breadth and depth of the network increases and becomes limited by computing resources thus hindering performance. A different CNN design was proposed in 1988 for application to decomposition of one-dimensional electromyography signals. This design was modified in 1989 to other convolution-based designs. With the rise of efficient GPU computing, it has become possible to train larger networks. In 2006, several publications described more efficient ways to train convolutional neural networks with more layers. In 2011, they were refined and implemented on a GPU, with impressive results. In 2012, Ciresan et al. significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters), the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images), and the ImageNet dataset. == Distinguishing features == While traditional multilayer perceptron models were successfully used for image recognition, due to the full connectivity between nodes they suffer from the curse of dimensionality and thus do not scale well to higher resolution images. For example, in CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular Neural Network would have 32*32*3 = 3072 weights. A 200x200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights. Such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart and close together on exactly the same footing. Clearly, the full connectivity of neurons is wasteful in the framework of image recognition, and the huge number of parameters quickly leads to overfitting. Convolutional neural networks are biologically inspired variants of multilayer perceptrons, designed to emulate the behaviour of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNN have the following distinguishing features: 3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. The neurons inside a layer are only connected to a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture. Local connectivity: following the concept of receptive fields, CNNs exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learnt "filters" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to non-linear "filters" that become increasingly "global" (i.e. responsive to a larger region of pixel space). This allows the network to first create good representations of small parts of the input, then assemble representations of larger areas from them. Shared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer detect exactly the same feature. Replicating units in this way allows for features to be detected regardless of their position in the visual field, thus constituting the property of translation invariance. Together, these properties allow convolutional neural networks to achieve better generalization on vision problems. Weight sharing also helps by dramatically reducing the number of free parameters being learnt, thus lowering the memory requirements for running the network. Decreasing the memory footprint allows the training of larger, more powerful networks. == Building blocks == A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. We discuss them further below: === Convolutional layer === The Convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when they see some specific type of feature at some spatial position in the input. Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map. ==== Local connectivity ==== When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume. The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern. ==== Spatial arrangement ==== Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and zero-padding. Depth of the output volume controls the number of neurons in the layer that connect to the same region of the input volume. All of these neurons will learn to activate for different features in the input. For example, if the first Convolutional Layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color. Stride controls how depth columns around the spatial dimensions (width and height) are allocated. When the stride is 1, a new depth column of neurons is allocated to spatial positions only 1 spatial unit apart. This leads to heavily overlapping receptive fields between the columns, and also to large output volumes. Conversely, if higher strides are used then the receptive fields will overlap less and the resulting output volume will have smaller dimensions spatially. Sometimes it is convenient to pad the input with zeros on the border of the input volume. The size of this zero-padding is a third hyperparameter. Zero padding allows to control the spatial size of the output volumes. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume. The spatial size of the output volume can be computed as a function of the input volume size , the receptive field size of the Conv Layer neurons , the stride with which they are applied , and the amount of zero padding used on the border. The formula for calculating how many neurons "fit" in a given volume is given by . If this number is not an integer, then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be when the stride is ensures that the input volume and output volume will have the same size spatially. ==== Parameter Sharing ==== Parameter sharing scheme is used in Convolutional Layers to control the number of free parameters. It relies on one reasonable assumption: That if one patch feature is useful to compute at some spatial position, then it should also be useful to compute at a different position. In other words, denoting a single 2-dimensional slice of depth as a depth slice, we constrain the neurons in each depth slice to use the same weights and bias. Since all neurons in a single depth slice are sharing the same parametrization, then the forward pass in each depth slice of the CONV layer can be computed as a convolution of the neuron's weights with the input volume (Hence the name: Convolutional Layer). Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter Sharing contributes to the translation invariance of the CNN architecture. It is important to notice that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure, in which we expect completely different features to be learned on different spatial locations. One practical example is when the input are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a locally connected layer. === Pooling layer === Another important concept of CNNs is pooling, which is a form of non-linear down-sampling. Pooling partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum. The intuition is that once a feature has been found, its exact location isn't as important as its rough location relative to other features. The function of the pooling layer is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. It is common to periodically insert a Pooling layer in-between successive Conv layers in a CNN architecture. The Pooling operation provides a form of translation invariance. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers. The depth dimension remains unchanged. In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice. Due to the aggressive reduction in the size of the representation (which is helpful only for smaller datasets to control overfitting), the current trend in the literature is towards using smaller filters or discarding the pooling layer altogether. === ReLU layer === ReLU is the abbreviation of Rectified Linear Units. This is a layer of neurons that applies the non-saturating activation function . It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer. Other functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent , , and the sigmoid function . Compared to other functions the usage of ReLU is preferable, because it results in the neural network training several times faster, without making a significant difference to generalisation accuracy. === Fully Connected layer === Finally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset. === Loss layer === The loss layer specifies how the network training penalizes the deviation between the predicted and true labels and is normally the last layer in the network. Various loss functions appropriate for different tasks may be used there. Softmax loss is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in . Euclidean loss is used for regressing to real-valued labels . == Layer patterns == The most common form of a CNN architecture stacks a few Conv-ReLU layers, follows them with POOL layers, and repeats this pattern until the input has been merged spatially to a small size. At some point, it is common to transition to fully connected layers. The last fully connected layer holds the output, such as the class scores. Here are some common CNN architectures that follow this pattern: INPUT -> FC implements a linear classifier INPUT -> CONV -> RELU -> FC INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC Here there is a single CONV layer between every POOL layer INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC Here there are two CONV layers stacked before every POOL layer. Stacking CONV layers with smaller filters as opposed to having one CONV layer with a large filter allows to express more powerful features of the input, with fewer parameters. As a practical disadvantage, more memory might be required to hold all the intermediate CONV layer results for the backpropagation step. == Choosing hyperparameters == CNNs use more hyperparameters than a standard MLP. While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimising convolutional networks. === Number of filters === Since feature map size decreases with depth, layers near the input layer will tend to have fewer filters while layers higher up can have more. To equalize computation at each layer, the product of the number of features and the number of pixel positions is typically picked to be roughly constant across layers. Preserving the information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) to be non-decreasing from one layer to the next. The number of feature maps directly controls capacity and depends on the number of available examples and the complexity of the task. === Filter shape === Common field shapes found in the literature vary greatly, and are usually chosen based on the dataset. Best results on MNIST-sized images (28x28) are usually in the 5x5 range on the first layer, while natural image datasets (often with hundreds of pixels in each dimension) tend to use larger first-layer filters of shape 12x12 or 15x15. The challenge is thus to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset. === Max Pooling Shape === Typical values are 2x2. Very large input volumes may warrant 4x4 pooling in the lower-layers. However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in discarding too much information. == Regularization methods == === Empirical === ==== Dropout ==== Since a fully connected layer occupies most of the parameters, it is prone to overfitting. The dropout method is introduced to prevent overfitting. At each training stage, individual nodes are either "dropped out" of the net with probability or kept with probability , so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages, the probability a hidden node will be retained (i.e. not dropped) is usually 0.5; for input nodes the retention probability should be much higher, intuitively because information is directly lost when input nodes are ignored. At testing time after training has finished, we would ideally like to find a sample average of all possible dropped-out networks; unfortunately this is unfeasible for large values of . However, we can find an approximation by using the full network with each node's output weighted by a factor of , so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates neural nets, and as such allows for model combination, at test time only a single network needs to be tested. By avoiding training all nodes on all training data, dropout decreases overfitting in neural nets. The method also significantly improves the speed of training. This makes model combination practical, even for deep neural nets. The technique seems to reduce the complex, tightly fitted interactions between nodes, leading them to learn more robust features which better generalize to new data. Dropout has been shown to improve the performance of neural networks on tasks in vision, speech recognition, document classification, and computational biology. ==== DropConnect ==== DropConnect is the generalization of Dropout in which each connection, rather than each output unit, can be dropped with probability . Each unit thus receives input from a random subset of units in the previous layer. DropConnect is similar to Dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. ==== Stochastic pooling ==== A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected. In stochastic pooling, the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent MNIST performance. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. ==== Artificial data ==== Since the degree of overfitting of a model is determined by both its power and the amount of training it receives, providing a convolutional network with extra training examples can reduce overfitting. Since these networks are usually already trained with all available data, one approach is to either generate new examples from scratch (if possible) or perturb the existing training samples to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original. === Explicit === ==== Network size ==== The simplest way to prevent overfitting of a network is simply to limit the number of hidden units and free parameters (connections) in the network. This restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a "zero norm". ==== Weight decay ==== A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors. L2 regularization is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather that some of its inputs a lot. L1 regularization is another relatively common form of regularization. It is possible to combine the L1 regularization with the L2 regularization (this is called Elastic net regularization). The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. ==== Max norm constraints ==== Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector of every neuron to satisfy . Typical values of are on orders of 3 or 4. Some papers report improvements when using this form of regularization. == Hierarchical coordinate frames == Pooling in convolutional networks loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). The precise spatial relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in different pools, helps retain the information about the position of a feature. But convolutional nets that just use translation cannot extrapolate their understanding of geometric relationships to a radically new viewpoint, like a different orientation or a different scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint. Currently, the common way to deal with this problem is to train the convolutional nets on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations, which is extremely computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic coordinate frame of the feature. Thus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach one can say that the higher level entity (e.g. face) is present when the lower level visual entities (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose ("pose vectors") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes. == Applications == === Image recognition === Convolutional neural networks are often used in image recognition systems. They have achieved an error rate of 0.23 percent on the MNIST database, which as of February 2012 is the lowest achieved on the database. Another paper on using CNN for image classification reported that the learning process was "surprisingly fast"; in the same paper, the best published results at the time were achieved in the MNIST database and the NORB database. When applied to facial recognition, they were able to contribute to a large decrease in error rate. In another paper, they were able to achieve a 97.6 percent recognition rate on "5,600 still images of more than 10 subjects". CNNs have been used to assess video quality in an objective way after being manually trained; the resulting system had a very low root mean square error. The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, which is large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. Performance of convolutional neural networks on the ImageNet tests is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease. In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded with competitive performance. The network trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations. === Video analysis === Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of different convolutional neural networks, responsible for spatial and temporal stream. Unsupervised learning schemes for training spatio-temporal features have also been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis. === Natural language processing === Convolutional neural networks have also seen use in the field of natural language processing. CNN models have subsequently been shown to be effective for various NLP problems and have achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction, and other traditional NLP tasks. === Drug discovery === Convolutional neural networks have been used in drug discovery. Predicting the interaction between molecules and biological proteins can be used to identify potential treatments that are more likely to be effective and safe. In 2015, Atomwise introduced AtomNet, the first deep learning neural networks for structure-based rational drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similarly to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for several disease targets, most notably treatments for the Ebola virus and multiple sclerosis. === Playing Go === Convolutional neural networks have been used in computer Go. In December 2014, Christopher Clark and Amos Storkey published a paper showing a convolutional network trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Shortly after it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move. A couple of CNNs for choosing moves to try ("policy network") and evaluating positions ("value network") driving MCTS were used by AlphaGo, Google Deepmind's program that was the first to beat a professional human player. == Fine-tuning == For many applications, only a small amount of training data is available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets. == Common libraries == Caffe: Caffe (replacement of Decaf) has been the most popular library for convolutional neural networks. It is created by the Berkeley Vision and Learning Center (BVLC). The advantages are that it has cleaner architecture and greater speed. It supports both CPU and GPU, easily switching between them. It is developed in C++, and has Python and MATLAB wrappers. In the developing of Caffe, protobuf is used to make researchers tune the parameters easily as well as adding or removing layers. Torch (www.torch.ch): A scientific computing framework with wide support for machine learning algorithms, written in C and lua. The main author is Ronan Collobert, and it is now widely used at Facebook AI Research, Google DeepMind and Twitter, among others. neon: The fastest framework for convolutional neural networks and Deep Learning with support for GPU and CPU backends. The front-end is in Python, while the fast kernels are written in custom shader assembly. Several pre-trained models are available. OverFeat: A pre-trained feature extractor by Pierre Sermanet. Cuda-convnet: A convnet implementation in CUDA MatConvnet Theano: written in Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation. Deeplearning4j: Deep learning in Java and Scala on GPU-enabled Spark deeplearning-hs: Deep learning in Haskell, supports computations with CUDA. TensorFlow: Google-supported Apache 2.0 license Theano-like library with support for CPU, GPU, mobile Veles: Neural network training management software from Samsung == See also == Neocognitron Convolution Deep learning Time delay neural network == References == == External links == A demonstration of a convolutional network created for character recognition Caffe Matlab toolbox MatConvnet Theano UFLDL Tutorial Deeplearning4j's Convolutional Nets 
In genetic algorithms, crossover is a genetic operator used to vary the programming of a chromosome or chromosomes from one generation to the next. It is analogous to reproduction and biological crossover, upon which genetic algorithms are based. Cross over is a process of taking more than one parent solutions and producing a child solution from them. There are methods for selection of the chromosomes. Those are also given below. == Methods of selection of chromosomes for crossoverEdit == Fitness proportionate selection (SCX) The individual is selected on the basis of fitness. The probability of an individual to be selected increases with the fitness of the individual greater or less than its competitor's fitness. Boltzmann selection Tournament selection Rank selection Steady state selection Truncation selection Local selection == Crossover techniquesEdit == Many crossover techniques exist for organisms which use different data structures to store themselves. === One-point crossoverEdit === A single crossover point on both parents' organism strings is selected. All data beyond that point in either organism string is swapped between the two parent organisms. The resulting organisms are the children: === Two-point crossoverEdit === Two-point crossover calls for two points to be selected on the parent organism strings. Everything between the two points is swapped between the parent organisms, rendering two child organisms: === "Cut and splice"Edit === Another crossover variant, the "cut and splice" approach, results in a change in length of the children strings. The reason for this difference is that each parent string has a separate choice of crossover point. === Uniform Crossover and Half Uniform CrossoverEdit === The Uniform Crossover uses a fixed mixing ratio between two parents. Unlike one- and two-point crossover, the Uniform Crossover enables the parent chromosomes to contribute the gene level rather than the segment level. If the mixing ratio is 0.5, the offspring has approximately half of the genes from first parent and the other half from second parent, although cross over points can be randomly chosen as seen below: The Uniform Crossover evaluates each bit in the parent strings for exchange with a probability of 0.5. Even though the uniform crossover is a poor method, empirical evidence suggest that it is a more exploratory approach to crossover than the traditional exploitative approach that maintains longer schemata. This results in a more complete search of the design space with maintaining the exchange of good information. Unfortunately, no satisfactory theory exists to explain the discrepancies between the Uniform Crossover and the traditional approaches. In the uniform crossover scheme (UX) individual bits in the string are compared between two parents. The bits are swapped with a fixed probability, typically 0.5. In the half uniform crossover scheme (HUX), exactly half of the nonmatching bits are swapped. Thus first the Hamming distance (the number of differing bits) is calculated. This number is divided by two. The resulting number is how many of the bits that do not match between the two parents will be swapped. === Three parent crossoverEdit === In this technique, the child is derived from three randomly chosen parents. Each bit of the first parent is compared with the same bit of the second parent. When these bits are the same it is used in the offspring, otherwise the bit from the third parent is used in the offspring. For example, the following three parents: p1 110100010p2 011001001p3 110110101 will produce the following offspring: op1p2p3 110100001 === Crossover for Ordered ChromosomesEdit === Depending on how the chromosome represents the solution, a direct swap may not be possible. One such case is when the chromosome is an ordered list, such as an ordered list of the cities to be travelled for the traveling salesman problem. There are many crossover methods for ordered chromosomes. The already mentioned N-point crossover can be applied for ordered chromosomes also, but this always need a corresponding repair process, actually, some ordered crossover methods are derived from the idea. However, sometimes a crossover of chromosomes produces recombinations which violate the constraint of ordering and thus need to be repaired. Several examples for crossover operators (also mutation operator) preserving a given order are given in: partially matched crossover (PMX): In this method, two crossover points are selected at random and PMX proceeds by position wise exchanges. The two crossover points give matching selection. It affects cross by position-by-position exchange operations. In this method parents are mapped to each other, hence we can also call it partially mapped crossover. cycle crossover (CX): Beginning at any gene in parent 1, the -th gene in parent 2 becomes replaced by it. The same is repeated for the displaced gene until the gene which is equal to the first inserted gene becomes replaced (cycle). order crossover operator (OX1): A portion of one parent is mapped to a portion of the other parent. From the replaced portion on, the rest is filled up by the remaining genes, where already present genes are omitted and the order is preserved. order-based crossover operator (OX2) position-based crossover operator (POS) voting recombination crossover operator (VR) alternating-position crossover operator (AP) sequential constructive crossover operator (SCX) Other possible methods include the edge recombination operator. == Crossover biasesEdit == For crossover operators which exchange contiguous sections of the chromosomes (e.g. k-point) the ordering of the variables may become important. This is particularly true when good solutions contain building blocks which might be disrupted by a non-respectful crossover operator. == See alsoEdit == Mutation (genetic algorithm) Chromosome (genetic algorithm) Fitness function Fitness approximation == ReferencesEdit == John Holland, Adaptation in Natural and Artificial Systems, University of Michigan Press, Ann Arbor, Michigan. 1975. ISBN 0-262-58111-6. Larry J. Eshelman, The CHC Adaptive Search Algorithm: How to Have Safe Search When Engaging in Nontraditional Genetic Recombination, in Gregory J. E. Rawlins editor, Proceedings of the First Workshop on Foundations of Genetic Algorithms. pages 265-283. Morgan Kaufmann, 1991. ISBN 1-55860-170-8. Tomasz D. Gwiazda, Genetic Algorithms Reference Vol.1 Crossover for single-objective numerical optimization problems, Tomasz Gwiazda, Lomianki, 2006. ISBN 83-923958-3-2. == External linksEdit == Newsgroup: comp.ai.genetic FAQ - see section on crossover (also known as recombination). 
Databricks is a company founded by the creators of Apache Spark, that aims to help clients with cloud-based big data processing using Spark. == People == Databricks grew out of the Amplab project at University of California, Berkeley that was involved in making Apache Spark, a distributed computing framework built atop Scala. Its co-founders are: Ali Ghodsi Andy Konwinski Ion Stoica, CEO, University of California, Berkeley professor and co-founder and CTO of Conviva. Patrick Wendell Reynold Xin Matei Zaharia, CTO, who created Apache Spark while a Ph.D. candidate at the University of California, Berkeley, and is currently a professor at the Massachusetts Institute of Technology. == History == In September 2013, Databricks announced that it had raised $13.9 million from Andreessen Horowitz and said it aimed to offer an alternative to Google's MapReduce system. In March 2014, Databricks certified Alpine Data Labs on Apache Spark. In April 2014, MapR, a big data analytics company, announced a partnership with Databricks to distribute and support the Apache Spark platform. Databricks was in the news again in June 2014, when it was reported that it had raised a $33 million Series B, led by New Enterprise Associates, along with additional investment from Series A investor Andreessen Horowitz. Along with the funding announcement, the company announced the launch of Databricks Cloud, a cloud computing solution with three main parts: the Databricks Platform, Spark and the Databricks workspace. Although Databricks Cloud would initially be designed to run on Amazon Web Services, the company said they would eventually make it compatible with the Google Compute Engine and Microsoft Azure. == Relation with Apache Spark == Databricks, founded by the team that created Spark, is closely involved with the development of Apache Spark, an open-source project incubated by the Apache Foundation. It is also one of the platinum sponsors, alongside Amazon Web Services, IBM, and SAP SE, of the Spark Summit, an annual conference for developers who use Spark. == References == == External links == Official website 
In probability and statistics, density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function. The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population. A variety of approaches to density estimation are used, including Parzen windows and a range of data clustering techniques, including vector quantization. The most basic form of density estimation is a rescaled histogram. == Example of density estimationEdit == We will consider records of the incidence of diabetes. The following is quoted verbatim from the data set description: A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes mellitus according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases. We used the 532 complete records. In this example, we construct three density estimates for "glu" (plasma glucose concentration), one conditional on the presence of diabetes, the second conditional on the absence of diabetes, and the third not conditional on diabetes. The conditional density estimates are then used to construct the probability of diabetes conditional on "glu". The "glu" data were obtained from the MASS package of the R programming language. Within R, ?Pima.tr and ?Pima.te give a fuller account of the data. The mean of "glu" in the diabetes cases is 143.1 and the standard deviation is 31.26. The mean of "glu" in the non-diabetes cases is 110.0 and the standard deviation is 24.29. From this we see that, in this data set, diabetes cases are associated with greater levels of "glu". This will be made clearer by plots of the estimated density functions. The first figure shows density estimates of p(glu | diabetes=1), p(glu | diabetes=0), and p(glu). The density estimates are kernel density estimates using a Gaussian kernel. That is, a Gaussian density function is placed at each data point, and the sum of the density functions is computed over the range of the data. From the density of "glu" conditional on diabetes, we can obtain the probability of diabetes conditional on "glu" via Bayes' rule. For brevity, "diabetes" is abbreviated "db." in this formula. The second figure shows the estimated posterior probability p(diabetes=1 | glu). From these data, it appears that an increased level of "glu" is associated with diabetes. === Script for exampleEdit === The following R commands will create the figures shown above. These commands can be entered at the command prompt by using cut and paste. Note that the above conditional density estimator uses bandwidths that are optimal for unconditional densities. Alternatively, one could use the method of Hall, Racine and Li (2004) and the R np package for automatic (data-driven) bandwidth selection that is optimal for conditional density estimates; see the np vignette for an introduction to the np package. The following R commands use the npcdens() function to deliver optimal smoothing. Note that the response "Yes"/"No" is a factor. The third figure uses optimal smoothing via the method of Hall, Racine, and Li indicating that the unconditional density bandwidth used in the second figure above yields a conditional density estimate that may be somewhat undersmoothed. == See alsoEdit == Kernel density estimation Mean integrated squared error Histogram Multivariate kernel density estimation Spectral density estimation Kernel embedding of distributions == ReferencesEdit == Sources Brian D. Ripley (1996). Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press. ISBN 978-0521460866. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. New York: Springer, 2001. ISBN 0-387-95284-5. (See Chapter 6.) Qi Li and Jeffrey S. Racine. Nonparametric Econometrics: Theory and Practice. Princeton University Press, 2007, ISBN 0-691-12161-3. (See Chapter 1.) D.W. Scott. Multivariate Density Estimation. Theory, Practice and Visualization. New York: Wiley, 1992. B.W. Silverman. Density Estimation. London: Chapman and Hall, 1986. ISBN 978-0-412-24620-3 == External linksEdit == CREEM: Centre for Research Into Ecological and Environmental Modelling Downloads for free density estimation software packages Distance 4 (from Research Unit for Wildlife Population Assessment "RUWPA") and WiSP. UCI Machine Learning Repository Content Summary (See "Pima Indians Diabetes Database" for the original data set of 732 records, and additional notes.) Free MATLAB code for one and two dimensional density estimation libAGF C++ software for variable kernel density estimation. 
As a subfield in artificial intelligence, Diagnosis is concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct. If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing. The computation is based on observations, which provide information on the current behaviour. The expression diagnosis also refers to the answer of the question of whether the system is malfunctioning or not, and to the process of computing the answer. This word comes from the medical context where a diagnosis is the process of identifying a disease by its symptoms. == Example == An example of diagnosis is the process of a garage mechanic with an automobile. The mechanic will first try to detect any abnormal behavior based on the observations on the car and his knowledge of this type of vehicle. If he finds out that the behavior is abnormal, the mechanic will try to refine his diagnosis by using new observations and possibly testing the system, until he discovers the faulty component. It means mechanic plays an important role in the vehicle diagnosis. == Expert diagnosis == The expert diagnosis (or diagnosis by expert system) is based on experience with the system. Using this experience, a mapping is built that efficiently associates the observations to the corresponding diagnoses. The experience can be provided: By a human operator. In this case, the human knowledge must be translated into a computer language. By examples of the system behaviour. In this case, the examples must be classified as correct or faulty (and, in the latter case, by the type of fault). Machine learning methods are then used to generalize from the examples. The main drawbacks of these methods are: The difficulty acquiring the expertise. The expertise is typically only available after a long period of use of the system (or similar systems). Thus, these methods are unsuitable for safety- or mission-critical systems (such as a nuclear power plant, or a robot operating in space). Moreover, the acquired expert knowledge can never be guaranteed to be complete. In case a previously unseen behaviour occurs, leading to an unexpected observation, it is impossible to give a diagnosis. The complexity of the learning. The off-line process of building an expert system can require a large amount of time and computer memory. The size of the final expert system. As the expert system aims to map any observation to a diagnosis, it will in some cases require a huge amount of storage space. The lack of robustness. If even a small modification is made on the system, the process of constructing the expert system must be repeated. A slightly different approach is to build an expert system from a model of the system rather than directly from an expertise. An example is the computation of a diagnoser for the diagnosis of discrete event systems. This approach can be seen as model-based, but it benefits from some advantages and suffers some drawbacks of the expert system approach. == Model-based diagnosis == Model-based diagnosis is an example of abductive reasoning using a model of the system. In general, it works as follows: We have a model that describes the behaviour of the system (or artefact). The model is an abstraction of the behaviour of the system and can be incomplete. In particular, the faulty behaviour is generally little-known, and the faulty model may thus not be represented. Given observations of the system, the diagnosis system simulates the system using the model, and compares the observations actually made to the observations predicted by the simulation. The modelling can be simplified by the following rules (where is the Abnormal predicate): (fault model) The semantics of these formulae is the following: if the behaviour of the system is not abnormal (i.e. if it is normal), then the internal (unobservable) behaviour will be and the observable behaviour . Otherwise, the internal behaviour will be and the observable behaviour . Given the observations , the problem is to determine whether the system behaviour is normal or not ( or ). This is an example of abductive reasoning. == Diagnosability == A system is said to be diagnosable if whatever the behavior of the system, we will be able to determine without ambiguity a unique diagnosis. The problem of diagnosability is very important when designing a system because on one hand one may want to reduce the number of sensors to reduce the cost, and on the other hand one may want to increase the number of sensors to increase the probability of detecting a faulty behavior. Several algorithms for dealing with these problems exist. One class of algorithms answers the question whether a system is diagnosable; another class looks for sets of sensors that make the system diagnosable, and optionally comply to criteria such as cost optimization. The diagnosability of a system is generally computed from the model of the system. In applications using model-based diagnosis, such a model is already present and doesn't need to be built from scratch. == Bibliography == Hamscher, W.; L. Console; J. de Kleer (1992). Readings in model-based diagnosis. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. ISBN 1-55860-249-6. == See also == AI effect Applications of artificial intelligence List of emerging technologies Outline of artificial intelligence == External links == === DX workshops === DX is the annual International Workshop on Principles of Diagnosis that started in 1989. DX 2014 DX 2013 DX 2012 DX 2011 DX 2010 DX 2009 DX 2008 DX 2007 DX 2006 DX 2005 DX 2004 DX 2003 DX 2002 DX 2001 DX 2000 DX 1999 DX 1998 DX 1997 
In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, and can be divided into feature selection and feature extraction. == Feature selectionEdit == Feature selection approaches try to find a subset of the original variables (also called features or attributes). There are three strategies; filter (e.g. information gain) and wrapper (e.g. search guided by accuracy) approaches, and embedded (features are selected to add or be removed while building the model based on the prediction errors). See also combinatorial optimization problems. In some cases, data analysis such as regression or classification can be done in the reduced space more accurately than in the original space. == Feature extractionEdit == Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning. The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors. Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled kernel PCA. Other prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and local tangent space alignment (LTSA). These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA. More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors. A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions, which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved. An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis. A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feed-forward neural networks with a bottle-neck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation. == Dimension reductionEdit == For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality. Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding. For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional Time series) running a fast approximate K-NN search using locality sensitive hashing, random projection, "sketches" or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option. == Advantages of dimensionality reductionEdit == It reduces the time and storage space required. Removal of multi-collinearity improves the performance of the machine learning model. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D. == See alsoEdit == == NotesEdit == == ReferencesEdit == Fodor,I. (2002) "A survey of dimension reduction techniques". Center for Applied Scientific Computing, Lawrence Livermore National, Technical Report UCRL-ID-148494 Cunningham, P. (2007) "Dimension Reduction" University College Dublin, Technical Report UCD-CSI-2007-7 Zahorian, Stephen A.; Hu, Hongbing (2011). "Nonlinear Dimensionality Reduction Methods for Use with Automatic Speech Recognition". Speech Technologies. doi:10.5772/16863. ISBN 978-953-307-996-7. == External linksEdit == JMLR Special Issue on Variable and Feature Selection ELastic MAPs Locally Linear Embedding A Global Geometric Framework for Nonlinear Dimensionality Reduction 
Dlib is a general purpose cross platform open source software library written in the C++ programming language. Its design is heavily influenced by ideas from design by contract and component-based software engineering. This means it is, first and foremost, a collection of independent software components. Since development began in 2002, dlib has grown to include a wide variety of tools. In particular, it now contains software components for dealing with networking, threads, graphical user interfaces, data structures, linear algebra, machine learning, image processing, data mining, XML and text parsing, numerical optimization, Bayesian networks, and numerous other tasks. In recent years, much of the development has been focused on creating a broad set of statistical machine learning tools and in 2009 dlib was published in the Journal of Machine Learning Research. Since then it has been used in a wide range of domains. == References == == External links == Official website dlib C++ Library on SourceForge.net DLib: Library for Machine Learning 
Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning algorithms. == Background == Consider the following situation, which is a general setting of many supervised learning problems. We have two spaces of objects and and would like to learn a function (often called hypothesis) which outputs an object , given . To do so, we have at our disposal a training set of a few examples where is an input and is the corresponding response that we wish to get from . To put it more formally, we assume that there is a joint probability distribution over and , and that the training set consists of instances drawn i.i.d. from . Note that the assumption of a joint probability distribution allows us to model uncertainty in predictions (e.g. from noise in data) because is not a deterministic function of , but rather a random variable with conditional distribution for a fixed . We also assume that we are given a non-negative real-valued loss function which measures how different the prediction of a hypothesis is from the true outcome . The risk associated with hypothesis is then defined as the expectation of the loss function: A loss function commonly used in theory is the 0-1 loss function: , where is the indicator notation. The ultimate goal of a learning algorithm is to find a hypothesis among a fixed class of functions for which the risk is minimal: == Empirical risk minimization == In general, the risk cannot be computed because the distribution is unknown to the learning algorithm (this situation is referred to as agnostic learning). However, we can compute an approximation, called empirical risk, by averaging the loss function on the training set: Empirical risk minimization principle states that the learning algorithm should choose a hypothesis which minimizes the empirical risk: Thus the learning algorithm defined by the ERM principle consists in solving the above optimization problem. == Properties == === Computational complexity === Empirical risk minimization for a classification problem with 0-1 loss function is known to be an NP-hard problem even for such relatively simple class of functions as linear classifiers. Though, it can be solved efficiently when minimal empirical risk is zero, i.e. data is linearly separable. In practice, machine learning algorithms cope with that either by employing a convex approximation to 0-1 loss function (like hinge loss for SVM), which is easier to optimize, or by posing assumptions on the distribution (and thus stop being agnostic learning algorithms to which the above result applies). == References == == Literature == Vapnik, V. (2000). The Nature of Statistical Learning Theory. Information Science and Statistics. Springer-Verlag. ISBN 978-0-387-98780-4. 
In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble refers only to a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives. == OverviewEdit == Supervised learning algorithms are commonly described as performing the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner. The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner. Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model, so ensembles may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. Fast algorithms such as decision trees are commonly used with ensembles (for example Random Forest), although slower algorithms can benefit from ensemble techniques as well. == Ensemble theoryEdit == An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to over-fitting of the training data. Empirically, ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods, therefore, seek to promote diversity among the models they combine. Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees). Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity. == Common types of ensemblesEdit == === Bayes optimal classifierEdit === The Bayes Optimal Classifier is a classification technique. It is an ensemble of all the hypotheses in the hypothesis space. On average, no other ensemble can outperform it. Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. The Bayes Optimal Classifier can be expressed with the following equation: where is the predicted class, is the set of all possible classes, is the hypothesis space, refers to a probability, and is the training data. As an ensemble, the Bayes Optimal Classifier represents a hypothesis that is not necessarily in . The hypothesis represented by the Bayes Optimal Classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in ). Unfortunately, the Bayes Optimal Classifier cannot be practically implemented for any but the most simple of problems. There are several reasons why the Bayes Optimal Classifier cannot be practically implemented: Most interesting hypothesis spaces are too large to iterate over, as required by the . Many hypotheses yield only a predicted class, rather than a probability for each class as required by the term . Computing an unbiased estimate of the probability of the training set given a hypothesis () is non-trivial. Estimating the prior probability for each hypothesis () is rarely feasible. === Bootstrap aggregating (bagging)Edit === Bootstrap aggregating, often abbreviated as bagging, involves having each model in the ensemble vote with equal weight. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy. An interesting application of bagging in unsupervised learning is provided here. === BoostingEdit === Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data. By far, the most common implementation of Boosting is Adaboost, although some newer algorithms are reported to achieve better results. === Bayesian parameter averagingEdit === Bayesian parameter averaging (BPA) is an ensemble technique that seeks to approximate the Bayes Optimal Classifier by sampling hypotheses from the hypothesis space, and combining them using Bayes' law. Unlike the Bayes optimal classifier, Bayesian model averaging can be practically implemented. Hypotheses are typically sampled using a Monte Carlo sampling technique such as MCMC. For example, Gibbs sampling may be used to draw hypotheses that are representative of the distribution . It has been shown that under certain circumstances, when hypotheses are drawn in this manner and averaged according to Bayes' law, this technique has an expected error that is bounded to be at most twice the expected error of the Bayes optimal classifier. Despite the theoretical correctness of this technique, it has been found to promote over-fitting and to perform worse, empirically, compared to simpler ensemble techniques such as bagging; however, these conclusions appear to be based on a misunderstanding of the purpose of Bayesian model averaging vs. model combination. === Bayesian model combinationEdit === Bayesian model combination (BMC) is an algorithmic correction to BMA. Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. The results from BMC have been shown to be better on average (with statistical significance) than BMA, and bagging. The use of Bayes' law to compute model weights necessitates computing the probability of the data given each model. Typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. This would work well if the ensemble were big enough to sample the entire model-space, but such is rarely possible. Consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. It essentially reduces to an unnecessarily complex method for doing model selection. The possible weightings for an ensemble can be visualized as lying on a simplex. At each vertex of the simplex, all of the weight is given to a single model in the ensemble. BMA converges toward the vertex that is closest to the distribution of the training data. By contrast, BMC converges toward the point where this distribution projects onto the simplex. In other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution. The results from BMA can often be approximated by using cross-validation to select the best model from a bucket of models. Likewise, the results from BMC may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings. === Bucket of modelsEdit === A "bucket of models" is an ensemble in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set. The most common approach used for model-selection is cross-validation selection (sometimes called a "bake-off contest"). It is described with the following pseudo-code: For each model m in the bucket: Do c times: (where 'c' is some constant) Randomly divide the training dataset into two datasets: A, and B. Train m with A Test m with B Select the model that obtains the highest average score Cross-Validation Selection can be summed up as: "try them all with the training set, and pick the one that works best". Gating is a generalization of Cross-Validation Selection. It involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. Often, a perceptron is used for the gating model. It can be used to pick the "best" model, or it can be used to give a linear weight to the predictions from each model in the bucket. When a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. Landmark learning is a meta-learning approach that seeks to solve this problem. It involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best. === StackingEdit === Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although in practice, a single-layer logistic regression model is often used as the combiner. Stacking typically yields performance better than any single one of the trained models. It has been successfully used on both supervised learning tasks (regression, classification and distance learning ) and unsupervised learning (density estimation). It has also been used to estimate bagging's error rate. It has been reported to out-perform Bayesian model-averaging. The two top-performers in the Netflix competition utilized blending, which may be considered to be a form of stacking. == ReferencesEdit == == Further readingEdit == Zhou Zhihua (2012). Ensemble Methods: Foundations and Algorithms. Chapman and Hall/CRC. ISBN 978-1-439-83003-1. Robert Schapire; Yoav Freund (2012). Boosting: Foundations and Algorithms. MIT. ISBN 978-0-262-01718-3. == External linksEdit == Ensemble learning at Scholarpedia, curated by Robi Polikar. The Waffles (machine learning) toolkit contains implementations of Bagging, Boosting, Bayesian Model Averaging, Bayesian Model Combination, Bucket-of-models, and other ensemble techniques 
Explanation-based learning (EBL) is a form of machine learning that exploits a very strong, or even perfect, domain theory to make generalizations or form concepts from training examples. == Details == An example of EBL using a perfect domain theory is a program that learns to play chess by being shown examples. A specific chess position that contains an important feature, say, "Forced loss of black queen in two moves," includes many irrelevant features, such as the specific scattering of pawns on the board. EBL can take a single training example and determine what are the relevant features in order to form a generalization. A domain theory is perfect or complete if it contains, in principle, all information needed to decide any question about the domain. For example, the domain theory for chess is simply the rules of chess. Knowing the rules, in principle it is possible to deduce the best move in any situation. However, actually making such a deduction is impossible in practice due to combinatoric explosion. EBL uses training examples to make searching for deductive consequences of a domain theory efficient in practice. In essence, an EBL system works by finding a way to deduce each training example from the system's existing database of domain theory. Having a short proof of the training example extends the domain-theory database, enabling the EBL system to find and classify future examples that are similar to the training example very quickly. The main drawback of the method---the cost of applying the learned proof macros, as these become numerous---was analyzed by Minton. === Basic Formulation === EBL software takes four inputs: a hypothesis space (the set of all possible conclusions) a domain theory (axioms about a domain of interest) training examples (specific facts that rule out some possible hypotheses) operationality criteria (criteria for determining which features in the domain are efficiently recognizable, e.g. which features are directly detectable using sensors) == Application == An especially good application domain for a EBL is natural language processing (NLP). Here a rich domain theory, i.e., a natural language grammar---although neither perfect nor complete, is tuned to a particular application or particular language usage, using a treebank (training examples). Rayner pioneered this work. The first successful industrial application was to a commercial NL interface to relational databases. The method has been successfully applied to several large-scale natural language parsing system, where the utility problem was solved by omitting the original grammar (domain theory) and using specialized LR-parsing techniques, resulting in huge speed-ups, at a cost in coverage, but with a gain in disambiguation. EBL-like techniques have also been applied to surface generation, the converse of parsing. When applying EBL to NLP, the operationality criteria can be hand-crafted, or can be inferred from the treebank using either the entropy of its or-nodes or a target coverage/disambiguation trade-off (= recall/precision trade-off = f-score). EBL can also be used to compile grammar-based language models for speech recognition, from general unification grammars. Note how the utility problem, first exposed by Minton, was solved by discarding the original grammar/domain theory, and that the quoted articles tend to contain the phrase grammar specialization---quite the opposite of the original term explanation-based generalization. Perhaps the best name for this technique would be data-driven search space reduction. Other people who worked on EBL for NLP include Guenther Neumann, Aravind Joshi, Srinivas Bangalore, and Khalil Sima'an. == References == 
In machine learning, feature learning or representation learning is a set of techniques that learn a feature: a transformation of raw data input to a representation that can be effectively exploited in machine learning tasks. This obviates manual feature engineering, which is otherwise necessary, and allows a machine to both learn at a specific task (using the features) and learn the features themselves: to learn how to learn. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor measurement is usually complex, redundant, and highly variable. Thus, it is necessary to discover useful features or representations from raw data. Traditional hand-crafted features often require expensive human labor and often rely on expert knowledge. Also, they normally do not generalize well. This motivates the design of efficient feature learning techniques, to automate and generalize this. Feature learning can be divided into two categories: supervised and unsupervised feature learning, analogous to these categories in machine learning generally. In supervised feature learning, features are learned with labeled input data. Examples include neural networks, multilayer perceptron, and (supervised) dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization, and various forms of clustering. == Supervised feature learning == Supervised feature learning is to learn features from labeled data. Several approaches are introduced in the following. === Supervised dictionary learning === Dictionary learning is to learn a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error (over the input data), together with L1 regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights). Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, a supervised dictionary learning technique was proposed by Mairal et al. in 2009. The authors apply dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse representation of data), and an L2 regularization on the parameters of the classifier. === Neural networks === Neural networks are used to illustrate a family of learning algorithms via a "network" consisting of multiple layers of inter-connected nodes. It is inspired by the nervous system, where the nodes are viewed as neurons and edges are viewed as synapse. Each edge has an associated weight, and the network defines computational rules that passes input data from the input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights). Multilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. == Unsupervised feature learning == Unsupervised feature learning is to learn features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where first, features are learned from an unlabeled dataset, which are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following. === K-means clustering === K-means clustering is an approach for vector quantization. In particular, given a set of n vectors, k-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, and suboptimal greedy algorithms have been developed for k-means clustering. In feature learning, k-means clustering can be used to group an unlabeled set of inputs into k clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest way is to add k binary features to each sample, where each feature j has value one iff the jth centroid learned by k-means is the closest to the sample under consideration. It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has used to train RBF networks). Coates and Ng note that certain variants of k-means behave similarly to sparse coding algorithms. In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that k-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task. K-means has also been shown to improve performance in the domain of NLP, specifically for named-entity recognition; there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings). === Principal component analysis === Principal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of n input data vectors, PCA generates p (which is much smaller than the dimension of the input data) right singular vectors corresponding to the p largest singular values of the data matrix, where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors. These p singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations. PCA is a linear feature learning approach since the p singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the data matrix on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix. PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case in many applications. PCA only relies on orthogonal transformations of the original data, and it only exploits the first- and second-order moments of the data, which may not well characterize the distribution of the data. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues). === Local linear embedding === Local linear embedding (LLE) is a nonlinear unsupervised learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Sam T. Roweis and Lawrence K. Saul in 2000. The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set. LLE consists of two major steps. The first step is for "neighbor-preserving," where each input data point Xi is reconstructed as a weighted sum of K nearest neighboring data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between a point and its reconstruction) under the constraint that the weights associated to each point sum up to one. The second step is for "dimension reduction," by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with data being fixed, which can be solved as a least squares problem; while in the second step, lower-dimensional points are optimized with the weights being fixed, which can be solved via sparse eigenvalue decomposition. The reconstruction weights obtained in the first step captures the "intrinsic geometric properties" of a neighborhood in the input data. It is assumed that original data lie on a smooth lower-dimensional manifold, and the "intrinsic geometric properties" captured by the weights of the original data are expected also on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying structure of data. === Independent component analysis === Independent component analysis (ICA) is technique for learning a representation of data using a weighted sum of independent non-Gaussian components. The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution. === Unsupervised dictionary learning === Different from supervised dictionary learning, unsupervised dictionary learning does not utilize the labels of the data and only exploits the structure underlying the data for optimizing the dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionary, where the number of dictionary elements is larger than the dimension of the input data. Aharon et al. proposed an algorithm known as K-SVD for learning from unlabeled input data a dictionary of elements that enables sparse representation of the data. == Multilayer/Deep architectures == The hierarchical architecture of the neural system inspires deep learning architectures for feature learning by stacking multiple layers of simple learning blocks. These architectures are often designed based on the assumption of distributed representation: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by previous level as input, and produces new representations as output, which is then fed to higher levels. The input of bottom layer is the raw data, and the output of the final layer is the final low-dimensional feature or representation. === Restricted Boltzmann machine === Restricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures. An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general Boltzmann machines with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an energy function, based on which a joint distribution of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent conditioned on the visible (hidden) variables. Such conditional independence facilitates computations on RBM. An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using the contrastive divergence (CD) algorithm by Geoffrey Hinton. In general, the training of RBM by solving the above maximization problem tends to result in non-sparse representations. The sparse RBM, a modification of the RBM, was proposed to enable sparse representations. The idea is to add a regularization term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant . === Autoencoder === An autoencoder consisting of encoder and decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov where the encoder uses raw data (e.g., image) as input and produces feature or representation as output, and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a greedy layer-by-layer manner: after one layer of feature detectors is learned, they are fed to upper layers as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with stochastic gradient descent methods. Training can be repeated until some stopping criteria is satisfied. == See also == Basis function Deep learning Feature detection (computer vision) Feature extraction Kernel trick Vector quantization == References == 
Symbolic artificial intelligence is the collective name for all methods in artificial intelligence research that are based on high-level "symbolic" (human-readable) representations of problems, logic and search. Symbolic AI was the dominant paradigm of AI research from the middle fifties until the late 1980s. John Haugeland gave the name GOFAI ("Good Old-Fashioned Artificial Intelligence") to symbolic AI in his 1985 book Artificial Intelligence: The Very Idea, which explored the philosophical implications of artificial intelligence research. In robotics the analogous term is GOFAIR ("Good Old-Fashioned Robotics"). The approach is based on the assumption that many aspects of intelligence can be achieved by the manipulation of symbols, an assumption defined as the "physical symbol systems hypothesis" by Allen Newell and Herbert A. Simon in the middle 1960s: The most successful form of symbolic AI is expert systems, which use a network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. Opponents of the symbolic approach include roboticists such as Rodney Brooks, who aims to produce autonomous robots without symbolic representation (or with only minimal representation) and computational intelligence researchers, who apply techniques such as neural networks and optimization to solve problems in machine learning and control engineering. Symbolic AI was intended to produce general, human-like intelligence in a machine, whereas most modern research is directed at specific sub-problems. Research into general intelligence is now studied in the sub-field of artificial general intelligence. == ReferencesEdit == Haugeland, John (1985), Artificial Intelligence: The Very Idea, Cambridge, Mass: MIT Press, ISBN 0-262-08153-9 == See alsoEdit == Approaches (in Artificial intelligence) History of artificial intelligence Physical symbol systems hypothesis Synthetic intelligence 
H2O is open-source software for big-data analysis. It is produced by the start-up H2O.ai (formerly 0xdata), which launched in 2011 in Silicon Valley. The speed and flexibility of H2O allow users to fit hundreds or thousands of potential models as part of discovering patterns in data. With H2O, users can throw models at data to find usable information, allowing H2O to discover patterns. Using H2O, Cisco estimates each month 20 thousand models of its customers' propensities to buy. H2O's mathematical core is developed with the leadership of Arno Candel; after H2O was rated as the best "open-source Java machine learning project" by GitHub's programming members, Candel was named to the first class of "Big Data All Stars" by Fortune in 2014. The firm's scientific advisors are experts on statistical learning theory and mathematical optimization. The H2O software runs can be called from the statistical package R and other environments. It is used for exploring and analyzing datasets held in cloud computing systems and in the Apache Hadoop Distributed File System as well as in the conventional operating-systems Linux, Mac OS, and Microsoft Windows. The H2O software is written in Java, Python, and R. Its graphical-user interface is compatible with four popular browsers: Chrome, Safari, Firefox, and Internet Explorer. == H2O == The H2O project aims to develop an analytical interface for cloud computing, providing users with intuitive tools for data analysis. === Leadership === H2O's chief executive, SriSatish Ambati, had helped to start Platfora, a big-data firm that develops software for the Apache Hadoop distributed file system. Ambati was frustrated with the performance of the R programming language on large data-sets and started the development of H2O software with encouragement from John Chambers, who created the S programming language at Bell Labs and who is a member of R's core team (which leads the development of R). Ambati co-founded 0xdata with Cliff Click, who serves as the chief technical officer of H2O. Click helped to write the HotSpot Server Compiler and worked with Azul Systems to construct a big-data Java virtual machine (JVM). Mathematical leadership is provided by the Dr. Arno Candel, who has the title "physicist and hacker". Candel was a founding engineer at Skytree, where he implemented methods for machine learning, before he developed the mathematical core of H2O. After H2O was rated as the best "open-source Java machine learning project" by GitHub's programming members, Candel (with 19 others) was named to the first class of "Big Data All Stars" by Fortune. ==== Scientific advisory council ==== H2O's Scientific Advisory Council lists three mathematical scientists, who are all professors at Stanford University: Professor Stephen P. Boyd is an expert in convex minimization and applications in statistics and electrical engineering. Robert Tibshirani, a collaborator with Bradley Efron on bootstrapping, is an expert on generalized additive models and statistical learning theory. Trevor Hastie, a collaborator of John Chambers on S, is an expert on generalized additive models and statistical learning theory. === H2O.ai: A Silicon Valley start-up === The software is open-source and freely distributed. The company receives fees for providing customer service and customized extensions. In November 2014, its twenty clients included Cisco, eBay, Nielsen, and PayPal, according to VentureBeat. The speed and flexibility of H2O allow users to fit hundreds or thousands of potential models as part of discovering patterns in data. With H2O, users can throw models at data to find usable information, according to Tye Rattenbury at Trifacta. Using H2O, Cisco estimates each month 20 thousand models of its customers' propensities to buy while Google fits different models for each client according to the time of day. === Mining of big data === Big datasets are too large to be analyzed using traditional software like R. The H2O software provides data structures and methods suitable for big data. H2O allow users to analyze and visualize whole sets of data without using the Procrustean strategy of studying only a small subset with a conventional statistical package. H2O's statistical repertoire includes generalized linear models and K-means clustering. ==== Iterative methods for real-time problems ==== H2O uses iterative methods that provide quick answers using all of the client's data. When a client cannot wait for an optimal solution, the client can interrupt the computations and use an approximate solution. In its approach to deep learning, H2O divides all the data into subsets and then analyzing each subset simultaneously using the same method. These processes are combined to estimate parameters by using the Hogwild scheme, a parallel stochastic gradient method. These methods allow H2O to provide answers that use all the client's data, rather than throwing away most of it and analyzing a subset with conventional software. === Software === ==== Programming languages ==== The H2O software was written with three programming languages: Java (6 or later), Python (2.7.x), and R (3.0.0 or later). ==== Operating systems ==== The H2O software can be run on conventional operating-systems: Microsoft Windows (7 or later), Mac OS X (10.9 or later), and Linux (Ubuntu 12.04 ; RHEL/CentOS 6 or later), It also runs on big-data systems, particularly Apache Hadoop Distributed File System (HDFS), several popular versions: Cloudera (5.1 or later), MapR (3.0 or later), and Hortonworks (HDP 2.1 or later). It operates also on cloud computing environments, for example using Amazon EC2, Google Compute Engine, and Microsoft Azure. The H2O Sparkling Water software is Databricks-certified on Apache Spark. ==== Graphical user interface and browsers ==== Its graphical user interface is compatible with four browsers (unless specified, in their latest versions as of 1 June 2015): Chrome, Safari, Firefox, Internet Explorer (IE10). == Notes == == References == Gage, Deborah (15 April 2013). "Platfora founder goes in search of big-data answers". Wall Street Journal. Retrieved 2 June 2015. Hackett, Robert (3 August 2014), Nusca, Andrew; Hackett, Robert; Gupta, Shalene, eds., "Arno Candel, physicist and hacker, 0xdata", Fortune, Meet Fortune's 2014 Big Data All-Stars, retrieved 2 June 2015 Hardy, Quentin (3 May 2014). "Valuable humans in our digital future". New York Times. Retrieved 1 June 2015. Harris, Derrick (14 August 2012). "How 0xdata wants to help everyone become data scientists". Gigaom Research. Retrieved 1 June 2015. Novet, Jordan (7 November 2014). "0xdata takes $8.9M and becomes H2O to match its open-source machine-learning project". VentureBeat. Retrieved 1 June 2015. == External links == H2O software at H2O.ai (formerly 0xdata) Github repository of H2O software YouTube channel of H2O.ai 
In computer science, artificial intelligence, and mathematical optimization, a heuristic is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution. This is achieved by trading optimality, completeness, accuracy, or precision for speed. In a way, it can be considered a shortcut. A heuristic function, also called simply a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution. == Definition and motivation == The objective of a heuristic is to produce a solution in a reasonable time frame that is good enough for solving the problem at hand. This solution may not be the best of all the actual solutions to this problem, or it may simply approximate the exact solution. But it is still valuable because finding it does not require a prohibitively long time. Heuristics may produce results by themselves, or they may be used in conjunction with optimization algorithms to improve their efficiency (e.g., they may be used to generate good seed values). Results about NP-hardness in theoretical computer science make heuristics the only viable option for a variety of complex optimization problems that need to be routinely solved in real-world applications. == Trade-off == The trade-off criteria for deciding whether to use a heuristic for solving a given problem include the following: Optimality: When several solutions exist for a given problem, does the heuristic guarantee that the best solution will be found? Is it actually necessary to find the best solution? Completeness: When several solutions exist for a given problem, can the heuristic find them all? Do we actually need all solutions? Many heuristics are only meant to find one solution. Accuracy and precision: Can the heuristic provide a confidence interval for the purported solution? Is the error bar on the solution unreasonably large? Execution time: Is this the best known heuristic for solving this type of problem? Some heuristics converge faster than others. Some heuristics are only marginally quicker than classic methods. In some cases, it may be difficult to decide whether the solution found by the heuristic is good enough, because the theory underlying that heuristic is not very elaborate. == Examples == === Simpler problem === One way of achieving the computational performance gain expected of a heuristic consists in solving a simpler problem whose solution is also a solution to the initial problem. Such a heuristic is unable to find all the solutions to the initial problem, but it may find one much faster because the simple problem is easy to solve. === Traveling salesman problem === An example of approximation is described by Jon Bentley for solving the traveling salesman problem (TSP) so as to select the order to draw using a pen plotter. TSP is known to be NP-Complete so an optimal solution for even moderate size problem is intractable. Instead, the greedy algorithm can be used to give a good but not optimal solution (it is an approximation to the optimal answer) in a reasonably short amount of time. The greedy algorithm heuristic says to pick whatever is currently the best next step regardless of whether that precludes good steps later. It is a heuristic in that practice says it is a good enough solution, theory says there are better solutions (and even can tell how much better in some cases). === Search === Another example of heuristic making an algorithm faster occurs in certain search problems. Initially, the heuristic tries every possibility at each step, like the full-space search algorithm. But it can stop the search at any time if the current possibility is already worse than the best solution already found. In such search problems, a heuristic can be used to try good choices first so that bad paths can be eliminated early (see alpha-beta pruning). === Newell and Simon: heuristic search hypothesis === In their Turing Award acceptance speech, Allen Newell and Herbert A. Simon discuss the heuristic search hypothesis: a physical symbol system will repeatedly generate and modify known symbol structures until the created structure matches the solution structure. Each successive iteration depends upon the step before it, thus the heuristic search learns what avenues to pursue and which ones to disregard by measuring how close the current iteration is to the solution. Therefore, some possibilities will never be generated as they are measured to be less likely to complete the solution. A heuristic method can accomplish its task by using search trees. However, instead of generating all possible solution branches, a heuristic selects branches more likely to produce outcomes than other branches. It is selective at each decision point, picking branches that are more likely to produce solutions. === Virus scanning === Many virus scanners use heuristic rules for detecting viruses and other forms of malware. Heuristic scanning looks for code and/or behavioral patterns indicative of a class or family of viruses, with different sets of rules for different viruses. If a file or executing process is observed to contain matching code patterns and/or to be performing that set of activities, then the scanner infers that the file is infected. The most advanced part of behavior-based heuristic scanning is that it can work against highly randomized polymorphic viruses, which simpler string scanning-only approaches cannot reliably detect. Heuristic scanning has the potential to detect many future viruses without requiring the virus to be detected somewhere, submitted to the virus scanner developer, analyzed, and a detection update for the scanner provided to the scanner's users. == Pitfalls == Some heuristics have a strong underlying theory; they are either derived in a top-down manner from the theory or inferred from experimental data. Others are just rules of thumb learned empirically without even a glimpse of theory. The latter are exposed to a number of pitfalls. When a heuristic is reused in various contexts because it has been seen to "work" in one context, without having been mathematically proven to meet a given set of requirements, it is possible that the current data set does not necessarily represent future data sets and that purported "solutions" turn out to be akin to noise. Statistical analysis can be conducted when employing heuristics to estimate the probability of incorrect outcomes. To use a heuristic for solving a search or a knapsack problem, it is necessary to check that the heuristic is admissible. Given a heuristic function meant to approximate the true optimal distance to the goal node in a directed graph containing total nodes or vertexes labeled , "admissible" means that for all where . If a heuristic is not admissible, it may never find the goal, either by ending up in a dead end of graph or by skipping back and forth between two nodes and where . == See also == Algorithm Constructive heuristic Genetic algorithm Heuristic Heuristic routing Heuristic evaluation: Method for identifying usability problems in user interfaces. Metaheuristic: Methods for controlling and tuning basic heuristic algorithms, usually with usage of memory and learning. Matheuristics: Optimization algorithms made by the interoperation of metaheuristics and mathematical programming (MP) techniques. Reactive search optimization: Methods using online machine learning principles for self-tuning of heuristics. == References == 
The IEEE Signal Processing Society is a society of the IEEE. It is also known by the acronym IEEE SPS. In the hierarchy of IEEE, the Signal Processing Society is one of close to 40 technical societies organized under the IEEE's Technical Activities Board. The IEEE Signal Processing Society is IEEE's first technical Society. From the IEEE SPS web site, the mission of the society is defined to be : "The IEEE Signal Processing Society is an international organization whose purpose is to: advance and disseminate state-of-the-art scientific information and resources; educate the signal processing community; and provide a venue for people to interact and exchange ideas." and the field of interest is defined to be: "Signal processing is the enabling technology for the generation, transformation, extraction, and interpretation of information. It comprises the theory, algorithms with associated architectures and implementations, and applications related to processing information contained in many different formats broadly designated as signals. Signal processing uses mathematical, statistical, computational, heuristic, and/or linguistic representations, formalisms, modeling techniques and algorithms for generating, transforming, transmitting, and learning from signals." == History == The Signal Processing society was formed in 1948 as the Professional Group on Audio of the Institute of Radio Engineers. == Activities == The Society organizes many conferences every year and operates local chapters around the world. It coordinates the operation of several councils, task forces, and technical committees. === Publications === The Signal Processing Society oversees the publication of nine periodicals, including a magazine, newsletter and scholarly journals: IEEE Journal of Selected Topics in Signal Processing IEEE Signal Processing Letters IEEE Signal Processing Magazine Inside Signal Processing e-Newsletter IEEE Transactions on Audio, Speech, and Language Processing IEEE Transactions on Image Processing IEEE Transactions on Information Forensics and Security IEEE Transactions on Signal Processing Co-sponsored journals include: IEEE Computing in Science and Engineering Magazine IEEE MultiMedia Magazine IEEE Sensors Journal IEEE Transactions on Medical Imaging IEEE Transactions on Mobile Computing IEEE Transactions on Multimedia IEEE Transactions on Wireless Communications Digital Library: Signal Processing electronic Library (SPeL) The IEEE Signal Processing electronic Library (SPeL) is a comprehensive electronic collection of more than 50 years of the IEEE Signal Processing Society's periodicals, which includes all four transactions and letters, newsletters, magazines, and technical proceedings of the two major Society conferences and of most workshops. IEEE SPeL also features four joint transactions, which the Society is a partner. The library collection is available on three DVD-ROMs. It has a search engine, which allows searches based on full-text, as well as searches by such fields as author, title, subject and date. The collection provides a comprehensive database, which contains information about the technical articles that are published in the Society's journals === Conferences === The Society organizes, sponsors, and co-sponsors numerous conferences and workshops every year. A list of them can be found here. == See also == International Conference on Acoustics, Speech, and Signal Processing (ICASSP) International Conference on Image Processing (ICIP) International Symposium on Biomedical Imaging (ISBI) International Symposium on Signal Processing and Information Technology (ISSPIT) International Conference on Signal and Image Processing (SIPS) International Conference on Information Processing in Sensor Networks (IPSN) == References == ^ "IEEE Societies, Councils and Technical Communities". Institute of Electrical and Electronics Engineers. Retrieved 2009-05-01. ^ IEEE Global History Network (2011). "IEEE Signal Processing Society History". IEEE History Center. Retrieved 7 July 2011. ^ "SPS Periodicals". IEEE Signal Processing Society. Retrieved 2009-05-01. == External links == The Signal Processing Society's Website 
Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints. Depending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming. == DefinitionEdit == Inductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases. Output of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language. In many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis, usually opposed to 'deductive' program synthesis, where the specification is usually complete. In other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples. The diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages. == HistoryEdit == Research on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers and work of Biermann. These approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid 1980s are surveyed by Smith. Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade. The advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro eventually spawning the new field of inductive logic programming (ILP). The early works of Plotkin, and his "relative least general generalization (rlgg)", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM. But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery. In parallel to work in ILP, Koza proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE and the systematic-search-based system MagicHaskeller. Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned. The early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem. The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold. More recently, the language learning problem was addressed by the inductive programming community. In the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below). Other ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures; abstraction has also been explored as a more powerful approach to cumulative learning and function invention. One powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming). == Application areasEdit == The first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where "learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations". Since then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming, the related areas of programming by example and programming by demonstration, and intelligent tutoring systems. Automatic data manipulation has also been subject of some 'killer applications' for inductive programming, such as the 'Flash Fill' tool in Microsoft Excel. Other areas where inductive inference has been recently applied are knowledge acquisition, artificial general intelligence, reinforcement learning and theory evaluation, and cognitive science in general. There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces. == See alsoEdit == Automatic programming Declarative programming Evolutionary programming Functional programming Genetic programming Grammar induction Inductive reasoning Inductive logic programming Inductive functional programming Logic programming Machine learning Probabilistic programming language Program synthesis Programming by example Programming by demonstration* Structure mining Test-driven development == External linksEdit == Inductive Programming community page, hosted by the University of Bamberg. == Further readingEdit == 
 == Supervised learningEdit == AODE Artificial neural network Backpropagation Autoencoders Hopfield networks Boltzmann machines Restricted Boltzmann Machines Spiking neural networks Bayesian statistics Bayesian network Bayesian knowledge base Case-based reasoning Gaussian process regression Gene expression programming Group method of data handling (GMDH) Inductive logic programming Instance-based learning Lazy learning Learning Automata Learning Vector Quantization Logistic Model Tree Minimum message length (decision trees, decision graphs, etc.) Nearest Neighbor Algorithm Analogical modeling Probably approximately correct learning (PAC) learning Ripple down rules, a knowledge acquisition methodology Symbolic machine learning algorithms Support vector machines Random Forests Ensembles of classifiers Bootstrap aggregating (bagging) Boosting (meta-algorithm) Ordinal classification Information fuzzy networks (IFN) Conditional Random Field ANOVA Linear classifiers Fisher's linear discriminant Logistic regression Multinomial logistic regression Naive Bayes classifier Perceptron Support vector machines Quadratic classifiers k-nearest neighbor Boosting Decision trees C4.5 Random forests ID3 CART SLIQ SPRINT Bayesian networks Naive Bayes Hidden Markov models == Unsupervised learningEdit == Expectation-maximization algorithm Vector Quantization Generative topographic map Information bottleneck method === Artificial neural networkEdit === Self-organizing map === Association rule learningEdit === Apriori algorithm Eclat algorithm FP-growth algorithm === Hierarchical clusteringEdit === Single-linkage clustering Conceptual clustering === Cluster analysisEdit === K-means algorithm Fuzzy clustering DBSCAN OPTICS algorithm === Outlier DetectionEdit === Local Outlier Factor == Semi-supervised learningEdit == == Reinforcement learningEdit == Temporal difference learning Q-learning Learning Automata SARSA == Deep learningEdit == Deep belief networks Deep Boltzmann machines Deep Convolutional neural networks Deep Recurrent neural networks Hierarchical temporal memory == OthersEdit == Data Pre-processing List of artificial intelligence projects 
The MIT Press is a university press affiliated with the Massachusetts Institute of Technology (MIT) in Cambridge, Massachusetts (United States). == History == The MIT Press traces its origins back to 1926 when MIT published under its own name a lecture series entitled Problems of Atomic Dynamics given by the visiting German physicist and later Nobel Prize winner, Max Born. Six years later, MIT's publishing operations were first formally instituted by the creation of an imprint called Technology Press in 1932. This imprint was founded by James R. Killian, Jr., at the time editor of MIT's alumni magazine and later to become MIT president. Technology Press published eight titles independently, then in 1937 entered into an arrangement with John Wiley & Sons in which Wiley took over marketing and editorial responsibilities. In 1962 the association with Wiley came to an end after a further 125 titles had been published. The press acquired its modern name after this separation, and has since functioned as an independent publishing house. A European marketing office was opened in 1969, and a Journals division was added in 1972. In the late 1970s, responding to changing economic conditions, the publisher narrowed the focus of their catalog to a few key areas, initially architecture, computer science and artificial intelligence, economics, and cognitive science. Other areas, such as technology and design, have been added since. The latest addition is environmental science. In January 2010 the MIT Press published its 9000th title, and published about 200 books and 30 journals. In 2012 the Press celebrated its 50th anniversary, including publishing a commemorative booklet on paper and online. == Business == MIT Press primarily publishes academic titles in the fields of Art & Architecture, Visual & Cultural Studies, Cognitive Science, Philosophy, Linguistics, Computer Science, Economics, Finance & Business, Environmental Science, Political Science, Life Sciences, Neuroscience, New Media, and Science, Technology, & Society. The MIT Press is a distributor for such publishers as Zone Books and Semiotext(e). In 2000, the MIT Press created CogNet, an online resource for the study of the brain and the cognitive sciences. The MIT Press also operates the MIT Press Bookstore showcasing both its front and backlist titles, along with a large selection of complementary works from other academic and trade publishers. The retail storefront is located next to the inbound Kendall Square Station of the MBTA Red Line subway in Cambridge, Massachusetts. The Bookstore offers customized selections from the MIT Press at many conferences and symposia in the Boston area, and sponsors occasional lectures and book signings at MIT. In 1981 the MIT Press published its first book under the Bradford Books imprint, Brainstorms: Philosophical Essays on Mind and Psychology by Daniel C. Dennett. == Logo == The Press uses a logo designed by its longtime art director, Muriel Cooper, in 1962. The design is based on the lower-case letters "mitp," hence the tall "t" at the fifth stripe and the drop "p" at the sixth-and-seventh stripe. The logo is somewhat reminiscent of the trigrams of the I Ching. The logo later served as a touchtone for the MIT Media Lab logo, which was redesigned in 2015 by Pentagram. == List of journals published by the MIT Press == The Arts & Humanities Economics International Affairs, History & Political Science Science & Technology == References == == External links == Official Website MIT Press Journals Homepage The MIT PressLog (news blog for the MIT Press) 
MLPACK is a machine learning software library for C++, built on top of the Armadillo library. MLPACK has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users. Its intended target users are scientists and engineers. It is open source software distributed under the BSD license, making it useful for developing both open source and proprietary software. Releases 1.0.11 and before were released under the LGPL license. The project is supported by the Georgia Institute of Technology and contributions from around the world. == Supported algorithms == Currently MLPACK supports the following algorithms: Collaborative Filtering Density Estimation Trees Euclidean Minimum Spanning Trees Fast Exact Max-Kernel Search (FastMKS) Gaussian Mixture Models (GMMs) Hidden Markov Models (HMMs) Kernel Principal Component Analysis (KPCA) K-Means Clustering Least-Angle Regression (LARS/LASSO) Local Coordinate Coding Locality-Sensitive Hashing (LSH) Logistic regression Naive Bayes Classifier Neighbourhood Components Analysis (NCA) Non-negative Matrix Factorization (NMF) Principal Components Analysis (PCA) Independent component analysis (ICA) Rank-Approximate Nearest Neighbor (RANN) Simple Least-Squares Linear Regression (and Ridge Regression) Sparse Coding Tree-based Neighbor Search (all-k-nearest-neighbors, all-k-furthest-neighbors), using either kd-trees or cover trees Tree-based Range Search == See also == Numerical linear algebra List of numerical libraries List of numerical analysis software Scientific computing Armadillo (C++ library) == References == == External links == Official website mlpack on GitHub 
MOA (Massive Online Analysis) is a free open-source software specific for Data stream mining with Concept drift. It's written in Java and developed at the University of Waikato, New Zealand. == Description == MOA is an open-source framework software that allows to build and run experiments of machine learning or data mining on evolving data streams. It includes a set of learners and stream generators that can be used from the Graphical User Interface (GUI), the command-line, and the Java API. MOA contains several collections of machine learning algorithms: Classification Bayesian classifiers Naive Bayes Naive Bayes Multinomial Decision trees classifiers Decision Stump Hoeffding Tree Hoeffding Option Tree Hoeffding Adaptive Tree Meta classifiers Bagging Boosting Bagging using ADWIN Bagging using Adaptive-Size Hoeffding Trees. Perceptron Stacking of Restricted Hoeffding Trees Leveraging Bagging Online Accuracy Updated Ensemble Function classifiers Perceptron Stochastic gradient descent (SGD) Pegasos Drift classifiers Multi-label classifiers Active learning classifiers Regression FIMTDD AMRules ClusteringStreamKM++ CluStream ClusTree D-Stream CobWeb. Outlier detectionSTORM Abstract-C COD MCOD AnyOut Recommender systems BRISMFPredictor Frequent pattern mining Itemsets Graphs Change detection algorithms These algorithms are designed for large scale machine learning, dealing with concept drift, and big data streams in real time. MOA supports bi-directional interaction with Weka (machine learning). MOA is free software released under the GNU GPL. == See also == ADAMS Workflow: Workflow engine for MOA and Weka (machine learning) Streams: Flexible module environment for the design and execution of data stream experiments Weka (machine learning) Vowpal Wabbit List of numerical analysis software == References == == External links == MOA Project home page at University of Waikato in New Zealand SAMOA Project home page at Yahoo Labs 
Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware. Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans. Machine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, and machine touch. == Machine Vision == Computer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Computer vision has many applications already in use today such as facial recognition, geographical modeling, and even aesthetic judgment. == Machine Hearing == Machine hearing also known as machine listening or computer audition is the ability of a computer or machine to take in and process sound data such as music or speech. This area has a wide range of application including music recording and compression, speech synthesis, and speech recognition. Many commonly used devices such as a smartphones, voice translators, and even cars make use of some form of machine hearing. == Machine Touch == Machine touch is an area of machine perception where tactile information is processed by a machine or computer. Applications include tactile perception of surface properties and dexterity whereby tactile information can enable intelligent reflexes and interaction with the environment. == See also == SLAM Robot == References == 
Mathematica is a symbolic mathematical computation program, sometimes called a computer algebra program, used in many scientific, engineering, mathematical, and computing fields. It was conceived by Stephen Wolfram and is developed by Wolfram Research of Champaign, Illinois. The Wolfram Language is the programming language used in Mathematica. == FeaturesEdit == Features of Mathematica include: Elementary and Special mathematical function libraries Matrix and data manipulation tools including support for sparse arrays Support for complex number, arbitrary precision, interval arithmetic and symbolic computation 2D and 3D data, function and geo visualization and animation tools Solvers for systems of equations, diophantine equations, ODEs, PDEs, DAEs, DDEs, SDEs and recurrence relations Numeric and symbolic tools for discrete and continuous calculus Multivariate statistics libraries including fitting, hypothesis testing, and probability and expectation calculations on over 140 distributions. Support for censored data, temporal data, time-series and unit based data Calculations and simulations on random processes and queues Machine learning tools for data, images and sounds Computational geometry in 2D, 3D and higher dimensions Finite element analysis including 2D and 3D adaptive mesh generation Constrained and unconstrained local and global optimization Programming language supporting procedural, functional and object oriented constructs Toolkit for adding user interfaces to calculations and applications Tools for 2D and 3D image processing and morphological image processing including image recognition Tools for visualizing and analysing directed and undirected graphs Tools for combinatoric problems Tools for text mining including regular expressions and semantic analysis Data mining tools such as cluster analysis, sequence alignment and pattern matching Number theory function library Tools for financial calculations including bonds, annuities, derivatives, options etc. Group theory and symbolic tensor functions Libraries for signal processing including wavelet analysis on sounds, images and data Linear and non-linear Control systems libraries Continuous and discrete integral transforms Import and export filters for data, images, video, sound, CAD, GIS, document and biomedical formats Database collection for mathematical, scientific, and socio-economic information and access to WolframAlpha data and computations Technical word processing including formula editing and automated report generating Tools for connecting to DLL, SQL, Java, .NET, C++, Fortran, CUDA, OpenCL, and http based systems Tools for parallel programming Using both "free-form linguistic input" (a natural language user interface) and Wolfram Language in notebook when connected to the Internet == InterfaceEdit == Mathematica is split into two parts, the kernel and the front end. The kernel interprets expressions (Wolfram Language code) and returns result expressions. Mathematica is a programming language that has evolved over several years of development The front end, designed by Theodore Gray, provides a GUI, which allows the creation and editing of Notebook documents containing program code with prettyprinting, formatted text together with results including typeset mathematics, graphics, GUI components, tables, and sounds. All content and formatting can be generated algorithmically or edited interactively. Most standard word processing capabilities are supported. It includes a spell-checker but does not spell check automatically as you type. Documents can be structured using a hierarchy of cells, which allow for outlining and sectioning of a document and support automatic numbering index creation. Documents can be presented in a slideshow environment for presentations. Notebooks and their contents are represented as Mathematica expressions that can be created, modified or analyzed by Mathematica programs. This allows conversion to other formats such as TeX or XML. The front end includes development tools such as a debugger, input completion and automatic syntax coloring. Among the alternative front ends is the Wolfram Workbench, an Eclipse based IDE, introduced in 2006. It provides project-based code development tools for Mathematica, including revision management, debugging, profiling, and testing. The Mathematica Kernel also includes a command line front end. Other interfaces include JMath, based on GNU readline and MASH which runs self-contained Mathematica programs (with arguments) from the UNIX command line. Wolfram Research has published a series of hands-on starter webcasts that introduce the user interface and the engine. == High-performance computingEdit == In recent years, the capabilities for high-performance computing have been extended with the introduction of packed arrays (version 4, 1999) and sparse matrices (version 5, 2003), and by adopting the GNU Multi-Precision Library to evaluate high-precision arithmetic. Version 5.2 (2005) added automatic multi-threading when computations are performed on multi-core computers. This release included CPU specific optimized libraries. In addition Mathematica is supported by third party specialist acceleration hardware such as ClearSpeed. In 2002, gridMathematica was introduced to allow user level parallel programming on heterogeneous clusters and multiprocessor systems and in 2008 parallel computing technology was included in all Mathematica licenses including support for grid technology such as Windows HPC Server 2008, Microsoft Compute Cluster Server and Sun Grid. Support for CUDA and OpenCL GPU hardware was added in 2010. Also, since version 8 it can generate C code, which is automatically compiled by a system C compiler, such as GCC or Visual Studio 2010. == DeploymentEdit == There are several ways to deploy applications written in Mathematica: Mathematica Player Pro is a runtime version of Mathematica that will run any Mathematica application but does not allow editing or creation of the code. A free-of-charge version, Wolfram CDF Player, is provided for running Mathematica programs that have been saved in the Computable Document Format (CDF). It can also view standard Mathematica files, but not run them. It includes plugins for common web browsers on Windows and Macintosh. webMathematica allows a web browser to act as a front end to a remote Mathematica server. It is designed to allow a user written application to be remotely accessed via a browser on any platform. It may not be used to give full access to Mathematica. Wolfram Language code can be converted to C code or to an automatically generated DLL. Wolfram Language code can be run on a Wolfram cloud service as a web-app or as an API == Connections with other applicationsEdit == Communication with other applications occurs through a protocol called MathLink. It allows communication between the Mathematica kernel and front-end, and also provides a general interface between the kernel and other applications. Wolfram Research freely distributes a developer kit for linking applications written in the C programming language to the Mathematica kernel through MathLink. Using J/Link., a Java program can ask Mathematica to perform computations; likewise, a Mathematica program can load Java classes, manipulate Java objects and perform method calls. Similar functionality is achieved with .NET /Link, but with .NET programs instead of Java programs. Other languages that connect to Mathematica include Haskell, AppleScript, Racket, Visual Basic, Python and Clojure. Links are available to many mathematical software packages including OpenOffice.org Calc, Microsoft Excel, MATLAB, R, Sage, SINGULAR, Wolfram SystemModeler, and Origin. Mathematical equations can be exchanged with other computational or typesetting software via MathML. Communication with SQL databases is achieved through built-in support for JDBC. Mathematica can also install web services from a WSDL description. It can access HDFS data via Hadoop. Mathematica can capture real-time data via a link to LabVIEW, from financial data feeds and directly from hardware devices via GPIB (IEEE 488), USB and serial interfaces. It automatically detects and reads from HID devices. == Computable dataEdit == Mathematica includes collections of curated data provided for use in computations. Mathematica is also integrated with Wolfram Alpha, an online service which provides additional data, some of which is kept updated in real time. Some of the data sets include astronomical, chemical, geopolitical, language, biomedical and weather data, in addition to mathematical data (such as knots and polyhedra). == Licensing and platform availabilityEdit == Mathematica is proprietary software licensed at a range of prices for commercial, educational, and other uses. Mathematica 10 is supported on various versions of Microsoft Windows (Vista, 7, 8 and 10), Apple's OS X, Linux, Raspbian and as an online service. All platforms are supported with 64-bit implementations. == Version historyEdit == Mathematica built on the ideas in Cole and Wolfram's earlier Symbolic Manipulation Program (SMP). The name of the program "Mathematica" was suggested to Stephen Wolfram by Apple co-founder Steve Jobs although Stephen Wolfram had thought about it earlier and rejected it. Wolfram Research has released the following versions of Mathematica: == See alsoEdit == Comparison of computer algebra systems Comparison of multi-paradigm programming languages Comparison of numerical analysis software Comparison of programming languages Comparison of regular expression engines Dynamic programming language Fourth-generation programming language Functional programming List of computer algebra systems List of computer simulation software List of graphing software Literate programming Mathematical markup language Mathematical software Wolfram Alpha, a web answer engine Wolfram Language Wolfram SystemModeler, a physical modeling and simulation tool which integrates with Mathematica == ReferencesEdit == == External linksEdit == Official website Mathematica Documentation Center Image identification website powered by Mathematica Wolfram Demonstrations Project Mathematica based demonstrations A little bit of Mathematica history documenting the growth of code base and number of functions over time 
Michael Irwin Jordan (born 1956) is an American scientist, Professor at the University of California, Berkeley and leading researcher in machine learning and artificial intelligence. == Biography == Jordan was born in Ponchatoula, Louisiana, to a working-class family, and received his BS magna cum laude in Psychology in 1978 from the Louisiana State University, his MS in Mathematics in 1980 from the Arizona State University and his PhD in Cognitive Science in 1985 from the University of California, San Diego. At the University of California, San Diego Jordan was a student of David Rumelhart and a member of the PDP Group in the 1980s. Jordan is currently a full professor at the University of California, Berkeley where his appointment is split across the Department of Statistics and the Department of EECS. He was a professor at MIT from 1988-1998. Jordan received numerous awards, including a best student paper award (with X. Nguyen and M. Wainwright) at the International Conference on Machine Learning (ICML 2004), a best paper award (with R. Jacobs) at the American Control Conference (ACC 1991), the ACM - AAAI Allen Newell Award, the IEEE Neural Networks Pioneer Award, and an NSF Presidential Young Investigator Award. In 2010 he was named a Fellow of the Association for Computing Machinery "for contributions to the theory and application of machine learning." It is notable that many of Jordan's graduate students and postdocs continue to strongly influence the machine learning field after their PhDs. Francis Bach, Zoubin Ghahramani, Tommi Jaakkola, Andrew Ng, Lawrence Saul and David Blei (all former students or postdocs of Jordan) have all continued to make significant contributions to the field. == Work == In the 1980s Jordan started developing recurrent neural networks as a cognitive model. In recent years, though, his work is less driven from a cognitive perspective and more from the background of traditional statistics. He popularised Bayesian networks in the machine learning community and is known for pointing out links between machine learning and statistics. Jordan was also prominent in the formalisation of variational methods for approximate inference and the popularisation of the expectation-maximization algorithm in machine learning. === Resignation from Machine Learning Journal === In 2001, Michael Jordan and others resigned from the Editorial Board of Machine Learning. In a public letter, they argued for less restrictive access; a new journal Journal of Machine Learning Research (JMLR) was created to support the evolution of the field of machine learning. == References == == External links == Homepage (at University of California, Berkeley) Published papers (chronological) 
Mlpy is a Python, open source, machine learning library built on top of NumPy/SciPy, the GNU Scientific Library and it makes an extensive use of the Cython language. mlpy provides a wide range of state-of-the-art machine learning methods for supervised and unsupervised problems and it is aimed at finding a reasonable compromise among modularity, maintainability, reproducibility, usability and efficiency. mlpy is multiplatform, it works with Python 2 and 3 and it is distributed under GPL3. Suited for general-purpose machine learning tasks, mlpy's motivating application field is bioinformatics, i.e. the analysis of high throughput omics data. == Features == Regression: least squares, ridge regression, least angle regression, elastic net, kernel ridge regression, support vector machines (SVM), partial least squares (PLS) Classification: linear discriminant analysis (LDA), Basic perceptron, Elastic Net, logistic regression, (Kernel) Support Vector Machines (SVM), Diagonal Linear Discriminant Analysis (DLDA), Golub Classifier, Parzen-based, (kernel) Fisher Discriminant Classifier, k-nearest neighbor, Iterative RELIEF, Classification Tree, Maximum Likelihood Classifier Clustering: hierarchical clustering, Memory-saving Hierarchical Clustering, k-means Dimensionality reduction: (Kernel) Fisher discriminant analysis (FDA), Spectral Regression Discriminant Analysis (SRDA), (kernel) Principal component analysis (PCA) Kernel-based functions are managed through a common kernel layer. In particular, the user can choose between supplying the data or a precomputed kernel in input space. Linear, polynomial, Gaussian, exponential and sigmoid kernels are available as default choices, and custom kernels can be defined as well. Many classification and regression algorithms are endowed with an internal feature ranking procedure: in alternative, mlpy implements the I-Relief algorithm. Recursive feature elimination (RFE) for linear classifiers and the KFDA-RFE algorithm are available for feature selection. Methods for feature list analysis (for example the Canberra stability indicator), data resampling and error evaluation are provided, together with different clustering analysis methods (Hierarchical, Memory-saving Hierarchical, k-means). Finally, dedicated submodules are included for longitudinal data analysis through wavelet transform (Continuous, Discrete and Undecimated) and dynamic programming algorithms (Dynamic Time Warping and variants). == See also == scikit-learn, an open source machine learning library for the Python programming language == References == == External links == mlpy: Machine Learning Python. http://arxiv.org/abs/1202.6548 Official website http://sourceforge.net/projects/mlpy 
A multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network. MLP is a modification of the standard linear perceptron and can distinguish data that are not linearly separable. == Theory == === Activation function === If a multilayer perceptron has a linear activation function in all neurons, that is, a linear function that maps the weighted inputs to the output of each neuron, then it is easily proved with linear algebra that any number of layers can be reduced to the standard two-layer input-output model (see perceptron). What makes a multilayer perceptron different is that some neurons use a nonlinear activation function which was developed to model the frequency of action potentials, or firing, of biological neurons in the brain. This function is modeled in several ways. The two main activation functions used in current applications are both sigmoids, and are described by , in which the former function is a hyperbolic tangent which ranges from -1 to 1, and the latter, the logistic function, is similar in shape but ranges from 0 to 1. Here is the output of the th node (neuron) and is the weighted sum of the input synapses. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions which are used in another class of supervised neural network models. === Layers === The multilayer perceptron consists of three or more layers (an input and an output layer with one or more hidden layers) of nonlinearly-activating nodes and is thus considered a deep neural network. Since an MLP is a Fully Connected Network, each node in one layer connects with a certain weight to every node in the following layer. Some people do not include the input layer when counting the number of layers and there is disagreement about whether should be interpreted as the weight from i to j or the other way around. === Learning through backpropagation === Learning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron. We represent the error in output node in the th data point (training example) by , where is the target value and is the value produced by the perceptron. We then make corrections to the weights of the nodes based on those corrections which minimize the error in the entire output, given by . Using gradient descent, we find our change in each weight to be where is the output of the previous neuron and is the learning rate, which is carefully selected to ensure that the weights converge to a response fast enough, without producing oscillations. In programming applications, this parameter typically ranges from 0.2 to 0.8. The derivative to be calculated depends on the induced local field , which itself varies. It is easy to prove that for an output node this derivative can be simplified to where is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is . This depends on the change in weights of the th nodes, which represent the output layer. So to change the hidden layer weights, we must first change the output layer weights according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function. == Terminology == The term "multilayer perceptron" often causes confusion. It is argued the model is not a single perceptron that has multiple layers. Rather, it contains many perceptrons that are organised into layers, leading some to believe that a more fitting term might therefore be "multilayer perceptron network". Moreover, these "perceptrons" are not really perceptrons in the strictest possible sense, as true perceptrons are a special case of artificial neurons that use a threshold activation function such as the Heaviside step function, whereas the artificial neurons in a multilayer perceptron are free to take on any arbitrary activation function. Consequently, whereas a true perceptron performs binary classification, a neuron in a multilayer perceptron is free to either perform classification or regression, depending upon its activation function. The two arguments raised above can be reconciled with the name "multilayer perceptron" if "perceptron" is simply interpreted to mean a binary classifier, independent of the specific mechanistic implementation of a classical perceptron. In this case, the entire network can indeed be considered to be a binary classifier with multiple layers. Furthermore, the term "multilayer perceptron" now does not specify the nature of the layers; the layers are free to be composed of general artificial neurons, and not perceptrons specifically. This interpretation of the term "multilayer perceptron" avoids the loosening of the definition of "perceptron" to mean an artificial neuron in general. == Applications == Multilayer perceptrons using a backpropagation algorithm are the standard algorithm for any supervised learning pattern recognition process and the subject of ongoing research in computational neuroscience and parallel distributed processing. They are useful in research in terms of their ability to solve problems stochastically, which often allows one to get approximate solutions for extremely complex problems like fitness approximation. MLPs were a popular machine learning solution in the 1980s, finding applications in diverse fields such as speech recognition, image recognition, and machine translation software, but have since the 1990s faced strong competition from the much simpler (and related) support vector machines. More recently, there has been some renewed interest in backpropagation networks due to the successes of deep learning and Border pairs method. == References == == External links == A Gentle Introduction to Backpropagation - An intuitive tutorial by Shashi Sathyanarayana This is an updated PDF version of a blog article that was previously linked here. This article contains pseudocode ("Training Wheels for Training Neural Networks") for implementing the algorithm. Weka: Open source data mining software with multilayer perceptron implementation. Border pairs method description. This is new, constructive, non iterative learning algorithm with many advantages over backpropagation algorithm. 
Multilinear subspace learning (MSL) aims to learn a specific small part of a large space of multidimensional objects having a particular desired property. It is a dimensionality reduction approach for finding a low-dimensional representation with certain preferred characteristics of high-dimensional tensor data through direct mapping, without going through vectorization. The term tensor in MSL refers to multidimensional arrays. Examples of tensor data include images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D). The mapping from a high-dimensional tensor space to a low-dimensional tensor space or vector space is named as multilinear projection. MSL methods are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA). In the literature, MSL is also referred to as tensor subspace learning or tensor subspace analysis. Research on MSL has progressed from heuristic exploration in 2000s (decade) to systematic investigation in 2010s. == Background == With the advances in data acquisition and storage technology, big data (or massive data sets) are being generated on a daily basis in a wide range of emerging applications. Most of these big data are multidimensional. Moreover, they are usually very-high-dimensional, with a large amount of redundancy, and only occupying a part of the input space. Therefore, dimensionality reduction is frequently employed to map high-dimensional data to a low-dimensional space while retaining as much information as possible. Linear subspace learning algorithms are traditional dimensionality reduction techniques that represent input data as vectors and solve for an optimal linear mapping to a lower-dimensional space. Unfortunately, they often become inadequate when dealing with massive multidimensional data. They result in very-high-dimensional vectors, lead to the estimation of a large number of parameters, and also break the natural structure and correlation in the original data. MSL is closely related to tensor decompositions. They both employ multilinear algebra tools. The difference is that tensor decomposition focuses on factor analysis, while MSL focuses on dimensionality reduction. MSL belongs to tensor-based computation and it can be seen as a tensor-level computational thinking of machine learning. == Multilinear projection == A multilinear subspace is defined through a multilinear projection that maps the (high-dimensional) input tensor space to a (lower-dimensional) vector/tensor space (the subspace). The original idea is due to Hitchcock in 1927. === Tensor-to-tensor projection (TTP) === A TTP is a direct projection of a high-dimensional tensor to a low-dimensional tensor of the same order, using N projection matrices for an Nth-order tensor. It can be performed in N steps with each step performing a tensor-matrix multiplication (product). The N steps are exchangeable. This projection is an extension of the higher-order singular value decomposition (HOSVD) to subspace learning. Hence, its origin is traced back to the Tucker decomposition in 1960s. === Tensor-to-vector projection (TVP) === A TVP is a direct projection of a high-dimensional tensor to a low-dimensional vector, which is also referred to as the rank-one projections. As TVP projects a tensor to a vector, it can be viewed as multiple projections from a tensor to a scalar. Thus, the TVP of a tensor to a P-dimensional vector consists of P projections from the tensor to a scalar. The projection from a tensor to a scalar is an elementary multilinear projection (EMP). In EMP, a tensor is projected to a point through N unit projection vectors. It is the projection of a tensor on a single line (resulting a scalar), with one projection vector in each mode. Thus, the TVP of a tensor object to a vector in a P-dimensional vector space consists of P EMPs. This projection is an extension of the canonical decomposition, also known as the parallel factors (PARAFAC) decomposition. == Typical approach in MSL == There are N sets of parameters to be solved, one in each mode. The solution to one set often depends on the other sets (except when N=1, the linear case). Therefore, the suboptimal iterative procedure in is followed. Initialization of the projections in each mode For each mode, fixing the projection in all the other mode, and solve for the projection in the current mode. Do the mode-wise optimization for a few iterations or until convergence. This is originated from the alternating least square method for multi-way data analysis. == Pros and cons == The advantages of MSL are: It preserves the structure and correlation in the original data before projection by operating on natural tensorial representation of multidimensional data. It can learn more compact representations than its linear counterpart. It needs to estimate a much smaller number of parameters and it has fewer problems in the small sample size scenario. It can handle big tensor data more efficiently with computations in much lower dimensions than linear methods. Thus, it leads to lower demand on computational resources. The disadvantages of MSL are: Most MSL algorithm are iterative. They may be affected by initialization method and have convergence problem. The solution obtained is local optimum. == Algorithms == Multilinear extension of PCA TTP-based: Multilinear Principal Component Analysis (MPCA) TVP-based: Uncorrelated Multilinear Principal Component Analysis (UMPCA) Multilinear extension of LDA TTP-based: Discriminant Analysis with Tensor Representation (DATER) TTP-based: General tensor discriminant analysis (GTDA) TVP-based: Uncorrelated Multilinear Discriminant Analysis (UMLDA) Multilinear extension of CCA TTP-based: Tensor Canonical Correlation Analysis (TCCA) TVP-based: Multilinear Canonical Correlation Analysis (MCCA) == Pedagogical resources == Survey: A survey of multilinear subspace learning for tensor data (open access version). Lecture: Video lecture on UMPCA at the 25th International Conference on Machine Learning (ICML 2008). == Code == MATLAB Tensor Toolbox by Sandia National Laboratories. The MPCA algorithm written in Matlab (MPCA+LDA included). The UMPCA algorithm written in Matlab (data included). The UMLDA algorithm written in Matlab (data included). == Tensor data sets == 3D gait data (third-order tensors): 128x88x20(21.2M); 64x44x20(9.9M); 32x22x10(3.2M); == See also == CP decomposition Dimension reduction Multilinear algebra Tensor Tensor decomposition Tensor software Tucker decomposition == References == 
Mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search. The classic example of a mutation operator involves a probability that an arbitrary bit in a genetic sequence will be changed from its original state. A common method of implementing the mutation operator involves generating a random variable for each bit in a sequence. This random variable tells whether or not a particular bit will be modified. This mutation procedure, based on the biological point mutation, is called single point mutation. Other types are inversion and floating point mutation. When the gene encoding is restrictive as in permutation problems, mutations are swaps, inversions, and scrambles. The purpose of mutation in GAs is preserving and introducing diversity. Mutation should allow the algorithm to avoid local minima by preventing the population of chromosomes from becoming too similar to each other, thus slowing or even stopping evolution. This reasoning also explains the fact that most GA systems avoid only taking the fittest of the population in generating the next but rather a random (or semi-random) selection with a weighting toward those that are fitter. For different genome types, different mutation types are suitable: Bit string mutation The mutation of bit strings ensue through bit flips at random positions. Example: The probability of a mutation of a bit is , where is the length of the binary vector. Thus, a mutation rate of per mutation and individual selected for mutation is reached. Flip Bit This mutation operator takes the chosen genome and inverts the bits (i.e. if the genome bit is 1, it is changed to 0 and vice versa). Boundary This mutation operator replaces the genome with either lower or upper bound randomly. This can be used for integer and float genes. Non-Uniform The probability that amount of mutation will go to 0 with the next generation is increased by using non-uniform mutation operator. It keeps the population from stagnating in the early stages of the evolution. It tunes solution in later stages of evolution. This mutation operator can only be used for integer and float genes. Uniform This operator replaces the value of the chosen gene with a uniform random value selected between the user-specified upper and lower bounds for that gene. This mutation operator can only be used for integer and float genes. Gaussian This operator adds a unit Gaussian distributed random value to the chosen gene. If it falls outside of the user-specified lower or upper bounds for that gene, the new gene value is clipped. This mutation operator can only be used for integer and float genes. == See also == Genetic algorithms == References == == Bibliography == John Holland, Adaptation in Natural and Artificial Systems, University of Michigan Press, Ann Arbor, Michigan. 1975. ISBN 0-262-58111-6. 
Neural Designer is a software tool for data mining based on machine learning techniques, a main area of artificial intelligence research. It has been developed from the open source library OpenNN, and contains a graphical user interface which simplifies data entry and interpretation of results. In 2014, this computer program was selected by the prestigious magazine Predictive Analytics Today among the top proprietary software for data mining. Also, during the same year, Big Data Analytics Today selected Neural Designer as one of the best brain inspired artificial intelligence projects. In 2015, Neural Designer has been chosen by the European Commission, within the Horizon 2020 program, as a disruptive technology in the ICT field. == Features == Neural Designer is a general predictive analytics software. It implements deep learning architectures with multiple non-linear layers and contains utilities to solve function regression and pattern recognition problems. The input to Neural Designer is a data set, and the output from it is a predictive model. That result takes the form of an explicit mathematical expression, which can be exported to any computer language or system. == Related tools == Weka: free machine learning and data mining software. RapidMiner: free and commercial machine learning framework implemented in Java. KNIME: free and commercial machine learning and data mining software. == See also == Artificial intelligence Artificial neural network Comparison of deep learning software Data mining Deep learning Machine learning Predictive analytics == References == 
In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set. Occam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability. == Introduction == Occam Learning is named after Occam's razor, which is a principle stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a formal and mathematical justification for this principle. It was first shown by Blumer, et al. that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, parsimony (of the output hypothesis) implies predictive power. == Definition of Occam learning == The succinctness of a concept in concept class can be expressed by the length of the shortest bit string that can represent in . Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data. Let and be concept classes containing target concepts and hypotheses respectively. Then, for constants and , a learning algorithm is an -Occam algorithm for using if, given a set of samples labeled according to a concept , outputs a hypothesis such that is consistent with on (that is, ), and where is the maximum length of any sample . An Occam algorithm is called efficient if it runs in time polynomial in , , and . We say a concept class is Occam learnable with respect to a hypothesis class if there exists an efficient Occam algorithm for using . == The relation between Occam and PAC learning == Occam learnability implies PAC learnability, as the following theorem of Blumer, et al. shows: === Theorem (Occam learning implies PAC learning) === Let be an efficient -Occam algorithm for using . Then there exists a constant such that for any , for any distribution , given samples drawn from and labelled according to a concept of length bits each, the algorithm will output a hypothesis such that with probability at least . Here, is with respect to the concept and distribution . This implies that the algorithm is also a PAC learner for the concept class using hypothesis class . A slightly more general formulation is as follows: === Theorem (Occam learning implies PAC learning, cardinality version) === Let . Let be an algorithm such that, given samples drawn from a fixed but unknown distribution and labeled according to a concept of length bits each, outputs a hypothesis that is consistent with the labeled samples. Then, there exists a constant such that if , then is guaranteed to output a hypothesis such that with probability at least . While the above theorems show that Occam learning is sufficient for PAC learning, it doesn't say anything about necessity. Board and Pitt show that, for a wide variety of concept classes, Occam learning is in fact necessary for PAC learning. They proved that for any concept class that is polynomially closed under exception lists, PAC learnability implies the existence of an Occam algorithm for that concept class. Concept classes that are polynomially closed under exception lists include Boolean formulas, circuits, deterministic finite automata, decision-lists, decision-trees, and other geometrically-defined concept classes. A concept class is polynomially closed under exception lists if there exists a polynomial-time algorithm such that, when given the representation of a concept and a finite list of exceptions, outputs a representation of a concept such that the concepts and agree except on the set . == Proof that Occam learning implies PAC learning == We first prove the Cardinality version. Call a hypothesis bad if , where again is with respect to the true concept and the underlying distribution . The probability that a set of samples is consistent with is at most , by the independence of the samples. By the union bound, the probability that there exists a bad hypothesis in is at most , which is less than if . This concludes the proof of the second theorem above. Using the second theorem, we can prove the first theorem. Since we have a -Occam algorithm, this means that any hypothesis output by can be represented by at most bits, and thus . This is less than if we set for some constant . Thus, by the Cardinality version Theorem, will output a consistent hypothesis with probability at least . This concludes the proof of the first theorem above. == Improving sample complexity for common problems == Though Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, conjunctions with few relevant variables, and decision lists. == Extensions == Occam algorithms have also been shown to be successful for PAC learning in the presence of errors, probabilistic concepts, function learning and Markovian non-independent examples. == See also == Structural Risk Minimization Computational learning theory == References == 
OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open source, licensed under the GNU Lesser General Public License. == Characteristics == The software implements any number of layers of non-linear processing units for supervised learning. This deep architecture allows the design of neural networks with universal approximation properties. On the other hand, it allows multiprocessing programming by means of OpenMP, in order to increase the computer performance. OpenNN contains data mining algorithms as a bundle of functions. These can be embedded in other software tools, using an application programming interface, for the integration of the predictive analytics tasks. In this regard, a graphical user interface is missing but some functions can be supported by specific visualization tools. == History == The development started in 2003 at the International Center for Numerical Methods in Engineering (CIMNE), within the research project funded by the European Union called RAMFLOOD. Then it continued as part of similar projects. At present, OpenNN is being developed by the startup company Artelnics. In 2014, Big Data Analytics Today rated OpenNN as the #1 brain inspired artificial intelligence project. Also, during the same year, ToppersWorld selected OpenNN among the top 5 open source data mining tools. == Applications == OpenNN is a general purpose artificial intelligence software package. It uses machine learning techniques for solving data mining and predictive analytics tasks in different fields. For instance, the library has been applied in the engineering, energy, or chemistry sectors. == Related libraries == Deeplearning4j, a deep learning library written for Java and Scala which is open source. Torch, an open source framework written in Lua with wide support for machine learning algorithms. == See also == Comparison of deep learning frameworks Neural Designer, also developed by Artelnics Artificial intelligence Machine learning Deep learning Artificial neural network == References == == External links == OpenNN project at GitHub OpenNN project at SourceForge 
Peter Norvig (born December 14, 1956) is an American computer scientist. He is a Director of Research (formerly Director of Search Quality) at Google Inc. == Educational background == He is a Fellow and Councilor of the Association for the Advancement of Artificial Intelligence and co-author, with Stuart Russell, of Artificial Intelligence: A Modern Approach, now the leading college text in the field. He previously was head of the Computational Sciences Division (now the Intelligent Systems Division) at NASA Ames Research Center, where he oversaw a staff of 200 scientists performing NASA's research and development in autonomy and robotics, automated software engineering and data analysis, neuroengineering, collaborative systems research, and simulation-based decision-making. Before that he was Chief Scientist at Junglee, where he helped develop one of the first Internet comparison shopping services; Chief designer at Harlequin Inc.; and Senior Scientist at Sun Microsystems Laboratories. Norvig received a Bachelor of Science in Applied Mathematics from Brown University and a Ph.D. in Computer Science from the University of California, Berkeley. == Research == Norvig has been an Assistant Professor at the University of Southern California and a Research Faculty Member at Berkeley. He has over fifty publications in various areas of Computer Science, concentrating on artificial intelligence, natural language processing, information retrieval and software engineering including the books Artificial Intelligence: A Modern Approach, Paradigms of AI Programming: Case Studies in Common Lisp, Verbmobil: A Translation System for Face-to-Face Dialog, and Intelligent Help Systems for UNIX. Norvig is one of the creators of JScheme. In 2006 he was inducted as a Fellow of the Association for Computing Machinery. Norvig is listed under "Academic Faculty & Advisors" for the Singularity University. In 2011, Norvig worked with Sebastian Thrun to develop a popular online course in Artificial Intelligence that had more than 160,000 students enrolled. He also teaches an online course via the Udacity platform. He believes that a teaching revolution, fostered by computer tools, is pending. == Non-academic writing == In 2001, Norvig published a short article titled "Teach Yourself Programming in Ten Years", arguing against the fashionable introductory programming textbooks that purported to teach programming in days or weeks. The article was widely shared and discussed, and has attracted contributed translations to over 20 languages. Norvig is also known for his "Gettysburg Powerpoint Presentation", a satire about bad presentation practices using Abraham Lincoln's famous Gettysburg Address. == References == == External links == The Prospects for AI, featuring Neil Jacobstein, Patrick Lincoln, Peter Norvig, and Bruno Olshausen An experiment by Norvig on Scientific opinion on climate change Teach Yourself Programming in Ten Years 
In computational learning theory, probably approximately correct learning (PAC learning) is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant. In this framework, the learner receives samples and must select a generalization function (called the hypothesis) from a certain class of possible functions. The goal is that, with high probability (the "probably" part), the selected function will have low generalization error (the "approximately correct" part). The learner must be able to learn the concept given any arbitrary approximation ratio, probability of success, or distribution of the samples. The model was later extended to treat noise (misclassified samples). An important innovation of the PAC framework is the introduction of computational complexity theory concepts to machine learning. In particular, the learner is expected to find efficient functions (time and space requirements bounded to a polynomial of the example size), and the learner itself must implement an efficient procedure (requiring an example count bounded to a polynomial of the concept size, modified by the approximation and likelihood bounds). == Definitions and terminology == In order to give the definition for something that is PAC-learnable, we first have to introduce some terminology. For the following definitions, two examples will be used. The first is the problem of character recognition given an array of bits encoding a binary-valued image. The other example is the problem of finding an interval that will correctly classify points within the interval as positive and the points outside of the range as negative. Let be a set called the instance space or the encoding of all the samples, and each instance have length assigned. In the character recognition problem, the instance space is . In the interval problem the instance space is , where denotes the set of all real numbers. A concept is a subset . One concept is the set of all patterns of bits in that encode a picture of the letter "P". An example concept from the second example is the set of all of the numbers between and . A concept class is a set of concepts over . This could be the set of all subsets of the array of bits that are skeletonized 4-connected (width of the font is 1). Let be a procedure that draws an example, , using a probability distribution and gives the correct label , that is 1 if and 0 otherwise. Say that there is an algorithm that given access to and inputs and that, with probability of at least , outputs a hypothesis that has error less than or equal to with examples drawn from with the distribution . If there is such an algorithm for every concept , for every distribution over , and for all and then is PAC learnable (or distribution-free PAC learnable). We can also say that is a PAC learning algorithm for . An algorithm runs in time if it draws at most examples and requires at most time steps. A concept class is efficiently PAC learnable if it is PAC learnable by an algorithm that runs in time polynomial in , and instance length. Depending on the error tolerance of the algorithm, learning is possible also in scenarios when data are corrupted or maliciously adulterated. == Equivalence == Under some regularity conditions these three conditions are equivalent: The concept class C is PAC learnable. The VC dimension of C is finite. C is a uniform Glivenko-Cantelli class. == See also == Machine learning Data mining Error Tolerance (PAC learning) == References == ^ L. Valiant. A theory of the learnable. Communications of the ACM, 27, 1984. ^ Kearns and Vazirani, pg. 1-12, ^ Balas Kausik Natarajan, Machine Learning , A Theoretical Approach, Morgan Kaufmann Publishers, 1991 == Further reading == M. Kearns, U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994. A textbook. D. Haussler. Overview of the Probably Approximately Correct (PAC) Learning Framework. An introduction to the topic. L. Valiant. Probably Approximately Correct. Basic Books, 2013. In which Valiant argues that PAC learning describes how organisms evolve and learn. 
In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification. The RVM has an identical functional form to the support vector machine, but provides probabilistic classification. It is actually equivalent to a Gaussian process model with covariance function: where is the kernel function (usually Gaussian),'s as the variances of the prior on the weight vector ,and are the input vectors of the training set. Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem). The relevance vector machine is patented in the United States by Microsoft. == See also == Kernel trick Platt scaling: turns an SVM into a probability model == References == == Software == dlib C++ Library The Kernel-Machine Library rvmbinary:R package for binary classification scikit-rvm fast-scikit-rvm , rvm tutorial == External links == Tipping's webpage on Sparse Bayesian Models and the RVM A Tutorial on RVM by Tristan Fletcher Applied tutorial on RVM Comparison of RVM and SVM 
In machine learning, feature learning or representation learning is a set of techniques that learn a feature: a transformation of raw data input to a representation that can be effectively exploited in machine learning tasks. This obviates manual feature engineering, which is otherwise necessary, and allows a machine to both learn at a specific task (using the features) and learn the features themselves: to learn how to learn. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor measurement is usually complex, redundant, and highly variable. Thus, it is necessary to discover useful features or representations from raw data. Traditional hand-crafted features often require expensive human labor and often rely on expert knowledge. Also, they normally do not generalize well. This motivates the design of efficient feature learning techniques, to automate and generalize this. Feature learning can be divided into two categories: supervised and unsupervised feature learning, analogous to these categories in machine learning generally. In supervised feature learning, features are learned with labeled input data. Examples include neural networks, multilayer perceptron, and (supervised) dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization, and various forms of clustering. == Supervised feature learning == Supervised feature learning is to learn features from labeled data. Several approaches are introduced in the following. === Supervised dictionary learning === Dictionary learning is to learn a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error (over the input data), together with L1 regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights). Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, a supervised dictionary learning technique was proposed by Mairal et al. in 2009. The authors apply dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse representation of data), and an L2 regularization on the parameters of the classifier. === Neural networks === Neural networks are used to illustrate a family of learning algorithms via a "network" consisting of multiple layers of inter-connected nodes. It is inspired by the nervous system, where the nodes are viewed as neurons and edges are viewed as synapse. Each edge has an associated weight, and the network defines computational rules that passes input data from the input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights). Multilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. == Unsupervised feature learning == Unsupervised feature learning is to learn features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where first, features are learned from an unlabeled dataset, which are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following. === K-means clustering === K-means clustering is an approach for vector quantization. In particular, given a set of n vectors, k-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, and suboptimal greedy algorithms have been developed for k-means clustering. In feature learning, k-means clustering can be used to group an unlabeled set of inputs into k clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest way is to add k binary features to each sample, where each feature j has value one iff the jth centroid learned by k-means is the closest to the sample under consideration. It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has used to train RBF networks). Coates and Ng note that certain variants of k-means behave similarly to sparse coding algorithms. In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that k-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task. K-means has also been shown to improve performance in the domain of NLP, specifically for named-entity recognition; there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings). === Principal component analysis === Principal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of n input data vectors, PCA generates p (which is much smaller than the dimension of the input data) right singular vectors corresponding to the p largest singular values of the data matrix, where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors. These p singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations. PCA is a linear feature learning approach since the p singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the data matrix on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix. PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case in many applications. PCA only relies on orthogonal transformations of the original data, and it only exploits the first- and second-order moments of the data, which may not well characterize the distribution of the data. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues). === Local linear embedding === Local linear embedding (LLE) is a nonlinear unsupervised learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Sam T. Roweis and Lawrence K. Saul in 2000. The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set. LLE consists of two major steps. The first step is for "neighbor-preserving," where each input data point Xi is reconstructed as a weighted sum of K nearest neighboring data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between a point and its reconstruction) under the constraint that the weights associated to each point sum up to one. The second step is for "dimension reduction," by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with data being fixed, which can be solved as a least squares problem; while in the second step, lower-dimensional points are optimized with the weights being fixed, which can be solved via sparse eigenvalue decomposition. The reconstruction weights obtained in the first step captures the "intrinsic geometric properties" of a neighborhood in the input data. It is assumed that original data lie on a smooth lower-dimensional manifold, and the "intrinsic geometric properties" captured by the weights of the original data are expected also on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying structure of data. === Independent component analysis === Independent component analysis (ICA) is technique for learning a representation of data using a weighted sum of independent non-Gaussian components. The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution. === Unsupervised dictionary learning === Different from supervised dictionary learning, unsupervised dictionary learning does not utilize the labels of the data and only exploits the structure underlying the data for optimizing the dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionary, where the number of dictionary elements is larger than the dimension of the input data. Aharon et al. proposed an algorithm known as K-SVD for learning from unlabeled input data a dictionary of elements that enables sparse representation of the data. == Multilayer/Deep architectures == The hierarchical architecture of the neural system inspires deep learning architectures for feature learning by stacking multiple layers of simple learning blocks. These architectures are often designed based on the assumption of distributed representation: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by previous level as input, and produces new representations as output, which is then fed to higher levels. The input of bottom layer is the raw data, and the output of the final layer is the final low-dimensional feature or representation. === Restricted Boltzmann machine === Restricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures. An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general Boltzmann machines with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an energy function, based on which a joint distribution of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent conditioned on the visible (hidden) variables. Such conditional independence facilitates computations on RBM. An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using the contrastive divergence (CD) algorithm by Geoffrey Hinton. In general, the training of RBM by solving the above maximization problem tends to result in non-sparse representations. The sparse RBM, a modification of the RBM, was proposed to enable sparse representations. The idea is to add a regularization term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant . === Autoencoder === An autoencoder consisting of encoder and decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov where the encoder uses raw data (e.g., image) as input and produces feature or representation as output, and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a greedy layer-by-layer manner: after one layer of feature detectors is learned, they are fed to upper layers as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with stochastic gradient descent methods. Training can be repeated until some stopping criteria is satisfied. == See also == Basis function Deep learning Feature detection (computer vision) Feature extraction Kernel trick Vector quantization == References == 
Robert Tibshirani (born July 10, 1956) is a Professor in the Departments of Statistics and Health Research and Policy at Stanford University. He was a Professor at the University of Toronto from 1985 to 1998. In his work, he develops statistical tools for the analysis of complex datasets, most recently in genomics and proteomics. His most well-known contributions are the LASSO method, which proposed the use of L1 penalization in regression and related problems, and Significance Analysis of Microarrays. He has also co-authored three well-known books: "Generalized Additive Models", "An Introduction to the Bootstrap", and "The Elements of Statistical Learning", the last of which is available for free from the author's website. == Biography == Tibshirani was born on 10 July 1956 in Niagara Falls, Ontario, Canada. He received his B. Math. in statistics and computer science from the University of Waterloo in 1979 and a Master's degree in Statistics from University of Toronto in 1980. Tibshirani joined the doctoral program at Stanford University in 1981 and received his Ph.D. in 1984 under the supervision of Bradley Efron. His dissertation was entitled "Local likelihood estimation". His son, Ryan Tibshirani, with whom he occasionally publishes scientific papers, is currently an Assistant Professor at Carnegie Mellon University in the department of Statistics, jointly in the department of the Machine Learning. == Honors and awards == Tibshirani received the COPSS Presidents' Award in 1996. Given jointly by the world's leading statistical societies, the award recognizes outstanding contributions to statistics by a statistician under the age of 40. He is a fellow of the Institute of Mathematical Statistics, the American Statistical Association, and a (Canadian) Steacie award winner. He was elected a Fellow of the Royal Society of Canada in 2001 and a member of the National Academy of Sciences in 2012. Tibshirani was made the 2012 Statistical Society of Canada's Gold Medalist at their yearly meeting in Guelph, Ontario for "exceptional contributions to methodology and theory for the analysis of complex data sets, smoothing and regression methodology, statistical learning, and classification, and application areas that include public health, genomics, and proteomics". He gave his Gold Medal Address at the 2013 meeting in Edmonton. == References == 
Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives). Example of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion, grasping, active object categorization, as well as interactive skills such as joint manipulation of an object with a human peer, and linguistic skills such as the grounded and situated meaning of human language. Learning can happen either through autonomous self-exploration or through guidance from a human teacher, like for example in robot learning by imitation. Robot learning can be closely related to adaptive control, reinforcement learning as well as developmental robotics which considers the problem of autonomous lifelong acquisition of repertoires of skills. While machine learning is frequently used by computer vision algorithms employed in the context of robotics, these applications are usually not referred to as "robot learning". == See also == Machine Learning Robotics Developmental robotics Cognitive robotics Evolutionary robotics Institute of Robotics in Scandinavia AB == External links == IEEE RAS Technical Committee on Robot Learning (official IEEE website) IEEE RAS Technical Committee on Robot Learning (TC members website) Robot Learning at the Max Planck Institute for Intelligent Systems and the Technical University Darmstadt Robot Learning at the Computational Learning and Motor Control lab Humanoid Robot Learning at the Advanced Telecommunication Research Center (ATR) (English) (Japanese) Learning Algorithms and Systems Laboratory at EPFL (LASA) Robot Learning at the Cognitive Robotics Lab of Juergen Schmidhuber at IDSIA and Technical University of Munich The Humanoid Project: Peter Nordin, Chalmers University of Technology Inria and Ensta ParisTech FLOWERS team, France: Autonomous lifelong learning in developmental robotics CITEC at University of Bielefeld, Germany Asada Laboratory, Department of Adaptive Machine Systems, Graduate School of Engineering, Osaka University, Japan The Laboratory for Perceptual Robotics, University of Massachusetts Amherst Amherst, USA Centre for Robotics and Neural Systems, Plymouth University Plymouth, United Kingdom Robot Learning Lab at Carnegie Mellon University Project Learning Humanoid Robots at University of Bonn Skilligent Robot Learning and Behavior Coordination System (commercial product) Robot Learning class at Cornell University Robot Learning and Interaction Lab at Italian Institute of Technology Reinforcement learning for robotics at Delft University of Technology 
scikit-learn (formerly scikits.learn) is an open source machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. == Overview == The scikit-learn project started as scikits.learn, a Google Summer of Code project by David Cournapeau. Its name stems from the notion that it is a "SciKit" (SciPy Toolkit), a separately-developed and distributed third-party extension to SciPy. The original codebase was later rewritten by other developers. Of the various scikits, scikit-learn as well as scikit-image were described as "well-maintained and popular" in November 2012. As of 2015, scikit-learn is under active development and is sponsored by INRIA, Telecom ParisTech and occasionally Google (through the Google Summer of Code). The scikit-learn API has been adopted by wise.io, who offer a proprietary implementation of random forests called wiseRF. wise.io's business partner Continuum IO claimed data throughput of up to 7.5 times that of scikit-learn's implementation; since then, the scikit-learn developers claim to have optimized their implementation to be competitive with wise.io's, except in terms of memory use. == Implementation == scikit-learn is largely written in Python, with some core algorithms written in Cython to achieve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. == See also == mlpy Orange NLTK == References == == External links == Official website scikit-learn on GitHub 
Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn from examples a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems and face verification. == Learning setup == There are three common setups for similarity and metric distance learning. Regression similarity learning. In this setup, pairs of objects are given together with a measure of their similarity . The goal is to learn a function that approximates for every new labeled triplet example . This is typically achieved by minimizing a regularized loss . Classification similarity learning. Given are pairs of similar objects and non similar objects . An equivalent formulation is that every pair is given together with a binary label that determines if the two objects are similar or not. The goal is again to learn a classifier that can decide if a new pair of objects is similar or not. Ranking similarity learning. Given are triplets of objects whose relative similarity obey a predefined order: is known to be more similar to than to . The goal is to learn a function such that for any new triplet of objects , it obeys . This setup assumes a weaker form of supervision than in regression, because instead of providing an exact measure of similarity, one only has to provide the relative order of similarity. For this reason, ranking-based similarity learning is easier to apply in real large scale applications. A common approach for learning similarity, is to model the similarity function as a bilinear form. For example, in the case of ranking similarity learning, one aims to learn a matrix W that parametrizes the similarity function . == Metric learning == Similarity learning is closely related to distance metric learning. Metric learning is the task of learning a distance function over objects. A metric or distance function has to obey four axioms: non-negativity, Identity of indiscernibles, symmetry and subadditivity / triangle inequality. In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric. When the objects are vectors in , then any matrix in the symmetric positive semi-definite cone defines a distance pseudo-metric of the space of x through the form . When is a symmetric positive definite matrix, is a metric. Moreover, as any symmetric positive semi-definite matrix can be decomposed as where and , the distance function can be rewritten equivalently . The distance corresponds to the Euclidean distance between the projected feature vectors and . Some well-known approaches for metric learning include Large margin nearest neighbor , Information theoretic metric learning (ITML). In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance. == Applications == Similarity learning is used in information retrieval for learning to rank, in face verification or face identification, and in recommendation systems. Also, many machine learning approaches rely on some metric. This includes unsupervised learning such as clustering, which groups together close or similar objects. It also includes supervised approaches like K-nearest neighbor algorithm which rely on labels of nearby objects to decide on the label of a new object. Metric learning has been proposed as a preprocessing step for many of these approaches . == Scalability == Metric and similarity learning naively scale quadraticly with the dimension of the input space, as can easily see when the learned metric has a bilinear form . Scaling to higher dimensions can be achieved by enforcing a sparseness structure over the matrix model, as done with HDSL, and with COMET . == Further reading == For further information on this topic, see the surveys on metric and similarity learning by Bellet et al. and Kulis. == References == 
Email filtering is the processing of email to organize it according to specified criteria. Most often this refers to the automatic processing of incoming messages, but the term also applies to the intervention of human intelligence in addition to anti-spam techniques, and to outgoing emails as well as those being received. Email filtering software inputs email. For its output, it might pass the message through unchanged for delivery to the user's mailbox, redirect the message for delivery elsewhere, or even throw the message away. Some mail filters are able to edit messages during processing. == MotivationEdit == Common uses for mail filters include organizing incoming email and removal of spam and computer viruses. A less common use is to inspect outgoing email at some companies to ensure that employees comply with appropriate laws. Users might also employ a mail filter to prioritize messages, and to sort them into folders based on subject matter or other criteria. == MethodsEdit == Mail filters can be installed by the user, either as separate programs (see links below), or as part of their email program (email client). In email programs, users can make personal, "manual" filters that then automatically filter mail according to the chosen criteria. Most email programs now also have an automatic spam filtering function. Mailbox providers can also install mail filters in their mail transfer agents as a service to all of their customers. Besides pass, redirect, and drop actions, this kind of filters can also reject a message back to the sender, who is presumed to generate a bounce message in this case. Anti-virus, anti-spam, URL filtering, and authentication-based rejections are common filter types. Corporations often use filters to protect their employees and their information technology assets. == Inbound and Outbound FilteringEdit == Mail filters can operate on inbound and outbound email traffic. Inbound email filtering involves scanning messages from the Internet addressed to users protected by the filtering system or for lawful interception. Outbound email filtering involves the reverse - scanning email messages from local users before any potentially harmful messages can be delivered to others on the Internet. One method of outbound email filtering that is commonly used by Internet service providers is transparent SMTP proxying, in which email traffic is intercepted and filtered via a transparent proxy within the network. Outbound filtering can also take place in an email server. Many corporations employ data leak prevention technology in their outbound mail servers to prevent the leakage of sensitive information via email. == CustomizationEdit == Mail filters have varying degrees of configurability. Sometimes they make decisions based on matching a regular expression. Other times, keywords in the message body are used, or perhaps the email address of the sender of the message. More complex control flow and logic is possible with programming languages; this is typically implemented with a data-driven programming language, such as procmail, which specifies conditions to match and actions to take on matching, which may involve further matching. Some more advanced filters, particularly anti-spam filters, use statistical document classification techniques such as the naive Bayes classifier. Image filtering can also be used that use complex image analysis algorithms to detect skin-tones and specific body shapes normally associated with pornographic images. == See alsoEdit == Bayesian spam filtering CRM114 Information filtering Markovian discrimination Outbound Spam Protection Sieve (mail filtering language) is an RFC standard for describing mail filters White list#Email whitelists == ReferencesEdit == == External linksEdit == Spam filtering at DMOZ 
Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, bioinformatics and baseball. == Introduction == The goal of learning is prediction. Learning falls into many categories, including supervised learning, unsupervised learning, online learning, and reinforcement learning. From the perspective of statistical learning theory, supervised learning is best understood. Supervised learning involves learning from a training set of data. Every point in the training is an input-output pair, where the input maps to an output. The learning problem consists of inferring the function that maps between the input and the output in a predictive fashion, such that the learned function can be used to predict output from future input. Depending of the type of output, supervised learning problems are either problems of regression or problems of classification. If the output takes a continuous range of values, it is a regression problem. Using Ohm's Law as an example, a regression could be performed with voltage as input and current as output. The regression would find the functional relationship between voltage and current to be , such that Classification problems are those for which the output will be an element from a discrete set of labels. Classification is very common for machine learning applications. In facial recognition, for instance, a picture of a person's face would be the input, and the output label would be that person's name. The input would be represented by a large multidimensional vector whose elements represent pixels in the picture. After learning a function based on the training set data, that function is validated on a test set of data, data that did not appear in the training set. == Formal Description == Take to be the vector space of all possible inputs, and to be the vector space of all possible outputs. Statistical learning theory takes the perspective that there is some unknown probability distribution over the product space , i.e. there exists some unknown . The training set is made up of samples from this probability distribution, and is notated Every is an input vector from the training data, and is the output that corresponds to it. In this formalism, the inference problem consists of finding a function such that . Let be a space of functions called the hypothesis space. The hypothesis space is the space of functions the algorithm will search through. Let be the loss functional, a metric for the difference between the predicted value and the actual value . The expected risk is defined to be The target function, the best possible function that can be chosen, is given by the that satisfies Because the probability distribution is unknown, a proxy measure for the expected risk must be used. This measure is based on the training set, a sample from this unknown probability distribution. It is called the empirical risk A learning algorithm that chooses the function that minimizes the empirical risk is called empirical risk minimization. == Loss Functions == The choice of loss function is a determining factor on the function that will be chosen by the learning algorithm. The loss function also affects the convergence rate for an algorithm. It is important for the loss function to be convex. Different loss functions are used depending on whether the problem is one of regression or one of classification. === Regression === The most common loss function for regression is the square loss function. This familiar loss function is used in ordinary least squares regression. The form is: The absolute value loss is also sometimes used: === Classification === In some sense the 0-1 indicator function is the most natural loss function for classification. It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output. For binary classification with , this is: where is the Heaviside step function. == Regularization == In machine learning problems, a major problem that arises is that of overfitting. Because learning is a prediction problem, the goal is not to find a function that most closely fits the (previously observed) data, but to find one that will most accurately predict output from future input. Empirical risk minimization runs this risk of overfitting: finding a function that matches the data exactly but does not predict future output well. Overfitting is symptomatic of unstable solutions; a small perturbation in the training set data would cause a large variation in the learned function. It can be shown that if the stability for the solution can be guaranteed, generalization and consistency are guaranteed as well. Regularization can solve the overfitting problem and give the problem stability. Regularization can be accomplished by restricting the hypothesis space . A common example would be restricting to linear functions: this can be seen as a reduction to the standard problem of linear regression. could also be restricted to polynomial of degree , exponentials, or bounded functions on L1. Restriction of the hypothesis space avoids overfitting because the form of the potential functions are limited, and so does not allow for the choice of a function that gives empirical risk arbitrarily close to zero. One example of regularization is Tikhonov regularization. This consists of minimizing where is a fixed and positive parameter, the regularization parameter. Tikhonov regularization ensures existence, uniqueness, and stability of the solution. == See also == Reproducing kernel Hilbert spaces are a useful choice for . Proximal gradient methods for learning == References == 
Stuart Jonathan Russell (born 1962) is a computer scientist known for his contributions to artificial intelligence. Stuart Russell was born in Portsmouth, England. He received his Bachelor of Arts degree with first-class honours in Physics from Wadham College, Oxford in 1982, and his Ph.D. in Computer Science from Stanford University in 1986. He then joined the faculty of the University of California, Berkeley, where he is currently Professor of Computer Science. He also holds an appointment as Adjunct Professor of Neurological Surgery at the University of California, San Francisco, where he pursues research in computational physiology and intensive-care unit monitoring. Stuart Russell was co-winner, in 1995, of the IJCAI Computers and Thought Award, the premier international award in artificial intelligence for researchers under 35. In 2003 he was inducted as a Fellow of the Association for Computing Machinery and in 2011 he became a Fellow of the American Association for the Advancement of Science. In 2005 he was awarded the ACM Karl V. Karlstrom Outstanding Educator Award. In 2012, he was appointed to the Blaise Pascal Chair in Paris, awarded to "internationally acclaimed foreign scientists in all disciplines," as well as the senior Chaire d'excellence of France's Agence Nationale de la Recherche. Along with Peter Norvig, he is the author of "Artificial Intelligence: A Modern Approach", a textbook used by over 1300 universities in 116 countries. He is on the Scientific Advisory Board for the Future of Life Institute and the Advisory Board of the Centre for the Study of Existential Risk. == Works == Russell, Stuart J.; Norvig, Peter (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Upper Saddle River: Prentice Hall. ISBN 9780136042594. Russell, Stuart J.; Tegmark, Max; Hawking, Stephen; Wilczek, Frank (2014). "Transcending Complacency on Superintelligent Machines". Huffington Post. == References == 
A tertiary source is an index and/or textual consolidation of primary and secondary sources. Some tertiary sources are not to be used for academic research, unless they can also be used as secondary sources, or to find other sources. == Overlap with secondary sources == Depending on the topic of research, a scholar may use a bibliography, dictionary, or encyclopedia as either a tertiary or a secondary source. This causes difficulty in defining many sources as either one type or the other. In some academic disciplines the distinction between a secondary and tertiary source is relative. In the United Nations International Scientific Information System (UNISIST) model, a secondary source is a bibliography, whereas a tertiary source is a synthesis of primary sources. == Types of tertiary sources == As tertiary sources, encyclopedias, textbooks, and compendia attempt to summarize and consolidate the source materials into an overview, but may also present subjective commentary and analysis (which are characteristics of secondary sources). Indexes, bibliographies, concordances, and databases may not provide much textual information, but as aggregates of primary and secondary sources, they are often considered tertiary sources. Almanacs, travel guides, field guides, and timelines are also examples of tertiary sources. Survey or overview articles are usually tertiary, though review articles in peer-reviewed academic journals are secondary (not be confused with film, book, etc. reviews, which are primary-source opinions). Some usually primary sources, such as user guides and manuals, are secondary or tertiary (depending on the nature of the material) when written by third parties. == See also == Research Source text Secondary source Primary source == Notes == 
Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and E. Fredkin University Professor at the Carnegie Mellon University (CMU). He is currently the Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Association for the Advancement of Science and a Fellow the Association for the Advancement of Artificial Intelligence. == Early life and education == Mitchell was born in Blossburg, Pennsylvania and grew up in Upstate New York, in the town of Vestal. He received his Bach of Science degree in Electrical Engineering from the Massachusetts Institute of Technology in 1973 and a Ph.D. from Stanford University under the direction of Bruce G. Buchanan in 1979. == Career == Mitchell began his teaching career at the Rutgers University in 1978. During his tenure at Rutgers, he held the positions of Assistant and Associate Professor in the Department of Computer Science. In 1986 he left Rutgers and joined Carnegie Mellon University, Pittsburgh as a Professor. In 1999 he became the E. Fredkin Professor in the School of Computer Science. In 2006 Mitchell was appointed as the first Chair of the Machine Learning Department within the School of Computer Science. He became University Professor in 2009. == Honors and awards == He was elected into the United States National Academy of Engineering in 2010 "for pioneering contributions and leadership in the methods and applications of machine learning." He is also a Fellow of the American Association for the Advancement of Science (AAAS) since 2008 and a Fellow the Association for the Advancement of Artificial Intelligence (AAAI) since 1990. Mitchell is also a recipient of the NSF Presidential Young Investigator Award in 1984. == Publications == Mitchell is a prolific author of scientific works on various topics in computer science, including machine learning, artificial intelligence, robotics, and cognitive neuroscience. He has authored about 130 scientific articles. Mitchell published one of the first textbooks in machine learning, entitled Machine Learning, in 1997. He is also a coauthor of the following books: J. Franklin, T. Mitchell, and S. Thrun (eds.), Recent Advances in Robot Learning, Kluwer Academic Publishers, 1996. T. Mitchell, J. Carbonell, and R. Michalski (eds.), Machine Learning: A Guide to Current Research, Kluwer Academic Publishers, 1986. R. Michalski, J. Carbonell, and T. Mitchell (eds.), Machine Learning: An Artificial Intelligence Approach, Volume 2, Morgan-Kaufman, 1986. R. Michalski, J. Carbonell, and T. Mitchell (eds.), Machine Learning: An Artificial Intelligence Approach, Tioga Press, 1983. == References == == External links == Official website List of PhDs and Postdocs supervised by Professor Tom Mitchell Tom M. Mitchell at the Mathematics Genealogy Project 
Torch is an open source machine learning library, a scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep machine learning, and uses an extremely fast scripting language LuaJIT, and an underlying C implementation. == torch == The core package of Torch is torch. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like max, min, sum, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix-vector multiplication, matrix-matrix multiplication, matrix-vector product and matrix product. The following exemplifies using torch via its REPL interpreter: The torch package also simplifies object oriented programming and serialization by providing various convenience functions which are used throughout its packages. The torch.class(classname, parentclass) function can be used to create object factories (classes). When the constructor is called, torch initializes and sets a Lua table with the user-defined metatable, which makes the table an object. Objects created with the torch factory can also be serialized, as long as they do not contain references to objects that cannot be serialized, such as Lua coroutines, and Lua userdata. However, userdata can be serialized if it is wrapped by a table (or metatable) that provides read() and write() methods. == nn == The nn package is used for building neural networks. It is divided into modular objects that share a common Module interface. Modules have a forward() and backward() method that allow them to feedforward and backpropagate, respectively. Modules can be joined together using module composites, like Sequential, Parallel and Concat to create complex task-tailored graphs. Simpler modules like Linear, Tanh and Max make up the basic component modules. This modular interface provides first-order automatic gradient differentiation. What follows is an example use-case for building a multilayer perceptron using Modules: Loss functions are implemented as sub-classes of Criterion, which has a similar interface to Module. It also has forward() and backward methods for computing the loss and backpropagating gradients, respectively. Criteria are helpful to train neural network on classical tasks. Common criteria are the Mean Squared Error criterion implemented in MSECriterion and the cross-entropy criterion implemented in ClassNLLCriterion. What follows is an example of a Lua function that can be iteratively called to train an mlp Module on input Tensor x, target Tensor y with a scalar learningRate: It also has StochasticGradient class for training a neural network using Stochastic gradient descent, although the Optim package provides much more options in this respect, like momentum and weight decay regularization. == Other packages == Many packages other than the above official packages are used with Torch. These are listed in the torch cheatsheet. These extra packages provide a wide range of utilities such as parallelism, asynchronous input/output, image processing, and so on. == Applications == Torch is used by Google DeepMind, the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. Torch has been extended for use on Android and iOS. It has been used to build hardware implementations for data flows like those found in neural networks. Facebook has released a set of extension modules as open source software. == Related libraries == Deeplearning4j, an open source deep learning library written for Java and Scala. OpenNN, an open source neural networks library written in C++ for deep learning. Theano, an open source deep learning library for Python. == See also == Comparison of deep learning frameworks == References == == External links == Official website "Torch". Repository (7 ed.). GitHub. 
Trevor John Hastie (born 27 June 1953) is an American statistician and computer scientist. He is currently serving as the John A. Overdeck Professor of Mathematical Sciences and Professor of Statistics at the Stanford University. Hastie is known for his contributions to applied statistics, especially in the field of machine learning, data mining, and bioinformatics. He has authored several popular books in statistical learning, including The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge. == Education and career == Hastie was born on 27 June 1953 in South Africa. He received his B.S. in statistics from the Rhodes University in 1976 and Master's degree from University of Cape Town in 1979. Hastie joined the doctoral program at Stanford University in 1980 and received his Ph.D. in 1984 under the supervision of Werner Stuetzle. His dissertation was "Principal Curves and Surfaces". Hastie began his professional career in 1977 with the South African Medical Research Council. After receiving his Master's degree in 1979, he spent a year interning at the London School of Hygiene & Tropical Medicine, the Johnson Space Center in Houston, and the Biomath department at Oxford University. After receiving his doctoral degree from Stanford, Hastie returned to South Africa to work with his former employer South African Medical Research Council. He returned to United States in 1986 and joined the AT&T Bell Laboratories in Murray Hill, New Jersey and remained there for nine years. He joined Stanford University in 1994 as Associate Professor in Statistics and Biostatistics. He was promoted to full Professor in 1999. During the period 2006-2009, he was the Chair of Department of Statistics at Stanford University. In 2013 he was named the John A. Overdeck Professor of Mathematical Sciences. == Awards and honors == Hastie is a Fellow of the Royal Statistical Society since 1979. He is also an elected Fellow of several professional and scholarly societies, including the Institute of Mathematical Statistics, the American Statistical Association, and the South African Statistical Society. He is a recipient of 'Myrto Lefkopolou Distinguished Lectureship' award of Biostatistics Department at the Harvard School of Public Health. == Publications == Hastie is a prolific author of scientific works on various topics in applied statistics, including statistical learning, data mining, statistical computing, and bioinformatics. He along with his collaborators has authored about 125 scientific articles. Many of Hastie's scientific articles were coauthored by his longtime collaborator, Robert Tibshirani. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge. He has coauthored the following books: T. Hastie and R. Tibshirani, Generalized Additive Models, Chapman and Hall, 1990. J. Chambers and T. Hastie, Statistical Models in S, Wadsworth/Brooks Cole, 1991. T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Prediction, Inference and Data Mining, Second Edition, Springer Verlag, 2009 (available for free from the author's website). == References == == External links == Official website Trevor Hastie at the Mathematics Genealogy Project 
Waikato Environment for Knowledge Analysis (Weka) is a popular suite of machine learning software written in Java, developed at the University of Waikato, New Zealand. It is free software licensed under the GNU General Public License. == Description == Weka (pronounced to rhyme with Mecca) is a workbench that contains a collection of visualization tools and algorithms for data analysis and predictive modeling, together with graphical user interfaces for easy access to these functions. The original non-Java version of Weka was a Tcl/Tk front-end to (mostly third-party) modeling algorithms implemented in other programming languages, plus data preprocessing utilities in C, and a Makefile-based system for running machine learning experiments. This original version was primarily designed as a tool for analyzing data from agricultural domains, but the more recent fully Java-based version (Weka 3), for which development started in 1997, is now used in many different application areas, in particular for educational purposes and research. Advantages of Weka include: Free availability under the GNU General Public License. Portability, since it is fully implemented in the Java programming language and thus runs on almost any modern computing platform. A comprehensive collection of data preprocessing and modeling techniques. Ease of use due to its graphical user interfaces. Weka supports several standard data mining tasks, more specifically, data preprocessing, clustering, classification, regression, visualization, and feature selection. All of Weka's techniques are predicated on the assumption that the data is available as one flat file or relation, where each data point is described by a fixed number of attributes (normally, numeric or nominal attributes, but some other attribute types are also supported). Weka provides access to SQL databases using Java Database Connectivity and can process the result returned by a database query. It is not capable of multi-relational data mining, but there is separate software for converting a collection of linked database tables into a single table that is suitable for processing using Weka. Another important area that is currently not covered by the algorithms included in the Weka distribution is sequence modeling. == User interfaces == Weka's main user interface is the Explorer, but essentially the same functionality can be accessed through the component-based Knowledge Flow interface and from the command line. There is also the Experimenter, which allows the systematic comparison of the predictive performance of Weka's machine learning algorithms on a collection of datasets. The Explorer interface features several panels providing access to the main components of the workbench: The Preprocess panel has facilities for importing data from a database, a comma-separated values (CSV) file, etc., and for preprocessing this data using a so-called filtering algorithm. These filters can be used to transform the data (e.g., turning numeric attributes into discrete ones) and make it possible to delete instances and attributes according to specific criteria. The Classify panel enables applying classification and regression algorithms (indiscriminately called classifiers in Weka) to the resulting dataset, to estimate the accuracy of the resulting predictive model, and to visualize erroneous predictions, receiver operating characteristic (ROC) curves, etc., or the model itself (if the model is amenable to visualization like, e.g., a decision tree). The Associate panel provides access to association rule learners that attempt to identify all important interrelationships between attributes in the data. The Cluster panel gives access to the clustering techniques in Weka, e.g., the simple k-means algorithm. There is also an implementation of the expectation maximization algorithm for learning a mixture of normal distributions. The Select attributes panel provides algorithms for identifying the most predictive attributes in a dataset. The Visualize panel shows a scatter plot matrix, where individual scatter plots can be selected and enlarged, and analyzed further using various selection operators. == Extension packages == In version 3.7.2 (thus not available in the stable "book" version of Weka), a package manager was added to allow the easier installation of extension packages. Some functionality that used to be included with Weka prior to this version has since been moved into such extension packages, but this change also makes it easier for other to contribute extensions to Weka and to maintain the software, as this modular architecture allows independent updates of the Weka core and individual extensions. == History == In 1993, the University of Waikato in New Zealand began development of the original version of Weka, which became a mix of Tcl/Tk, C, and Makefiles. In 1997, the decision was made to redevelop Weka from scratch in Java, including implementations of modeling algorithms. In 2005, Weka received the SIGKDD Data Mining and Knowledge Discovery Service Award. In 2006, Pentaho Corporation acquired an exclusive licence to use Weka for business intelligence. It forms the data mining and predictive analytics component of the Pentaho business intelligence suite. All-time ranking on Sourceforge.net as of 2011-08-26, 243 (with 2,487,213 downloads) == Related tools == Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI) is a similar project to Weka with a focus on cluster analysis, i.e., unsupervised methods. KNIME is a machine learning and data mining software implemented in Java. Massive Online Analysis (MOA) is an open-source project for large scale mining of data streams, also developed at the University of Waikato in New Zealand. Neural Designer is a data mining software based on deep learning techniques written in C++. Orange is a similar open-source project for data mining, machine learning and visualization written in Python and C++. RapidMiner is a commercial machine learning framework implemented in Java which integrates Weka. == See also == Machine learning Data mining List of numerical analysis software == References == == External links == Official website at University of Waikato in New Zealand Official Weka Wiki with FAQs, HOWTOs, code-snippets, etc. 
Apriori is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis. == Overview == The Apriori algorithm was proposed by Agarwal and Srikant in 1994. Apriori is designed to operate on databases containing transactions (for example, collections of items bought by customers, or details of a website frequentation). Other algorithms are designed for finding association rules in data having no transactions (Winepi and Minepi), or having no timestamps (DNA sequencing). Each transaction is seen as a set of items (an itemset). Given a threshold , the Apriori algorithm identifies the item sets which are subsets of at least transactions in the database. Apriori uses a "bottom up" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length from item sets of length . Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent -length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates. The pseudo code for the algorithm is given below for a transaction database , and a support threshold of . Usual set theoretic notation is employed, though note that is a multiset. is the candidate set for level . At each step, the algorithm is assumed to generate the candidate sets from the large item sets of the preceding level, heeding the downward closure lemma. accesses a field of the data structure that represents candidate set , which is initially assumed to be zero. Many details are omitted below, usually the most important part of the implementation is the data structure used for storing the candidate sets, and counting their frequencies. == Examples == === Example 1 === Consider the following database, where each row is a transaction and each cell is an individual item of the transaction: The association rules that can be determined from this database are the following: 100% of sets with alpha also contain beta 50% of sets with alpha, beta also have epsilon 50% of sets with alpha, beta also have theta we can also illustrate this through a variety of examples === Example 2 === Assume that a large supermarket tracks sales data by stock-keeping unit (SKU) for each item: each item, such as "butter" or "bread", is identified by a numerical SKU. The supermarket has a database of transactions where each transaction is a set of SKUs that were bought together. Let the database of transactions consist of following itemsets: We will use Apriori to determine the frequent item sets of this database. To do so, we will say that an item set is frequent if it appears in at least 3 transactions of the database: the value 3 is the support threshold. The first step of Apriori is to count up the number of occurrences, called the support, of each member item separately, by scanning the database a first time. We obtain the following result All the itemsets of size 1 have a support of at least 3, so they are all frequent. The next step is to generate a list of all pairs of the frequent items: The pairs {1,2}, {2,3}, {2,4}, and {3,4} all meet or exceed the minimum support of 3, so they are frequent. The pairs {1,3} and {1,4} are not. Now, because {1,3} and {1,4} are not frequent, any larger set which contains {1,3} or {1,4} cannot be frequent. In this way, we can prune sets: we will now look for frequent triples in the database, but we can already exclude all the triples that contain one of these two pairs: in the example, there are no frequent triplets -- {2,3,4} is below the minimal threshold, and the other triplets were excluded because they were super sets of pairs that were already below the threshold. We have thus determined the frequent sets of items in the database, and illustrated how some items were not counted because one of their subsets was already known to be below the threshold. == Limitations == Apriori, while historically significant, suffers from a number of inefficiencies or trade-offs, which have spawned other algorithms. Candidate generation generates large numbers of subsets (the algorithm attempts to load up the candidate set with as many as possible before each scan). Bottom-up subset exploration (essentially a breadth-first traversal of the subset lattice) finds any maximal subset S only after all of its proper subsets. Later algorithms such as Max-Miner try to identify the maximal frequent item sets without enumerating their subsets, and perform "jumps" in the search space rather than a purely bottom-up approach. == References == == External links == ARtool, GPL Java association rule mining application with GUI, offering implementations of multiple algorithms for discovery of frequent patterns and extraction of association rules (includes Apriori) ELKI includes Java implementations of Apriori, Eclat and FPGrowth. SPMF offers Java open-source implementations of Apriori and several variations such as AprioriClose, UApriori, AprioriInverse, AprioriRare, MSApriori, AprioriTID, and other more efficient algorithms such as FPGrowth and LCM. Christian Borgelt provides C implementations for Apriori and many other frequent pattern mining algorithms (Eclat, FPGrowth, etc.). The code is distributed as free software under the MIT license. The R package arules contains Apriori and Eclat and infrastructure for representing, manipulating and analyzing transaction data and patterns. Orange, an open-source data mining suite, contains widgets for enumerating itemsets and association rules based on Apriori algorithm. 
 == Supervised learningEdit == AODE Artificial neural network Backpropagation Autoencoders Hopfield networks Boltzmann machines Restricted Boltzmann Machines Spiking neural networks Bayesian statistics Bayesian network Bayesian knowledge base Case-based reasoning Gaussian process regression Gene expression programming Group method of data handling (GMDH) Inductive logic programming Instance-based learning Lazy learning Learning Automata Learning Vector Quantization Logistic Model Tree Minimum message length (decision trees, decision graphs, etc.) Nearest Neighbor Algorithm Analogical modeling Probably approximately correct learning (PAC) learning Ripple down rules, a knowledge acquisition methodology Symbolic machine learning algorithms Support vector machines Random Forests Ensembles of classifiers Bootstrap aggregating (bagging) Boosting (meta-algorithm) Ordinal classification Information fuzzy networks (IFN) Conditional Random Field ANOVA Linear classifiers Fisher's linear discriminant Logistic regression Multinomial logistic regression Naive Bayes classifier Perceptron Support vector machines Quadratic classifiers k-nearest neighbor Boosting Decision trees C4.5 Random forests ID3 CART SLIQ SPRINT Bayesian networks Naive Bayes Hidden Markov models == Unsupervised learningEdit == Expectation-maximization algorithm Vector Quantization Generative topographic map Information bottleneck method === Artificial neural networkEdit === Self-organizing map === Association rule learningEdit === Apriori algorithm Eclat algorithm FP-growth algorithm === Hierarchical clusteringEdit === Single-linkage clustering Conceptual clustering === Cluster analysisEdit === K-means algorithm Fuzzy clustering DBSCAN OPTICS algorithm === Outlier DetectionEdit === Local Outlier Factor == Semi-supervised learningEdit == == Reinforcement learningEdit == Temporal difference learning Q-learning Learning Automata SARSA == Deep learningEdit == Deep belief networks Deep Boltzmann machines Deep Convolutional neural networks Deep Recurrent neural networks Hierarchical temporal memory == OthersEdit == Data Pre-processing List of artificial intelligence projects 
Bayesian statistics is a subset of the field of statistics in which the evidence about the true state of the world is expressed in terms of degrees of belief or, more specifically, Bayesian probabilities. Such an interpretation is only one of a number of interpretations of probability and there are other statistical techniques that are not based on "degrees of belief". One formulation of the "key ideas of Bayesian statistics" is "that probability is orderly opinion, and that inference from data is nothing other than the revision of such opinion in the light of relevant new information." == Outline == The general set of statistical techniques can be divided into a number of activities, many of which have special Bayesian versions. === Statistical inference === Bayesian inference is an approach to statistical inference, that is distinct from frequentist inference. It is specifically based on the use of Bayesian probabilities to summarize evidence. === Statistical modeling === The formulation of statistical models using Bayesian statistics has the unique feature of requiring the specification of prior distributions for any unknown parameters. These prior distributions are as integral to a Bayesian approach to statistical modelling as the expression of probability distributions. Prior distributions can be either hyperparameters or hyperprior distributions. === Design of experiments === The Bayesian design of experiments includes a concept called 'influence of prior beliefs'. This approach uses sequential analysis techniques to include the outcome of earlier experiments in the design of the next experiment. This is achieved by updating 'beliefs' through the use of prior and posterior distribution. This allows the design of experiments to make good use of resources of all types. An example of this is the multi-armed bandit problem. === Statistical graphics === Statistical graphics includes methods for data exploration, for model validation, etc. The use of certain modern computational techniques for Bayesian inference, specifically the various types of Markov chain Monte Carlo techniques, have led to the need for checks, often made in graphical form, on the validity of such computations in expressing the required posterior distributions. == References == == External links == Eliezer S. Yudkowsky. "An Intuitive Explanation of Bayes' Theorem" (webpage). Retrieved 2015-06-15. Theo Kypraios. "A Gentle Tutorial in Bayesian Statistics" (PDF) (PDF). Retrieved 2013-11-03. Theo Kypraios. "Introduction to Bayesian Statistics" (PDF) (PDF). Retrieved 2014-05-05. Jordi Vallverdu. "Bayesians Versus Frequentists A Philosophical Debate on Statistical Reasoning". Retrieved 2015-12-17. Bayesian statistics David Spiegelhalter, Kenneth Rice Scholarpedia 4(8):5230. doi:10.4249/scholarpedia.5230 Bayesian modeling book and examples available for downloading. 
C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. It became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008. == Algorithm == C4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy. The training data is a set of already classified samples. Each sample consists of a p-dimensional vector , where the represent attribute values or features of the sample, as well as the class in which falls. At each node of the tree, C4.5 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C4.5 algorithm then recurs on the smaller sublists. This algorithm has a few base cases. All the samples in the list belong to the same class. When this happens, it simply creates a leaf node for the decision tree saying to choose that class. None of the features provide any information gain. In this case, C4.5 creates a decision node higher up the tree using the expected value of the class. Instance of previously-unseen class encountered. Again, C4.5 creates a decision node higher up the tree using the expected value. === Pseudocode === In pseudocode, the general algorithm for building decision trees is: Check for base cases For each attribute a Find the normalized information gain ratio from splitting on a Let a_best be the attribute with the highest normalized information gain Create a decision node that splits on a_best Recur on the sublists obtained by splitting on a_best, and add those nodes as children of node == Implementations == J48 is an open source Java implementation of the C4.5 algorithm in the weka data mining tool. == Improvements from ID.3 algorithm == C4.5 made a number of improvements to ID3. Some of these are: Handling both continuous and discrete attributes - In order to handle continuous attributes, C4.5 creates a threshold and then splits the list into those whose attribute value is above the threshold and those that are less than or equal to it. Handling training data with missing attribute values - C4.5 allows attribute values to be marked as ? for missing. Missing attribute values are simply not used in gain and entropy calculations. Handling attributes with differing costs. Pruning trees after creation - C4.5 goes back through the tree once it's been created and attempts to remove branches that do not help by replacing them with leaf nodes. == Improvements in C5.0/See5 algorithm == Quinlan went on to create C5.0 and See5 (C5.0 for Unix/Linux, See5 for Windows) which he markets commercially. C5.0 offers a number of improvements on C4.5. Some of these are: Speed - C5.0 is significantly faster than C4.5 (several orders of magnitude) Memory usage - C5.0 is more memory efficient than C4.5 Smaller decision trees - C5.0 gets similar results to C4.5 with considerably smaller decision trees. Support for boosting - Boosting improves the trees and gives them more accuracy. Weighting - C5.0 allows you to weight different cases and misclassification types. Winnowing - a C5.0 option automatically winnows the attributes to remove those that may be unhelpful. Source for a single-threaded Linux version of C5.0 is available under the GPL. == See also == ID3 algorithm == References == ^ Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993. ^ Umd.edu - Top 10 Algorithms in Data Mining ^ S.B. Kotsiantis, Supervised Machine Learning: A Review of Classification Techniques, Informatica 31(2007) 249-268, 2007 ^ J. R. Quinlan. Improved use of continuous attributes in c4.5. Journal of Artificial Intelligence Research, 4:77-90, 1996. ^ Is See5/C5.0 Better Than C4.5? ^ M. Kuhn and K. Johnson, Applied Predictive Modeling, Springer 2013 == External links == Original implementation on Ross Quinlan's homepage: http://www.rulequest.com/Personal/ See5 and C5.0 
In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble refers only to a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives. == OverviewEdit == Supervised learning algorithms are commonly described as performing the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner. The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner. Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model, so ensembles may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. Fast algorithms such as decision trees are commonly used with ensembles (for example Random Forest), although slower algorithms can benefit from ensemble techniques as well. == Ensemble theoryEdit == An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to over-fitting of the training data. Empirically, ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods, therefore, seek to promote diversity among the models they combine. Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees). Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity. == Common types of ensemblesEdit == === Bayes optimal classifierEdit === The Bayes Optimal Classifier is a classification technique. It is an ensemble of all the hypotheses in the hypothesis space. On average, no other ensemble can outperform it. Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. The Bayes Optimal Classifier can be expressed with the following equation: where is the predicted class, is the set of all possible classes, is the hypothesis space, refers to a probability, and is the training data. As an ensemble, the Bayes Optimal Classifier represents a hypothesis that is not necessarily in . The hypothesis represented by the Bayes Optimal Classifier, however, is the optimal hypothesis in ensemble space (the space of all possible ensembles consisting only of hypotheses in ). Unfortunately, the Bayes Optimal Classifier cannot be practically implemented for any but the most simple of problems. There are several reasons why the Bayes Optimal Classifier cannot be practically implemented: Most interesting hypothesis spaces are too large to iterate over, as required by the . Many hypotheses yield only a predicted class, rather than a probability for each class as required by the term . Computing an unbiased estimate of the probability of the training set given a hypothesis () is non-trivial. Estimating the prior probability for each hypothesis () is rarely feasible. === Bootstrap aggregating (bagging)Edit === Bootstrap aggregating, often abbreviated as bagging, involves having each model in the ensemble vote with equal weight. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy. An interesting application of bagging in unsupervised learning is provided here. === BoostingEdit === Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data. By far, the most common implementation of Boosting is Adaboost, although some newer algorithms are reported to achieve better results. === Bayesian parameter averagingEdit === Bayesian parameter averaging (BPA) is an ensemble technique that seeks to approximate the Bayes Optimal Classifier by sampling hypotheses from the hypothesis space, and combining them using Bayes' law. Unlike the Bayes optimal classifier, Bayesian model averaging can be practically implemented. Hypotheses are typically sampled using a Monte Carlo sampling technique such as MCMC. For example, Gibbs sampling may be used to draw hypotheses that are representative of the distribution . It has been shown that under certain circumstances, when hypotheses are drawn in this manner and averaged according to Bayes' law, this technique has an expected error that is bounded to be at most twice the expected error of the Bayes optimal classifier. Despite the theoretical correctness of this technique, it has been found to promote over-fitting and to perform worse, empirically, compared to simpler ensemble techniques such as bagging; however, these conclusions appear to be based on a misunderstanding of the purpose of Bayesian model averaging vs. model combination. === Bayesian model combinationEdit === Bayesian model combination (BMC) is an algorithmic correction to BMA. Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. The results from BMC have been shown to be better on average (with statistical significance) than BMA, and bagging. The use of Bayes' law to compute model weights necessitates computing the probability of the data given each model. Typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. This would work well if the ensemble were big enough to sample the entire model-space, but such is rarely possible. Consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. It essentially reduces to an unnecessarily complex method for doing model selection. The possible weightings for an ensemble can be visualized as lying on a simplex. At each vertex of the simplex, all of the weight is given to a single model in the ensemble. BMA converges toward the vertex that is closest to the distribution of the training data. By contrast, BMC converges toward the point where this distribution projects onto the simplex. In other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution. The results from BMA can often be approximated by using cross-validation to select the best model from a bucket of models. Likewise, the results from BMC may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings. === Bucket of modelsEdit === A "bucket of models" is an ensemble in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set. The most common approach used for model-selection is cross-validation selection (sometimes called a "bake-off contest"). It is described with the following pseudo-code: For each model m in the bucket: Do c times: (where 'c' is some constant) Randomly divide the training dataset into two datasets: A, and B. Train m with A Test m with B Select the model that obtains the highest average score Cross-Validation Selection can be summed up as: "try them all with the training set, and pick the one that works best". Gating is a generalization of Cross-Validation Selection. It involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. Often, a perceptron is used for the gating model. It can be used to pick the "best" model, or it can be used to give a linear weight to the predictions from each model in the bucket. When a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. Landmark learning is a meta-learning approach that seeks to solve this problem. It involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best. === StackingEdit === Stacking (sometimes called stacked generalization) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although in practice, a single-layer logistic regression model is often used as the combiner. Stacking typically yields performance better than any single one of the trained models. It has been successfully used on both supervised learning tasks (regression, classification and distance learning ) and unsupervised learning (density estimation). It has also been used to estimate bagging's error rate. It has been reported to out-perform Bayesian model-averaging. The two top-performers in the Netflix competition utilized blending, which may be considered to be a form of stacking. == ReferencesEdit == == Further readingEdit == Zhou Zhihua (2012). Ensemble Methods: Foundations and Algorithms. Chapman and Hall/CRC. ISBN 978-1-439-83003-1. Robert Schapire; Yoav Freund (2012). Boosting: Foundations and Algorithms. MIT. ISBN 978-0-262-01718-3. == External linksEdit == Ensemble learning at Scholarpedia, curated by Robi Polikar. The Waffles (machine learning) toolkit contains implementations of Bagging, Boosting, Bayesian Model Averaging, Bayesian Model Combination, Bucket-of-models, and other ensemble techniques 
Data clustering is the process of dividing data elements from original images into classes or clusters so that items in the same class are as similar as possible, and items in different classes are as dissimilar as possible. Depending on the nature of the data and the purpose for which clustering is being used, different measures of similarity may be used to place items into classes, where the similarity measure controls how the clusters are formed. Some examples of measures that can be used as in clustering include distance, connectivity, and intensity. In hard clustering, data is divided into distinct clusters, where each data element belongs to exactly one cluster. In fuzzy clustering (also referred to as soft clustering), data elements can belong to more than one cluster, and associated with each element is a set of membership levels. These indicate the strength of the association between that data element and a particular cluster. Fuzzy clustering is a process of assigning these membership levels, and then using them to assign data elements to one or more clusters. One of the most widely used fuzzy clustering algorithms is the Fuzzy C-Means (FCM) Algorithm (Bezdek 1981). The FCM algorithm attempts to partition a finite collection of elements into a collection of c fuzzy clusters with respect to some given criterion. Given a finite set of data, the algorithm returns a list of cluster centres and a partition matrix , where each element tells the degree to which element belongs to cluster . Like the K-means clustering, the FCM aims to minimize an objective function: where: This differs from the k-means objective function by the addition of the membership values and the fuzzifier , with . The fuzzifier determines the level of cluster fuzziness. A large results in smaller memberships and hence, fuzzier clusters. In the limit , the memberships converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge, is commonly set to 2. == Fuzzy c-means clustering == In fuzzy clustering, every point has a degree of belonging to clusters, as in fuzzy logic, rather than belonging completely to just one cluster. Thus, points on the edge of a cluster, may be in the cluster to a lesser degree than points in the center of cluster. An overview and comparison of different fuzzy clustering algorithms is available. Any point x has a set of coefficients giving the degree of being in the kth cluster wk(x). With fuzzy c-means, the centroid of a cluster is the mean of all points, weighted by their degree of belonging to the cluster: The degree of belonging, wk(x), is related inversely to the distance from x to the cluster center as calculated on the previous pass. It also depends on a parameter m that controls how much weight is given to the closest center. The fuzzy c-means algorithm is very similar to the k-means algorithm: Choose a number of clusters. Assign randomly to each point coefficients for being in the clusters. Repeat until the algorithm has converged (that is, the coefficients' change between two iterations is no more than , the given sensitivity threshold) : Compute the centroid for each cluster, using the formula above. For each point, compute its coefficients of being in the clusters, using the formula above. The algorithm minimizes intra-cluster variance as well, but has the same problems as k-means; the minimum is a local minimum, and the results depend on the initial choice of weights. Using a mixture of Gaussians along with the expectation-maximization algorithm is a more statistically formalized method which includes some of these ideas: partial membership in classes. Another algorithm closely related to Fuzzy C-Means is Soft K-means. Fuzzy c-means has been a very important tool for image processing in clustering objects in an image. In the 70's, mathematicians introduced the spatial term into the FCM algorithm to improve the accuracy of clustering under noise. == See also == FLAME Clustering Cluster Analysis Expectation-maximization algorithm (a similar, but more statistically formalized method) == References == == External links == Fuzzy Clustering in Wolfram Research Extended Fuzzy Clustering Algorithms by Kaymak, U. and Setnes, M. Fuzzy Clustering in C++ and Boost by Antonio Gulli Concise description with examples 
Generative topographic map (GTM) is a machine learning method that is a probabilistic counterpart of the self-organizing map (SOM), is probably convergent and does not require a shrinking neighborhood or a decreasing step size. It is a generative model: the data is assumed to arise by first probabilistically picking a point in a low-dimensional space, mapping the point to the observed high-dimensional input space (via a smooth function), then adding noise in that space. The parameters of the low-dimensional probability distribution, the smooth map and the noise are all learned from the training data using the expectation-maximization (EM) algorithm. GTM was introduced in 1996 in a paper by Christopher Bishop, Markus Svensen, and Christopher K. I. Williams. == Details of the algorithm == The approach is strongly related to density networks which use importance sampling and a multi-layer perceptron to form a non-linear latent variable model. In the GTM the latent space is a discrete grid of points which is assumed to be non-linearly projected into data space. A Gaussian noise assumption is then made in data space so that the model becomes a constrained mixture of Gaussians. Then the model's likelihood can be maximized by EM. In theory, an arbitrary nonlinear parametric deformation could be used. The optimal parameters could be found by gradient descent, etc. The suggested approach to the nonlinear mapping is to use a radial basis function network (RBF) to create a nonlinear mapping between the latent space and the data space. The nodes of the RBF network then form a feature space and the nonlinear mapping can then be taken as a linear transform of this feature space. This approach has the advantage over the suggested density network approach that it can be optimised analytically. == Uses == In data analysis, GTMs are like a nonlinear version of principal components analysis, which allows high-dimensional data to be modelled as resulting from Gaussian noise added to sources in lower-dimensional latent space. For example, to locate stocks in plottable 2D space based on their hi-D time-series shapes. Other applications may want to have fewer sources than data points, for example mixture models. In generative deformational modelling, the latent and data spaces have the same dimensions, for example, 2D images or 1 audio sound waves. Extra 'empty' dimensions are added to the source (known as the 'template' in this form of modelling), for example locating the 1D sound wave in 2D space. Further nonlinear dimensions are then added, produced by combining the original dimensions. The enlarged latent space is then projected back into the 1D data space. The probability of a given projection is, as before, given by the product of the likelihood of the data under the Gaussian noise model with the prior on the deformation parameter. Unlike conventional spring-based deformation modelling, this has the advantage of being analytically optimizable. The disadvantage is that it is a 'data-mining' approach, i.e. the shape of the deformation prior is unlikely to be meaningful as an explanation of the possible deformations, as it is based on a very high, artificial- and arbitrarily constructed nonlinear latent space. For this reason the prior is learned from data rather than created by a human expert, as is possible for spring-based models. == Comparison with Kohonen's self-organizing maps == While nodes in the self-organizing map (SOM) can wander around at will, GTM nodes are constrained by the allowable transformations and their probabilities. If the deformations are well-behaved the topology of the latent space is preserved. The SOM was created as a biological model of neurons and is a heuristic algorithm. By contrast, the GTM has nothing to do with neuroscience or cognition and is a probabilistically principled model. Thus, it has a number of advantages over SOM, namely: it explicitly formulates a density model over the data. it uses a cost function that quantifies how well the map is trained. it uses a sound optimization procedure (EM algorithm). GTM was introduced by Bishop, Svensen and Williams in their Technical Report in 1997 (Technical Report NCRG/96/015, Aston University, UK) published later in Neural Computation. It was also described in the PhD thesis of Markus Svensen (Aston, 1998). == Applications == == See also == Self-organizing map (SOM) Artificial Neural Network Connectionism Data mining Machine learning Nonlinear dimensionality reduction Neural network software Pattern recognition == External links == Bishop, Svensen and Williams Generative Topographic Mapping paper Generative topographic mapping developed at the Neural Computing Research Group os Aston University (UK). ( Matlab toolbox ) 
In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory. Instance-based learning is a kind of lazy learning. It is called instance-based because it constructs hypotheses directly from the training instances themselves. This means that the hypothesis complexity can grow with the data: in the worst case, a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is O(n). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data: instance-based learners may simply store a new instance or throw an old instance away. Examples of instance-based learning algorithm are the k-nearest neighbor algorithm, kernel machines and RBF networks. These store (a subset of) their training set; when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision. To battle the memory complexity of storing all training instances, as well as the risk of overfitting to noise in the training set, instance reduction algorithms have been proposed. Gagliardi applies this family of classifiers in medical field as second-opinion diagnostic tools and as tools for the knowledge extraction phase in the process of knowledge discovery in databases. One of these classifiers (called Prototype exemplar learning classifier (PEL-C) is able to extract a mixture of abstracted prototypical cases (that are syndromes) and selected atypical clinical cases. == See also == Analogical modeling == References == 
In machine learning, lazy learning is a learning method in which generalization beyond the training data is delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries. The main advantage gained in employing a lazy learning method, such as Case based reasoning, is that the target function will be approximated locally, such as in the k-nearest neighbor algorithm. Because the target function is approximated locally for each query to the system, lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain. The disadvantages with lazy learning include the large space requirement to store the entire training dataset. Particularly noisy training data increases the case base unnecessarily, because no abstraction is made during the training phase. Another disadvantage is that lazy learning methods are usually slower to evaluate, though this is coupled with a faster training phase. Lazy classifiers are most useful for large datasets with few attributes. == See also == Instance-based learning == References == Lazy Learning for Local Regression The Lazy Learning Package 
In computer science, a logistic model tree (LMT) is a classification model with an associated supervised training algorithm that combines logistic regression (LR) and decision tree learning. Logistic model trees are based on the earlier idea of a model tree: a decision tree that has linear regression models at its leaves to provide a piecewise linear regression model (where ordinary decision trees with constants at their leaves would produce a piecewise constant model). In the logistic variant, the LogitBoost algorithm is used to produce an LR model at every node in the tree; the node is then split using the C4.5 criterion. Each LogitBoost invocation is warm-started from its results in the parent node. Finally, the tree is pruned. The basic LMT induction algorithm uses cross-validation to find a number of LogitBoost iterations that does not overfit the training data. A faster version has been proposed that uses the Akaike information criterion to control LogitBoost stopping. == References == == See also == C4.5 algorithm 
 == Supervised learningEdit == AODE Artificial neural network Backpropagation Autoencoders Hopfield networks Boltzmann machines Restricted Boltzmann Machines Spiking neural networks Bayesian statistics Bayesian network Bayesian knowledge base Case-based reasoning Gaussian process regression Gene expression programming Group method of data handling (GMDH) Inductive logic programming Instance-based learning Lazy learning Learning Automata Learning Vector Quantization Logistic Model Tree Minimum message length (decision trees, decision graphs, etc.) Nearest Neighbor Algorithm Analogical modeling Probably approximately correct learning (PAC) learning Ripple down rules, a knowledge acquisition methodology Symbolic machine learning algorithms Support vector machines Random Forests Ensembles of classifiers Bootstrap aggregating (bagging) Boosting (meta-algorithm) Ordinal classification Information fuzzy networks (IFN) Conditional Random Field ANOVA Linear classifiers Fisher's linear discriminant Logistic regression Multinomial logistic regression Naive Bayes classifier Perceptron Support vector machines Quadratic classifiers k-nearest neighbor Boosting Decision trees C4.5 Random forests ID3 CART SLIQ SPRINT Bayesian networks Naive Bayes Hidden Markov models == Unsupervised learningEdit == Expectation-maximization algorithm Vector Quantization Generative topographic map Information bottleneck method === Artificial neural networkEdit === Self-organizing map === Association rule learningEdit === Apriori algorithm Eclat algorithm FP-growth algorithm === Hierarchical clusteringEdit === Single-linkage clustering Conceptual clustering === Cluster analysisEdit === K-means algorithm Fuzzy clustering DBSCAN OPTICS algorithm === Outlier DetectionEdit === Local Outlier Factor == Semi-supervised learningEdit == == Reinforcement learningEdit == Temporal difference learning Q-learning Learning Automata SARSA == Deep learningEdit == Deep belief networks Deep Boltzmann machines Deep Convolutional neural networks Deep Recurrent neural networks Hierarchical temporal memory == OthersEdit == Data Pre-processing List of artificial intelligence projects 
Biometrika is a peer-reviewed scientific journal published by Oxford University Press for the Biometrika Trust. It was established in October, 1901. The editor-in-chief is A. C. Davison. The principal focus of this journal is theoretical statistics. It was established in 1901 and originally appeared quarterly. It changed to three issues per year in 1977 but returned to quarterly publication in 1992. == History == Biometrika was established in 1901 by Francis Galton, Karl Pearson, and Raphael Weldon to promote the study of biometrics. The history of Biometrika is covered by Cox (2001). The name of the journal was chosen by Pearson, but Francis Edgeworth insisted that it be spelt with a "k" and not a "c". Since the 1930s, it has been a journal for statistical theory and methodology. Galton's role in the journal was essentially that of a patron and the journal was run by Pearson and Weldon and after Weldon's death in 1906 by Pearson alone until he died in 1936. In the early days, the American biologists Charles Davenport and Raymond Pearl were nominally involved but they dropped out. On Pearson's death his son Egon Pearson became editor and remained in this position until 1966. David Cox was editor for the next 25 years. So, in its first 65 years Biometrika had effectively a total of just three editors, and in its first 90 years only four. Other people who were deeply involved in the journal included William Palin Elderton, an associate of Pearson's who published several articles in the early days and in 1935 became chairman of the Biometrika Trust. In the very first issue, the editors presented a clear statement of purpose: It is intended that Biometrika shall serve as a means not only of collecting or publishing under one title biological data of a kind not systematically collected or published elsewhere in any other periodical, but also of spreading a knowledge of such statistical theory as may be requisite for their scientific treatment. Its contents were to include: memoirs on variation, inheritance, and selection in animals and plants, based upon the examination of statistically large numbers of specimens those developments of statistical theory which are applicable to biological problems numerical tables and graphical solutions tending to reduce the labour of statistical arithmetic abstracts of memoirs, dealing with these subjects, which are published elsewhere notes on current biometric work and unsolved problems Early volumes contained many reports on biological topics, but over the twentieth century, Biometrika became a "journal of statistics in which emphasis is placed on papers containing original theoretical contributions of direct or potential value in applications." Thus, of the five types of contents envisaged by its founders, only the second and to a lesser extent the third remain, largely shorn of their biological roots. In his centenary tribute to Karl Pearson, J. B. S. Haldane likened him to Columbus who "set out for China, and discovered America." The same might be said of Pearson's journal. == Historical reference == To mark the centenary of "one of the world's leading academic journals in statistical theory and methodology" a commemorative volume was produced, containing articles that had appeared in a special issue of the journal and a selection of classic papers published in the journal in the years 1939-71. == Abstracting and indexing == The journal is abstracted and indexed in: According to the Journal Citation Reports, the journal has a 2011 impact factor of 1.912. == References == == External links == Official website Biometrika (1901). The Internet Archive. 2011. 
Biostatistics (or biometry) is the application of statistics to a wide range of topics in biology. The science of biostatistics encompasses the design of biological experiments, especially in medicine, pharmacy, agriculture and fishery; the collection, summarization, and analysis of data from those experiments; and the interpretation of, and inference from, the results. A major branch of this is medical biostatistics, which is exclusively concerned with medicine and health. == History == Biostatistical reasoning and modeling were of critical importance to the foundation theories of modern biology. In the early 1900s, after the rediscovery of Gregor Mendel's Mendelian inheritance work, the gaps in understanding between genetics and evolutionary Darwinism led to vigorous debate among biometricians, such as Walter Weldon and Karl Pearson, and Mendelians, such as Charles Davenport, William Bateson and Wilhelm Johannsen. By the 1930s, statisticians and models built on statistical reasoning had helped to resolve these differences and to produce the neo-Darwinian modern evolutionary synthesis. The leading figures in the establishment of population genetics and this synthesis all relied on statistics and developed its use in biology. Ronald Fisher developed several basic statistical methods in support of his work studying the field experiments at Rothamsted Research, including in his 1930 book The Genetical Theory of Natural Selection Sewall G. Wright developed F-statistics and methods of computing them J. B. S. Haldane's book, The Causes of Evolution, reestablished natural selection as the premier mechanism of evolution by explaining it in terms of the mathematical consequences of Mendelian genetics. These individuals and the work of other biostatisticians, mathematical biologists, and statistically inclined geneticists helped bring together evolutionary biology and genetics into a consistent, coherent whole that could begin to be quantitatively modeled. In parallel to this overall development, the pioneering work of D'Arcy Thompson in On Growth and Form also helped to add quantitative discipline to biological study. Despite the fundamental importance and frequent necessity of statistical reasoning, there may nonetheless have been a tendency among biologists to distrust or deprecate results which are not qualitatively apparent. One anecdote describes Thomas Hunt Morgan banning the Friden calculator from his department at Caltech, saying "Well, I am like a guy who is prospecting for gold along the banks of the Sacramento River in 1849. With a little intelligence, I can reach down and pick up big nuggets of gold. And as long as I can do that, I'm not going to let any people in my department waste scarce resources in placer mining." == Scope and training programs == Almost all educational programmes in biostatistics are at postgraduate level. They are most often found in schools of public health, affiliated with schools of medicine, forestry, or agriculture, or as a focus of application in departments of statistics. In the United States, where several universities have dedicated biostatistics departments, many other top-tier universities integrate biostatistics faculty into statistics or other departments, such as epidemiology. Thus, departments carrying the name "biostatistics" may exist under quite different structures. For instance, relatively new biostatistics departments have been founded with a focus on bioinformatics and computational biology, whereas older departments, typically affiliated with schools of public health, will have more traditional lines of research involving epidemiological studies and clinical trials as well as bioinformatics. In larger universities where both a statistics and a biostatistics department exist, the degree of integration between the two departments may range from the bare minimum to very close collaboration. In general, the difference between a statistics program and a biostatistics program is twofold: (i) statistics departments will often host theoretical/methodological research which are less common in biostatistics programs and (ii) statistics departments have lines of research that may include biomedical applications but also other areas such as industry (quality control), business and economics and biological areas other than medicine. == Recent developments in modern biostatistics == The advent of modern computer technology and relatively cheap computing resources have enabled computer-intensive biostatistical methods like bootstrapping and resampling methods. Furthermore, new biomedical technologies like microarrays, next generation sequencers (for genomics) and mass spectrometry (for proteomics) generate enormous amounts of (redundant) data that can only be analyzed with biostatistical methods. For example, a microarray can measure all the genes of the human genome simultaneously, but only a fraction of them will be differentially expressed in diseased vs. non-diseased states. One might encounter the problem of multicolinearity: Due to high intercorrelation between the predictors (in this case say genes), the information of one predictor might be contained in another one. It could be that only 5% of the predictors are responsible for 90% of the variability of the response. In such a case, one would apply the biostatistical technique of dimension reduction (for example via principal component analysis). Classical statistical techniques like linear or logistic regression and linear discriminant analysis do not work well for high dimensional data (i.e. when the number of observations n is smaller than the number of features or predictors p: n < p). As a matter of fact, one can get quite high R2-values despite very low predictive power of the statistical model. These classical statistical techniques (esp. least squares linear regression) were developed for low dimensional data (i.e. where the number of observations n is much larger than the number of predictors p: n >> p). In cases of high dimensionality, one should always consider an independent validation test set and the corresponding residual sum of squares (RSS) and R2 of the validation test set, not those of the training set. In recent times, random forests have gained popularity. This technique, invented by the statistician Leo Breiman, generates a lot of decision trees randomly and uses them for classification (In classification the response is on a nominal or ordinal scale, as opposed to regression where the response is on a ratio scale). Decision trees have of course the advantage that you can draw them and interpret them (even with a very basic understanding of mathematics and statistics). Random Forrests have thus been used for clinical decision support systems. Gene Set Enrichment Analysis (GSEA) is a new method for analyzing biological high throughput experiments. With this method, one does not consider the perturbation of single genes but of whole (functionally related) gene sets. These gene sets might be known biochemical pathways or otherwise functionally related genes. The advantage of this approach is that it is more robust: It is more likely that a single gene is found to be falsely perturbed than it is that a whole pathway is falsely perturbed. Furthermore, one can integrate the accumulated knowledge about biochemical pathways (like the JAK-STAT signaling pathway) using this approach. == Applications of biostatistics == Public health, including epidemiology, health services research, nutrition, environmental health and healthcare policy & management. Design and analysis of clinical trials in medicine Assessment of severity state of a patient with prognosis of outcome of a disease. Population genetics, and statistical genetics in order to link variation in genotype with a variation in phenotype. This has been used in agriculture to improve crops and farm animals (animal breeding). In biomedical research, this work can assist in finding candidates for gene alleles that can cause or influence predisposition to disease in human genetics Analysis of genomics data, for example from microarray or proteomics experiments. Often concerning diseases or disease stages. Ecology, ecological forecasting Biological sequence analysis Systems biology for gene network inference or pathways analysis. == See also == Bioinformatics Epidemiological method Epidemiology Group size measures Health indicator List of biostatistics journals Mathematical and theoretical biology Quantitative parasitology == References == == External links == The International Biometric Society The Collection of Biostatistics Research Archive Guide to Biostatistics (MedPageToday.com) Biomedical Statistics 
Clinical study design is the formulation of trials and experiments, as well as observational studies in medical, clinical and other types of research (e.g., epidemiological) involving human beings. The goal of a clinical study is to assess the safety, efficacy, and / or the mechanism of action of an investigational medicinal product, or new drug or device that is in development, but potentially not yet approved by a health authority (e.g. FDA). Some of the considerations here are shared under the more general topic of design of experiments but there can be others, in particular related to patient confidentiality and ethics. == Outline of types of designs for clinical studies == === Treatment studies === Randomized controlled trial Blind trial Non-blind trial Adaptive clinical trial Nonrandomized trial (quasi-experiment) Interrupted time series design (measures on a sample or a series of samples from the same population are obtained several times before and after a manipulated event or a naturally occurring event) - considered a type of quasi-experiment === Observational studies === Cohort study Prospective cohort Retrospective cohort Time series study Case-control study Nested case-control study Cross-sectional study Community survey (a type of cross-sectional study) Ecological study == Important considerations == When choosing a study design, many factors must be taken into account. Different types of studies are subject to different types of bias. For example, recall bias is likely to occur in cross-sectional or case-control studies where subjects are asked to recall exposure to risk factors. Subjects with the relevant condition (e.g. breast cancer) may be more likely to recall the relevant exposures that they had undergone (e.g. hormone replacement therapy) than subjects who don't have the condition. The ecological fallacy may occur when conclusions about individuals are drawn from analyses conducted on grouped data. The nature of this type of analysis tends to overestimate the degree of association between variables. === Seasonal studies === Conducting studies in seasonal indications (such as allergies, Seasonal Affective Disorder, influenza, and others) can complicate a trial as patients must be enrolled quickly. Additionally, seasonal variations and weather patterns can affect a seasonal study. == Other terms == The term retrospective study is sometimes used as another term for a case-control study. This use of the term "retrospective study" is misleading, however, and should be avoided because other research designs besides case-control studies are also retrospective in orientation. Superiority trials are designed to demonstrate that one treatment is more effective than another. This type of study design is often used to test the effectiveness of a treatment compared to placebo. Non-inferiority trials are designed to demonstrate that a treatment is at least not appreciably worse than another. This type of study design is often employed when comparing a new treatment to an established medical standard of care. Equivalence trials are designed to demonstrate that one treatment is as effective as another. When using "parallel groups", each patient receives one treatment; in a "crossover study", each patient receives several treatments. A longitudinal study assesses research subjects over two or more points in time; by contrast, a cross-sectional study assesses research subjects at only one point in time (so case-control, cohort, and randomized studies are not cross-sectional). == See also == Conceptual framework Epidemiological methods Epidemiology Experimental control Meta-analysis Operationalization Academic clinical trials == References == == External links == Epidemiologic.org Epidemiologic Inquiry online weblog for epidemiology researchers Epidemiology Forum An epidemiology discussion and forum community to foster debates and collaborations in epidemiology Some aspects of study design Tufts University web site Comparison of strength Description of study designs from the National Cancer Institute Political Science Research Design Handbook Truman State University website 
In ecology, a community or biocoenosis is an assemblage or association of populations of two or more different species occupying the same geographical area and in a particular time. The term community has a variety of uses. In its simplest form it refers to groups of organisms in a specific place or time, for example, "the fish community of Lake Ontario before industrialization". Community ecology or synecology is the study of the interactions between species in communities on many spatial and temporal scales, including the distribution, structure, abundance, demography, and interactions between coexisting populations. The primary focus of community ecology is on the interactions between populations as determined by specific genotypic and phenotypic characteristics. Community ecology has its origin in European plant sociology. Modern community ecology examines patterns such as variation in species richness, equitability, productivity and food web structure (see community structure); it also examines processes such as predator-prey population dynamics, succession, and community assembly. On a deeper level the meaning and value of the community concept in ecology is up for debate. Communities have traditionally been understood on a fine scale in terms of local processes constructing (or destructing) an assemblage of species, such as the way climate change is likely to affect the make-up of grass communities. Recently this local community focus has been criticised. Robert Ricklefs has argued that it is more useful to think of communities on a regional scale, drawing on evolutionary taxonomy and biogeography, where some species or clades evolve and others go extinct. == Theories == === Holistic theory === Clements developed a holistic (or organismic) concept of community, as it was a superorganism or discrete unit, with sharp boundaries. === Individualistic theory === Gleason developed the individualistic (also known as open or continuum) concept of community, with the abundance of a population of a species changing gradually along complex environmental gradients, but individually, not equally to other populations. In that view, it is possible that individualistic distribution of species gives rise to discrete communities as well as to continuum. Niches would not overlap. === Neutral theory === In the neutral theory view of the community (or metacommunity), popularized by Hubbell, the abundance of a population of a species changes not because of the environmental conditions and its niche, which could overlap with others. Each population would have the same adaptive value (competitive and dispersal abilities), and local and regional composition and abundance would be determined primarily by stochastic demographic processes and dispersal limitation. == Interspecific interactions == Species interact in various ways: competition, predation, parasitism, mutualism, commensalism, etc. The organization of a biological community with respect to ecological interactions is referred to as community structure. === Competition === Species can compete with each other for finite resources. It is considered to be an important limiting factor of population size, biomass and species richness. Many types of competition have been described, but proving the existence of these interactions is a matter of debate. Direct competition has been observed between individuals, populations and species, but there is little evidence that competition has been the driving force in the evolution of large groups. Interference competition: occurs when an individual of one species directly interferes with an individual of another species. Examples include a lion chasing a hyena from a kill, or a plant releasing allelopathic chemicals to impede the growth of a competing species. Exploitative competition: occurs via the consumption of resources. When an individual of one species consumes a resource (e.g., food, shelter, sunlight, etc.), that resource is no longer available to be consumed by a member of a second species. Exploitative competition is thought to be more common in nature, but care must be taken to distinguish it from apparent competition. Apparent competition: occurs when two species share a predator. The populations of both species can be depressed by predation without direct exploitative competition. === Predation === Predation is hunting another species for food. This is a positive-negative (+ -) interaction in that the predator species benefits while the prey species is harmed. Some predators kill their prey before eating them (e.g., a hawk killing a mouse). Other predators are parasites that feed on prey while alive (e.g., a vampire bat feeding on a cow). Herbivores feed on plants (e.g., a cow grazing). Predation may affect the population size of predators and prey and the number of species coexisting in a community. === Mutualism === Mutualism is an interaction between species in which both benefit. Examples include Rhizobium bacteria growing in nodules on the roots of legumes and insects pollinating the flowers of angiosperms. === Commensalism === Commensalism is a type of relationship among organisms in which one organism benefits while the other organism is neither benefited nor harmed. The organism that benefited is called the commensal while the other organism that is neither benefited nor harmed is called the host. For example, an epiphytic orchid attached to the tree for support benefits the orchid but neither harms nor benefits the tree. The opposite of commensalism is amensalism, an interspecific relationship in which a product of one organism has a negative effect on another organism. == Community structure == A major research theme among community ecology has been whether ecological communities have a (nonrandom) structure and, if so however to characterise this structure. Forms of community structure include aggregation and nestedness. == See also == == References == == Further reading == Akin, Wallace E. (1991). Global Patterns: Climate, Vegetation, and Soils. University of Oklahoma Press. ISBN 0-8061-2309-5. Barbour, Burke, and Pitts, 1987. Terrestrial Plant Ecology, 2nd ed. Cummings, Menlo Park, CA. Morin, Peter J. (1999). Community Ecology. Wiley-Blackwell Press. ISBN 978-0-86542-350-3. Odum, E. P. (1959) Fundamentals of ecology. W. B. Saunders Co., Philadelphia and London. Ricklefs, R.E. (2005) The Economy of Nature, 6th ed. WH Freeman, USA. Ricketts, Taylor H., Eric Dinerstein, David M. Olson, Colby J. Loucks et al. (WWF) (1999). Terrestrial Ecoregions of North America: a conservation assessment. Island Press. ISBN 1-55963-722-6. == External links == Community, BioMineWiki Identify microbial species in a community, BioMineWiki Glossary, Status and Trends of the Nation's Biological Resources, USGS. Glossary, ENTRIX Environmental Consultants. 
In statistics, count data is a statistical data type, a type of data in which the observations can take only the non-negative integer values {0, 1, 2, 3, ...}, and where these integers arise from counting rather than ranking. The statistical treatment of count data is distinct from that of binary data, in which the observations can take only two values, usually represented by 0 and 1, and from ordinal data, which may also consist of integers but where the individual values fall on an arbitrary scale and only the relative ranking is important. Statistical analyses involving count data can take several forms depending on the context in which the data arise. simple counts, such as the number of occurrences of thunderstorms in a calendar year, observed for several years. categorical data in which the counts represent the numbers of items falling into each of several categories. The latter are treated separately as different methodologies apply, and the following applies to simple counts. == Count variables == An individual piece of count data is often termed a count variable. When such a variable is treated as a random variable, the Poisson, binomial and negative binomial distributions are commonly used to represent its distribution. == Graphical examination == Graphical examination of count data may be aided by the use of data transformations chosen to have the property of stabilising the sample variance. In particular, the square root transformation might be used when data can be approximated by a Poisson distribution (although other transformation have modestly improved properties), while an inverse sine transformation is available when a binomial distribution is preferred. == Relating count data to other variables == Here the count variable would be treated as a dependent variable. Statistical methods such as least squares and analysis of variance are designed to deal with continuous dependent variables. These can be adapted to deal with count data by using data transformations such as the square root transformation, but such methods have several drawbacks; they are approximate at best and estimate parameters that are often hard to interpret. The Poisson distribution can form the basis for some analyses of count data and in this case Poisson regression may be used. This is a special case of the class of generalized linear models which also contains specific forms of model capable of using the binomial distribution (binomial regression, logistic regression) or the negative binomial distribution where the assumptions of the Poisson model are violated, in particular when the range of count values is limited or when overdispersion is present. == See also == Index of dispersion == Further reading == Cameron, A. C.; Trivedi, P. K. (2013). Regression Analysis of Count Data Book (Second ed.). Cambridge University Press. ISBN 978-1-107-66727-3. Hilbe, Joseph M. (2011). Negative Binomial Regression (Second ed.). Cambridge University Press. ISBN 978-0-521-19815-8. Winkelmann, Rainer (2008). Econometric Analysis of Count Data (Fifth ed.). Springer. doi:10.1007/978-3-540-78389-3. ISBN 978-3-540-77648-2. 
Data collection is the process of gathering and measuring information on targeted variables in an established systematic fashion, which then enables one to answer relevant questions and evaluate outcomes. The data collection component of research is common to all fields of study including physical and social sciences, humanities and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture quality evidence that then translates to rich data analysis and allows the building of a convincing and credible answer to questions that have been posed. == Importance == Regardless of the field of study or preference for defining data (quantitative or qualitative), accurate data collection is essential to maintaining the integrity of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors occurring. A formal data collection process is necessary as it ensures that data gathered are both defined and accurate and that subsequent decisions based on arguments embodied in the findings are valid. The process provides both a baseline from which to measure and in certain cases a target on what to improve. == Types == Generally there are three types of data collection and they are 1. Surveys: Standardized paper-and -pencil or phone questionnaires that ask predetermined questions. 2. Interviews: Structured or unstructured one-on-one directed conversations with key individuals or leaders in a community. 3. Focus groups: Structured interviews with small groups of like individuals using standardized questions, follow-up questions, and exploration of other topics that arise to better understand participants Consequences from improperly collected data include: Inability to answer research questions accurately. Inability to repeat and validate the study. == Impact of faulty data == Distorted findings result in wasted resources and can mislead other researchers to pursue fruitless avenues of investigation. This compromises decisions for public policy. While the degree of impact from faulty data collection may vary by discipline and the nature of investigation, there is the potential to cause disproportionate harm when these research results are used to support public policy recommendations. == References == == See also == == External links == Bureau of Statistics, Guyana by Arun Sooknarine 
Among the kinds of data that national leaders need are the demographic statistics of their population. Records of births, deaths, marriages, immigration and emigration and a regular census of population provide information that is key to making sound decisions about national policy. A useful summary of such data is the population pyramid. It provides data about the sex and age distribution of the population in an accessible graphical format. Another summary is called the life table. For a cohort of persons born in the same year, it traces and projects their life experiences from birth to death. For a given cohort, the proportion expected to survive each year (or decade in an abridged life table) is presented in tabular or graphical form. The ratio of males to females by age indicates the consequences of differing mortality rates on the sexes. Thus, while values above one are common for newborns, the ratio dwindles until it is well below one for the older population. == See also == Demographic window 
In mathematical modelling and statistical modelling, there are dependent and independent variables. The models investigate how the former depend on the latter. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e. potential reasons for variation. Models test or explain the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly. == Use == === Mathematics === In mathematics, a function is a rule for taking an input (usually number or set of numbers) and providing an output (which is also usually a number). A symbol that stands for an arbitrary input is called an independent variable, while a symbol that stands for an arbitrary output is called a dependent variable. The most common symbol for the input is x, and the most common symbol for the output is y; the function itself is commonly written . It is possible to have multiple independent variables and/or multiple dependent variables. For instance, in multivariable calculus, one often encounters functions of the form , where z is an dependent variable and x and y are independent variables. Functions with multiple outputs are often written as vector-valued functions. In advanced mathematics, a function between a set X and a set Y is a subset of the Cartesian product such that every element of X appears in an ordered pair with exactly one element of Y. In this situation, a symbol representing an element of X may be called a independent variable and a symbol representing an element of Y may be called an dependent variable, such as when X is a manifold and the symbol x represents an arbitrary point in the manifold. However, many advanced textbooks do not distinguish between dependent and independent variables. === Statistics === In a statistics experiment, the dependent variable is the event studied and expected to change whenever the independent variable is altered. In data mining tools (for multivariate statistics and machine learning), the depending variable is assigned a role as target variable (or in some tools as label attribute), while a dependent variable may be assigned a role as regular variable. Known values for the target variable are provided for the training data set and test data set, but should be predicted for other data. The target variable is used in supervised learning algorithms but not in non-supervised learning. === Modelling === In mathematical modelling, the dependent variable is studied to see if and how much it varies as the independent variables vary. In the simple stochastic linear model the term is the i th value of the dependent variable and is i th value of the independent variable. The term is known as the "error" and contains the variability of the dependent variable not explained by the independent variable. With multiple independent variables, the expression is: , where n is the number of independent variables. === Simulation === In simulation, the dependent variable is changed in response to changes in the independent variables. == Statistics synonyms == An independent variable is also known as a "predictor variable", "regressor", "controlled variable", "manipulated variable", "explanatory variable", "exposure variable" (see reliability theory), "risk factor" (see medical statistics), "feature" (in machine learning and pattern recognition) or an "input variable." A dependent variable is also known as a "response variable", "regressand", "predicted variable", "measured variable", "explained variable", "experimental variable", "responding variable", "outcome variable", and "output variable". "Explanatory variable" is preferred by some authors over "independent variable" when the quantities treated as "independent variables" may not be statistically independent. If the independent variable is referred to as an "explanatory variable" then the term "response variable" is preferred by some authors for the dependent variable. "Explained variable" is preferred by some authors over "dependent variable" when the quantities treated as "dependent variables" may not be statistically dependent. If the dependent variable is referred to as an "explained variable" then the term "predictor variable" is preferred by some authors for the independent variable. Variables may also be referred to by their form: continuous, binary/dichotomous, nominal categorical, and ordinal categorical, among others. == Other variables == A variable may be thought to alter the dependent or independent variables, but may not actually be the focus of the experiment. So that variable will be kept constant or monitored to try to minimise its effect on the experiment. Such variables may be designated as either a "controlled variable" , "control variable", or "extraneous variable". Extraneous variables, if included in a regression as independent variables, may aid a researcher with accurate response parameter estimation, prediction, and goodness of fit, but are not of substantive interest to the hypothesis under examination. For example, in a study examining the effect of post-secondary education on lifetime earnings, some extraneous variables might be gender, ethnicity, social class, genetics, intelligence, age, and so forth. A variable is extraneous only when it can be assumed (or shown) to influence the dependent variable. If included in a regression, it can improve the fit of the model. If it is excluded from the regression and if it has a non-zero covariance with one or more of the independent variables of interest, its omission will bias the regression's result for the effect of that independent variable of interest. This effect is called confounding or omitted variable bias; in these situations, design changes and/or statistical control is necessary. Extraneous variables are often classified into three types: Subject variables, which are the characteristics of the individuals being studied that might affect their actions. These variables include age, gender, health status, mood, background, etc. Blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how a person behaves. Gender, the presence of racial discrimination, language, or other factors may qualify as such variables. Situational variables are features of the environment in which the study or research was conducted, which have a bearing on the outcome of the experiment in a negative way. Included are the air temperature, level of activity, lighting, and the time of day. In quasi-experiments, differentiating between dependent and other variables may be downplayed in favour of differentiating between those variables that can be altered by the researcher and those that cannot. Variables in quasi-experiments may be referred to as "extraneous variables", "subject variables", "blocking variables", "situational variables", "pseudo-independent variables", "ex post facto variables", "natural group variables" or "non-manipulated variables". In modelling, variability that is not covered by the independent variable is designated by and is known as the "residual", "side effect", "error", "unexplained share", "residual variable", or "tolerance". == Examples == Effects of vitamin C on life span In a study whether taking vitamin C pills daily make people live longer, researchers will dictate the vitamin C intake of a group of people over time. One part of the group will be given vitamin C pills daily. The other part of the group will be given a placebo pill. Nobody in the group knows which part they are in. The researchers will check the life span of the people in both groups. Here, the dependent variable is the life span and the independent variable is a binary variable for the use or non-use of vitamin C. Effect of fertilizer on plant growth In a study measuring the influence of different quantities of fertilizer on plant growth, the independent variable would be the amount of fertilizer used. The dependent variable would be the growth in height or mass of the plant. The controlled variables would be the type of plant, the type of fertilizer, the amount of sunlight the plant gets, the size of the pots, etc. Effect of drug dosage on symptom severity In a study of how different doses of a drug affect the severity of symptoms, a researcher could compare the frequency and intensity of symptoms when different doses are administered. Here the independent variable is the dose and the dependent variable is the frequency/intensity of symptoms. Effect of temperature on pigmentation In measuring the amount of color removed from beetroot samples at different temperatures, temperature is the independent variable and amount of pigment removed is the dependent variable. Effect of education on wealth In sociology, in measuring the effect of education on income or wealth, the dependent variable is level of income/wealth and the independent variable is the education level of the individual. == References == 
Descriptive statistics is the discipline of quantitatively describing the main features of a collection of information, or the quantitative description itself. Descriptive statistics are distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aim to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent. This generally means that descriptive statistics, unlike inferential statistics, are not developed on the basis of probability theory. Even when a data analysis draws its main conclusions using inferential statistics, descriptive statistics are generally also presented. For example in a paper reporting on a study involving human subjects, there typically appears a table giving the overall sample size, sample sizes in important subgroups (e.g., for each treatment or exposure group), and demographic or clinical characteristics such as the average age, the proportion of subjects of each sex, and the proportion of subjects with related comorbidities. Some measures that are commonly used to describe a data set are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness. == Use in statistical analysisEdit == Descriptive status provides simple summaries about the sample and about the observations that have been made. Such summaries may be either quantitative, i.e. summary statistics, or visual, i.e. simple-to-understand graphs. These summaries may either form the basis of the initial description of the data as part of a more extensive statistical analysis, or they may be sufficient in and of themselves for a particular investigation. For example, the shooting percentage in basketball is a descriptive statistic that summarizes the performance of a player or a team. This number is the number of shots made divided by the number of shots taken. For example, a player who shoots 33% is making approximately one shot in every three. The percentage summarizes or describes multiple discrete events. Consider also the grade point average. This single number describes the general performance of a student across the range of their course experiences. The use of descriptive and summary statistics has an extensive history and, indeed, the simple tabulation of populations and of economic data was the first way the topic of statistics appeared. More recently, a collection of summarisation techniques has been formulated under the heading of exploratory data analysis: an example of such a technique is the box plot. In the business world, descriptive statistics provides a useful summary of many types of data. For example, investors and brokers may use a historical account of return behavior by performing empirical and analytical analyses on their investments in order to make better investing decisions in the future. === Univariate analysisEdit === Univariate analysis involves describing the distribution of a single variable, including its central tendency (including the mean, median, and mode) and dispersion (including the range and quantiles of the data-set, and measures of spread such as the variance and standard deviation). The shape of the distribution may also be described via indices such as skewness and kurtosis. Characteristics of a variable's distribution may also be depicted in graphical or tabular format, including histograms and stem-and-leaf display. === Bivariate analysisEdit === When a sample consists of more than one variable, descriptive statistics may be used to describe the relationship between pairs of variables. In this case, descriptive statistics include: Cross-tabulations and contingency tables Graphical representation via scatterplots Quantitative measures of dependence Descriptions of conditional distributions The main reason for differentiating univariate and bivariate analysis is that bivariate analysis is not only simple descriptive analysis, but also it describes the relationship between two different variables. Quantitative measures of dependence include correlation (such as Pearson's r when both variables are continuous, or Spearman's rho if one or both are not) and covariance (which reflects the scale variables are measured on). The slope, in regression analysis, also reflects the relationship between variables. The unstandardised slope indicates the unit change in the criterion variable for a one unit change in the predictor. The standardised slope indicates this change in standardised (z-score) units. Highly skewed data are often transformed by taking logarithms. Use of logarithms makes graphs more symmetrical and look more similar to the normal distribution, making them easier to interpret intuitively. == ReferencesEdit == == External linksEdit == Descriptive Statistics Lecture: University of Pittsburgh Supercourse: http://www.pitt.edu/~super1/lecture/lec0421/index.htm 
In mathematical modelling and statistical modelling, there are dependent and independent variables. The models investigate how the former depend on the latter. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e. potential reasons for variation. Models test or explain the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly. == Use == === Mathematics === In mathematics, a function is a rule for taking an input (usually number or set of numbers) and providing an output (which is also usually a number). A symbol that stands for an arbitrary input is called an independent variable, while a symbol that stands for an arbitrary output is called a dependent variable. The most common symbol for the input is x, and the most common symbol for the output is y; the function itself is commonly written . It is possible to have multiple independent variables and/or multiple dependent variables. For instance, in multivariable calculus, one often encounters functions of the form , where z is an dependent variable and x and y are independent variables. Functions with multiple outputs are often written as vector-valued functions. In advanced mathematics, a function between a set X and a set Y is a subset of the Cartesian product such that every element of X appears in an ordered pair with exactly one element of Y. In this situation, a symbol representing an element of X may be called a independent variable and a symbol representing an element of Y may be called an dependent variable, such as when X is a manifold and the symbol x represents an arbitrary point in the manifold. However, many advanced textbooks do not distinguish between dependent and independent variables. === Statistics === In a statistics experiment, the dependent variable is the event studied and expected to change whenever the independent variable is altered. In data mining tools (for multivariate statistics and machine learning), the depending variable is assigned a role as target variable (or in some tools as label attribute), while a dependent variable may be assigned a role as regular variable. Known values for the target variable are provided for the training data set and test data set, but should be predicted for other data. The target variable is used in supervised learning algorithms but not in non-supervised learning. === Modelling === In mathematical modelling, the dependent variable is studied to see if and how much it varies as the independent variables vary. In the simple stochastic linear model the term is the i th value of the dependent variable and is i th value of the independent variable. The term is known as the "error" and contains the variability of the dependent variable not explained by the independent variable. With multiple independent variables, the expression is: , where n is the number of independent variables. === Simulation === In simulation, the dependent variable is changed in response to changes in the independent variables. == Statistics synonyms == An independent variable is also known as a "predictor variable", "regressor", "controlled variable", "manipulated variable", "explanatory variable", "exposure variable" (see reliability theory), "risk factor" (see medical statistics), "feature" (in machine learning and pattern recognition) or an "input variable." A dependent variable is also known as a "response variable", "regressand", "predicted variable", "measured variable", "explained variable", "experimental variable", "responding variable", "outcome variable", and "output variable". "Explanatory variable" is preferred by some authors over "independent variable" when the quantities treated as "independent variables" may not be statistically independent. If the independent variable is referred to as an "explanatory variable" then the term "response variable" is preferred by some authors for the dependent variable. "Explained variable" is preferred by some authors over "dependent variable" when the quantities treated as "dependent variables" may not be statistically dependent. If the dependent variable is referred to as an "explained variable" then the term "predictor variable" is preferred by some authors for the independent variable. Variables may also be referred to by their form: continuous, binary/dichotomous, nominal categorical, and ordinal categorical, among others. == Other variables == A variable may be thought to alter the dependent or independent variables, but may not actually be the focus of the experiment. So that variable will be kept constant or monitored to try to minimise its effect on the experiment. Such variables may be designated as either a "controlled variable" , "control variable", or "extraneous variable". Extraneous variables, if included in a regression as independent variables, may aid a researcher with accurate response parameter estimation, prediction, and goodness of fit, but are not of substantive interest to the hypothesis under examination. For example, in a study examining the effect of post-secondary education on lifetime earnings, some extraneous variables might be gender, ethnicity, social class, genetics, intelligence, age, and so forth. A variable is extraneous only when it can be assumed (or shown) to influence the dependent variable. If included in a regression, it can improve the fit of the model. If it is excluded from the regression and if it has a non-zero covariance with one or more of the independent variables of interest, its omission will bias the regression's result for the effect of that independent variable of interest. This effect is called confounding or omitted variable bias; in these situations, design changes and/or statistical control is necessary. Extraneous variables are often classified into three types: Subject variables, which are the characteristics of the individuals being studied that might affect their actions. These variables include age, gender, health status, mood, background, etc. Blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how a person behaves. Gender, the presence of racial discrimination, language, or other factors may qualify as such variables. Situational variables are features of the environment in which the study or research was conducted, which have a bearing on the outcome of the experiment in a negative way. Included are the air temperature, level of activity, lighting, and the time of day. In quasi-experiments, differentiating between dependent and other variables may be downplayed in favour of differentiating between those variables that can be altered by the researcher and those that cannot. Variables in quasi-experiments may be referred to as "extraneous variables", "subject variables", "blocking variables", "situational variables", "pseudo-independent variables", "ex post facto variables", "natural group variables" or "non-manipulated variables". In modelling, variability that is not covered by the independent variable is designated by and is known as the "residual", "side effect", "error", "unexplained share", "residual variable", or "tolerance". == Examples == Effects of vitamin C on life span In a study whether taking vitamin C pills daily make people live longer, researchers will dictate the vitamin C intake of a group of people over time. One part of the group will be given vitamin C pills daily. The other part of the group will be given a placebo pill. Nobody in the group knows which part they are in. The researchers will check the life span of the people in both groups. Here, the dependent variable is the life span and the independent variable is a binary variable for the use or non-use of vitamin C. Effect of fertilizer on plant growth In a study measuring the influence of different quantities of fertilizer on plant growth, the independent variable would be the amount of fertilizer used. The dependent variable would be the growth in height or mass of the plant. The controlled variables would be the type of plant, the type of fertilizer, the amount of sunlight the plant gets, the size of the pots, etc. Effect of drug dosage on symptom severity In a study of how different doses of a drug affect the severity of symptoms, a researcher could compare the frequency and intensity of symptoms when different doses are administered. Here the independent variable is the dose and the dependent variable is the frequency/intensity of symptoms. Effect of temperature on pigmentation In measuring the amount of color removed from beetroot samples at different temperatures, temperature is the independent variable and amount of pigment removed is the dependent variable. Effect of education on wealth In sociology, in measuring the effect of education on income or wealth, the dependent variable is level of income/wealth and the independent variable is the education level of the individual. == References == 
In mathematical modelling and statistical modelling, there are dependent and independent variables. The models investigate how the former depend on the latter. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e. potential reasons for variation. Models test or explain the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly. == Use == === Mathematics === In mathematics, a function is a rule for taking an input (usually number or set of numbers) and providing an output (which is also usually a number). A symbol that stands for an arbitrary input is called an independent variable, while a symbol that stands for an arbitrary output is called a dependent variable. The most common symbol for the input is x, and the most common symbol for the output is y; the function itself is commonly written . It is possible to have multiple independent variables and/or multiple dependent variables. For instance, in multivariable calculus, one often encounters functions of the form , where z is an dependent variable and x and y are independent variables. Functions with multiple outputs are often written as vector-valued functions. In advanced mathematics, a function between a set X and a set Y is a subset of the Cartesian product such that every element of X appears in an ordered pair with exactly one element of Y. In this situation, a symbol representing an element of X may be called a independent variable and a symbol representing an element of Y may be called an dependent variable, such as when X is a manifold and the symbol x represents an arbitrary point in the manifold. However, many advanced textbooks do not distinguish between dependent and independent variables. === Statistics === In a statistics experiment, the dependent variable is the event studied and expected to change whenever the independent variable is altered. In data mining tools (for multivariate statistics and machine learning), the depending variable is assigned a role as target variable (or in some tools as label attribute), while a dependent variable may be assigned a role as regular variable. Known values for the target variable are provided for the training data set and test data set, but should be predicted for other data. The target variable is used in supervised learning algorithms but not in non-supervised learning. === Modelling === In mathematical modelling, the dependent variable is studied to see if and how much it varies as the independent variables vary. In the simple stochastic linear model the term is the i th value of the dependent variable and is i th value of the independent variable. The term is known as the "error" and contains the variability of the dependent variable not explained by the independent variable. With multiple independent variables, the expression is: , where n is the number of independent variables. === Simulation === In simulation, the dependent variable is changed in response to changes in the independent variables. == Statistics synonyms == An independent variable is also known as a "predictor variable", "regressor", "controlled variable", "manipulated variable", "explanatory variable", "exposure variable" (see reliability theory), "risk factor" (see medical statistics), "feature" (in machine learning and pattern recognition) or an "input variable." A dependent variable is also known as a "response variable", "regressand", "predicted variable", "measured variable", "explained variable", "experimental variable", "responding variable", "outcome variable", and "output variable". "Explanatory variable" is preferred by some authors over "independent variable" when the quantities treated as "independent variables" may not be statistically independent. If the independent variable is referred to as an "explanatory variable" then the term "response variable" is preferred by some authors for the dependent variable. "Explained variable" is preferred by some authors over "dependent variable" when the quantities treated as "dependent variables" may not be statistically dependent. If the dependent variable is referred to as an "explained variable" then the term "predictor variable" is preferred by some authors for the independent variable. Variables may also be referred to by their form: continuous, binary/dichotomous, nominal categorical, and ordinal categorical, among others. == Other variables == A variable may be thought to alter the dependent or independent variables, but may not actually be the focus of the experiment. So that variable will be kept constant or monitored to try to minimise its effect on the experiment. Such variables may be designated as either a "controlled variable" , "control variable", or "extraneous variable". Extraneous variables, if included in a regression as independent variables, may aid a researcher with accurate response parameter estimation, prediction, and goodness of fit, but are not of substantive interest to the hypothesis under examination. For example, in a study examining the effect of post-secondary education on lifetime earnings, some extraneous variables might be gender, ethnicity, social class, genetics, intelligence, age, and so forth. A variable is extraneous only when it can be assumed (or shown) to influence the dependent variable. If included in a regression, it can improve the fit of the model. If it is excluded from the regression and if it has a non-zero covariance with one or more of the independent variables of interest, its omission will bias the regression's result for the effect of that independent variable of interest. This effect is called confounding or omitted variable bias; in these situations, design changes and/or statistical control is necessary. Extraneous variables are often classified into three types: Subject variables, which are the characteristics of the individuals being studied that might affect their actions. These variables include age, gender, health status, mood, background, etc. Blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how a person behaves. Gender, the presence of racial discrimination, language, or other factors may qualify as such variables. Situational variables are features of the environment in which the study or research was conducted, which have a bearing on the outcome of the experiment in a negative way. Included are the air temperature, level of activity, lighting, and the time of day. In quasi-experiments, differentiating between dependent and other variables may be downplayed in favour of differentiating between those variables that can be altered by the researcher and those that cannot. Variables in quasi-experiments may be referred to as "extraneous variables", "subject variables", "blocking variables", "situational variables", "pseudo-independent variables", "ex post facto variables", "natural group variables" or "non-manipulated variables". In modelling, variability that is not covered by the independent variable is designated by and is known as the "residual", "side effect", "error", "unexplained share", "residual variable", or "tolerance". == Examples == Effects of vitamin C on life span In a study whether taking vitamin C pills daily make people live longer, researchers will dictate the vitamin C intake of a group of people over time. One part of the group will be given vitamin C pills daily. The other part of the group will be given a placebo pill. Nobody in the group knows which part they are in. The researchers will check the life span of the people in both groups. Here, the dependent variable is the life span and the independent variable is a binary variable for the use or non-use of vitamin C. Effect of fertilizer on plant growth In a study measuring the influence of different quantities of fertilizer on plant growth, the independent variable would be the amount of fertilizer used. The dependent variable would be the growth in height or mass of the plant. The controlled variables would be the type of plant, the type of fertilizer, the amount of sunlight the plant gets, the size of the pots, etc. Effect of drug dosage on symptom severity In a study of how different doses of a drug affect the severity of symptoms, a researcher could compare the frequency and intensity of symptoms when different doses are administered. Here the independent variable is the dose and the dependent variable is the frequency/intensity of symptoms. Effect of temperature on pigmentation In measuring the amount of color removed from beetroot samples at different temperatures, temperature is the independent variable and amount of pigment removed is the dependent variable. Effect of education on wealth In sociology, in measuring the effect of education on income or wealth, the dependent variable is level of income/wealth and the independent variable is the education level of the individual. == References == 
In machine learning and pattern recognition, a feature is an individual measurable property of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of "feature" is related to that of explanatory variable used in statistical techniques such as linear regression. The initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability. Extracting or selecting features is a combination of art and science; developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert. Automating this process is feature learning, where a machine not only uses features for learning, but learns the features itself. == Classification == A set of numeric features can be conveniently described by a feature vector. An example of reaching a two way classification from a feature vector (related to the perceptron) consists of calculating the scalar product between the feature vector and a vector of weights, comparing the result with a threshold, and deciding the class based on the comparison. Algorithms for classification from a feature vector include nearest neighbor classification, neural networks, and statistical techniques such as Bayesian approaches. == Examples == In character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others. In speech recognition, features for recognizing phonemes can include noise ratios, length of sounds, relative power, filter matches and many others. In spam detection algorithms, features may include the presence or absence of certain email headers, the email structure, the language, the frequency of specific terms, the grammatical correctness of the text. In computer vision, there are a large number of possible features, such as edges and objects. == References == == See also == Covariate Hashing trick 
Frequentist inference is one of a number of possible techniques of formulating generally applicable schemes for making statistical inference: drawing conclusions from sample data by the emphasis on the frequency or proportion of the data. An alternative name is frequentist statistics. This is the inference framework in which the well-established methodologies of statistical hypothesis testing and confidence intervals are based. Other than frequentistic inference, the main alternative approach to statistical inference is Bayesian inference, while another is fiducial inference. While "Bayesian inference" is sometimes held to include the approach to inference leading to optimal decisions, a more restricted view is taken here for simplicity. == Basis == Frequentist inference has been associated with the frequentist interpretation of probability, specifically that any given experiment can be considered as one of an infinite sequence of possible repetitions of the same experiment, each capable of producing statistically independent results. In this view, the frequentist inference approach to drawing conclusions from data is effectively to require that the correct conclusion should be drawn with a given (high) probability, among this notional set of repetitions. However, exactly the same procedures can be developed under a subtly different formulation. This is one where a pre-experiment point of view is taken. It can be argued that the design of an experiment should include, before undertaking the experiment, decisions about exactly what steps will be taken to reach a conclusion from the data yet to be obtained. These steps can be specified by the scientist so that there is a high probability of reaching a correct decision where, in this case, the probability relates to a yet to occur set of random events and hence does not rely on the frequency interpretation of probability. This formulation has been discussed by Neyman, among others. Similarly, Bayesian inference has often been thought of as almost equivalent to the Bayesian interpretation of probability and thus that the essential difference between frequentist inference and Bayesian inference is the same as the difference between the two interpretations of what a "probability" means. However, where appropriate, Bayesian inference (meaning in this case an application of Bayes' theorem) is used by those employing a frequentist interpretation of probabilities. There are two major differences in the frequentist and Bayesian approaches to inference that are not included in the above consideration of the interpretation of probability: In a frequentist approach to inference, unknown parameters are often, but not always, treated as having fixed but unknown values that are not capable of being treated as random variates in any sense, and hence there is no way that probabilities can be associated with them. In contrast, a Bayesian approach to inference does allow probabilities to be associated with unknown parameters, where these probabilities can sometimes have a frequency probability interpretation as well as a Bayesian one. The Bayesian approach allows these probabilities to have an interpretation as representing the scientist's belief that given values of the parameter are true [see Bayesian probability - Personal probabilities and objective methods for constructing priors]. While "probabilities" are involved in both approaches to inference, the probabilities are associated with different types of things. The result of a Bayesian approach can be a probability distribution for what is known about the parameters given the results of the experiment or study. The result of a frequentist approach is either a "true or false" conclusion from a significance test or a conclusion in the form that a given sample-derived confidence interval covers the true value: either of these conclusions has a given probability of being correct, where this probability has either a frequency probability interpretation or a pre-experiment interpretation. == See also == Probability interpretations == References == 
Grouped data is a statistical term used in data analysis. A raw dataset can be organized by constructing a table showing the frequency distribution of the variable (whose values are given in the raw dataset). Such a frequency table is often referred to as grouped data. == Example == The idea of grouped data can be illustrated by considering the following raw dataset: Table 1: Time taken (in seconds) by a group of students to answer a simple math question The above data can be organised into a frequency distribution (or a grouped data) in several ways. One method is to use intervals as a basis. The smallest value in the above data is 8 and the largest is 34. The interval from 8 to 34 is broken up into smaller subintervals (called class intervals). For each class interval, the amount of data items falling in this interval is counted. This number is called the frequency of that class interval. The results are tabulated as a frequency table as follows: Table 2: Frequency distribution of the time taken (in seconds) by the group of students to answer a simple math question Another method of grouping the data is to use some qualitative characteristics instead of numerical intervals. For example, suppose in the above example, there are three types of students: 1) Below normal, if the response time is 5 to 14 seconds, 2) normal if it is between 15 and 24 seconds, and 3) above normal if it is 25 seconds or more, then the grouped data looks like: Table 3: Frequency distribution of the three types of students == Mean of grouped data == An estimate, , of the mean of the population from which the data are drawn can be calculated from the grouped data as: In this formula, x refers to the midpoint of the class intervals, and f is the class frequency. Note that the result of this will be different from the sample mean of the ungrouped data. The mean for the grouped data in the above example, can be calculated as follows: Thus, the mean of the grouped data is == See also == Data binning Level of measurement Frequency distribution Discretization of continuous features Logistic regression#Minimum chi-squared estimator for grouped data == Notes == ^ Newbold et al., 2009, pages 14 to 17 == References == Newbold, P.; Carlson, W.; Thorne, B. (2009). Statistics for Business and Economics (Seventh ed.). Pearson Education. ISBN 978-0-13-507248-6. 
In mathematical modelling and statistical modelling, there are dependent and independent variables. The models investigate how the former depend on the latter. The dependent variables represent the output or outcome whose variation is being studied. The independent variables represent inputs or causes, i.e. potential reasons for variation. Models test or explain the effects that the independent variables have on the dependent variables. Sometimes, independent variables may be included for other reasons, such as for their potential confounding effect, without a wish to test their effect directly. == Use == === Mathematics === In mathematics, a function is a rule for taking an input (usually number or set of numbers) and providing an output (which is also usually a number). A symbol that stands for an arbitrary input is called an independent variable, while a symbol that stands for an arbitrary output is called a dependent variable. The most common symbol for the input is x, and the most common symbol for the output is y; the function itself is commonly written . It is possible to have multiple independent variables and/or multiple dependent variables. For instance, in multivariable calculus, one often encounters functions of the form , where z is an dependent variable and x and y are independent variables. Functions with multiple outputs are often written as vector-valued functions. In advanced mathematics, a function between a set X and a set Y is a subset of the Cartesian product such that every element of X appears in an ordered pair with exactly one element of Y. In this situation, a symbol representing an element of X may be called a independent variable and a symbol representing an element of Y may be called an dependent variable, such as when X is a manifold and the symbol x represents an arbitrary point in the manifold. However, many advanced textbooks do not distinguish between dependent and independent variables. === Statistics === In a statistics experiment, the dependent variable is the event studied and expected to change whenever the independent variable is altered. In data mining tools (for multivariate statistics and machine learning), the depending variable is assigned a role as target variable (or in some tools as label attribute), while a dependent variable may be assigned a role as regular variable. Known values for the target variable are provided for the training data set and test data set, but should be predicted for other data. The target variable is used in supervised learning algorithms but not in non-supervised learning. === Modelling === In mathematical modelling, the dependent variable is studied to see if and how much it varies as the independent variables vary. In the simple stochastic linear model the term is the i th value of the dependent variable and is i th value of the independent variable. The term is known as the "error" and contains the variability of the dependent variable not explained by the independent variable. With multiple independent variables, the expression is: , where n is the number of independent variables. === Simulation === In simulation, the dependent variable is changed in response to changes in the independent variables. == Statistics synonyms == An independent variable is also known as a "predictor variable", "regressor", "controlled variable", "manipulated variable", "explanatory variable", "exposure variable" (see reliability theory), "risk factor" (see medical statistics), "feature" (in machine learning and pattern recognition) or an "input variable." A dependent variable is also known as a "response variable", "regressand", "predicted variable", "measured variable", "explained variable", "experimental variable", "responding variable", "outcome variable", and "output variable". "Explanatory variable" is preferred by some authors over "independent variable" when the quantities treated as "independent variables" may not be statistically independent. If the independent variable is referred to as an "explanatory variable" then the term "response variable" is preferred by some authors for the dependent variable. "Explained variable" is preferred by some authors over "dependent variable" when the quantities treated as "dependent variables" may not be statistically dependent. If the dependent variable is referred to as an "explained variable" then the term "predictor variable" is preferred by some authors for the independent variable. Variables may also be referred to by their form: continuous, binary/dichotomous, nominal categorical, and ordinal categorical, among others. == Other variables == A variable may be thought to alter the dependent or independent variables, but may not actually be the focus of the experiment. So that variable will be kept constant or monitored to try to minimise its effect on the experiment. Such variables may be designated as either a "controlled variable" , "control variable", or "extraneous variable". Extraneous variables, if included in a regression as independent variables, may aid a researcher with accurate response parameter estimation, prediction, and goodness of fit, but are not of substantive interest to the hypothesis under examination. For example, in a study examining the effect of post-secondary education on lifetime earnings, some extraneous variables might be gender, ethnicity, social class, genetics, intelligence, age, and so forth. A variable is extraneous only when it can be assumed (or shown) to influence the dependent variable. If included in a regression, it can improve the fit of the model. If it is excluded from the regression and if it has a non-zero covariance with one or more of the independent variables of interest, its omission will bias the regression's result for the effect of that independent variable of interest. This effect is called confounding or omitted variable bias; in these situations, design changes and/or statistical control is necessary. Extraneous variables are often classified into three types: Subject variables, which are the characteristics of the individuals being studied that might affect their actions. These variables include age, gender, health status, mood, background, etc. Blocking variables or experimental variables are characteristics of the persons conducting the experiment which might influence how a person behaves. Gender, the presence of racial discrimination, language, or other factors may qualify as such variables. Situational variables are features of the environment in which the study or research was conducted, which have a bearing on the outcome of the experiment in a negative way. Included are the air temperature, level of activity, lighting, and the time of day. In quasi-experiments, differentiating between dependent and other variables may be downplayed in favour of differentiating between those variables that can be altered by the researcher and those that cannot. Variables in quasi-experiments may be referred to as "extraneous variables", "subject variables", "blocking variables", "situational variables", "pseudo-independent variables", "ex post facto variables", "natural group variables" or "non-manipulated variables". In modelling, variability that is not covered by the independent variable is designated by and is known as the "residual", "side effect", "error", "unexplained share", "residual variable", or "tolerance". == Examples == Effects of vitamin C on life span In a study whether taking vitamin C pills daily make people live longer, researchers will dictate the vitamin C intake of a group of people over time. One part of the group will be given vitamin C pills daily. The other part of the group will be given a placebo pill. Nobody in the group knows which part they are in. The researchers will check the life span of the people in both groups. Here, the dependent variable is the life span and the independent variable is a binary variable for the use or non-use of vitamin C. Effect of fertilizer on plant growth In a study measuring the influence of different quantities of fertilizer on plant growth, the independent variable would be the amount of fertilizer used. The dependent variable would be the growth in height or mass of the plant. The controlled variables would be the type of plant, the type of fertilizer, the amount of sunlight the plant gets, the size of the pots, etc. Effect of drug dosage on symptom severity In a study of how different doses of a drug affect the severity of symptoms, a researcher could compare the frequency and intensity of symptoms when different doses are administered. Here the independent variable is the dose and the dependent variable is the frequency/intensity of symptoms. Effect of temperature on pigmentation In measuring the amount of color removed from beetroot samples at different temperatures, temperature is the independent variable and amount of pigment removed is the dependent variable. Effect of education on wealth In sociology, in measuring the effect of education on income or wealth, the dependent variable is level of income/wealth and the independent variable is the education level of the individual. == References == 
In mathematics, the term linear function refers to two distinct but related notions: In calculus and related areas, a linear function is a polynomial function of degree zero or one, or is the zero polynomial. In linear algebra and functional analysis, a linear function is a linear map. == As a polynomial functionEdit == In calculus, analytic geometry and related areas, a linear function is a polynomial of degree one or less, including the zero polynomial (the latter not being considered to have degree zero). When the function is of only one variable, it is of the form where a and b are constants, often real numbers. The graph of such a function of one variable is a nonvertical line. a is frequently referred to as the slope of the line, and b as the intercept. For a function of any finite number of independent variables, the general formula is , and the graph is a hyperplane of dimension k. A constant function is also considered linear in this context, as it is a polynomial of degree zero or is the zero polynomial. Its graph, when there is only one independent variable, is a horizontal line. In this context, the other meaning (a linear map) may be referred to as a homogeneous linear function or a linear form. In the context of linear algebra, this meaning (polynomial functions of degree 0 or 1) is a special kind of affine map. == As a linear mapEdit == In linear algebra, a linear function is a map f between two vector spaces that preserves vector addition and scalar multiplication: Here a denotes a constant belonging to some field K of scalars (for example, the real numbers) and x and y are elements of a vector space, which might be K itself. Some authors use "linear function" only for linear maps that take values in the scalar field; these are also called linear functionals. The "linear functions" of calculus qualify as "linear maps" when (and only when) , or, equivalently, when the constant . Geometrically, the graph of the function must pass through the origin. == See alsoEdit == Homogeneous function Nonlinear system Piecewise linear function Linear interpolation Discontinuous linear map == NotesEdit == ^ "The term linear function, which is not used here, means a linear form in some textbooks and an affine function in others." Vaserstein 2006, p. 50-1 ^ Stewart 2012, p. 23 ^ Shores 2007, p. 71 ^ Gelfand 1961 == ReferencesEdit == Izrail Moiseevich Gelfand (1961), Lectures on Linear Algebra, Interscience Publishers, Inc., New York. Reprinted by Dover, 1989. ISBN 0-486-66082-6 Thomas S. Shores (2007), Applied Linear Algebra and Matrix Analysis, Undergraduate Texts in Mathematics, Springer. ISBN 0-387-33195-6 James Stewart (2012), Calculus: Early Transcendentals, edition 7E, Brooks/Cole. ISBN 978-0-538-49790-9 Leonid N. Vaserstein (2006), "Linear Programming", in Leslie Hogben, ed., Handbook of Linear Algebra, Discrete Mathematics and Its Applications, Chapman and Hall/CRC, chap. 50. ISBN 1-584-88510-6 == External linksEdit == 
Statistics is the mathematical science involving the collection, analysis and interpretation of data. A number of specialties have evolved to apply statistical theory and methods to various disciplines. Certain topics have "statistical" in their name but relate to manipulations of probability distributions rather than to statistical analysis. Actuarial science is the discipline that applies mathematical and statistical methods to assess risk in the insurance and finance industries. Astrostatistics is the discipline that applies statistical analysis to the understanding of astronomical data. Biostatistics is a branch of biology that studies biological phenomena and observations by means of statistical analysis, and includes medical statistics. Business analytics is a rapidly developing business process that applies statistical methods to data sets (often very large) to develop new insights and understanding of business performance & opportunities Chemometrics is the science of relating measurements made on a chemical system or process to the state of the system via application of mathematical or statistical methods. Demography is the statistical study of all populations. It can be a very general science that can be applied to any kind of dynamic population, that is, one that changes over time or space. Econometrics is a branch of economics that applies statistical methods to the empirical study of economic theories and relationships. Environmental statistics is the application of statistical methods to environmental science. Weather, climate, air and water quality are included, as are studies of plant and animal populations. Epidemiology is the study of factors affecting the health and illness of populations, and serves as the foundation and logic of interventions made in the interest of public health and preventive medicine. Geostatistics is a branch of geography that deals with the analysis of data from disciplines such as petroleum geology, hydrogeology, hydrology, meteorology, oceanography, geochemistry, geography. Machine Learning Operations research (or Operational Research) is an interdisciplinary branch of applied mathematics and formal science that uses methods such as mathematical modeling, statistics, and algorithms to arrive at optimal or near optimal solutions to complex problems. Population ecology is a sub-field of ecology that deals with the dynamics of species populations and how these populations interact with the environment. Psychometric is the theory and technique of educational and psychological measurement of knowledge, abilities, attitudes, and personality traits. Quality control reviews the factors involved in manufacturing and production; it can make use of statistical sampling of product items to aid decisions in process control or in accepting deliveries. Quantitative psychology is the science of statistically explaining and changing mental processes and behaviors in humans. Reliability Engineering is the study of the ability of a system or component to perform its required functions under stated conditions for a specified period of time Statistical finance, an area of econophysics, is an empirical attempt to shift finance from its normative roots to a positivist framework using exemplars from statistical physics with an emphasis on emergent or collective properties of financial markets. Statistical mechanics is the application of probability theory, which includes mathematical tools for dealing with large populations, to the field of mechanics, which is concerned with the motion of particles or objects when subjected to a force. Statistical physics is one of the fundamental theories of physics, and uses methods of probability theory in solving physical problems. Statistical Signal Processing Statistical thermodynamics is the study of the microscopic behaviors of thermodynamic systems using probability theory and provides a molecular level interpretation of thermodynamic quantities such as work, heat, free energy, and entropy. == See also == List of statistics topics 
Medical statistics deals with applications of statistics to medicine and the health sciences, including epidemiology, public health, forensic medicine, and clinical research. Medical statistics has been a recognized branch of statistics in the United Kingdom for more than 40 years but the term has not come into general use in North America, where the wider term 'biostatistics' is more commonly used. However, "biostatistics" more commonly connotes all applications of statistics to biology. Medical Statistics are a sub discipline of Statistics. "It is the science of summarizing, collecting, presenting and interpreting data in medical practice, and using them to estimate the magnitude of associations and test hypotheses. It has a central role in medical investigations. It not only provides a way of organizing information on a wider and more formal basis than relying on the exchange of anecdotes and personal experience, but also takes into account the intrinsic variation inherent in most biological processes." == Pharmaceutical statistics == Pharmaceutical statistics is the application of statistics to matters concerning the pharmaceutical industry. This can be from issues of design of experiments, to analysis of drug trials, to issues of commercialization of a medicine. There are many professional bodies concerned with this field including: European Federation of Statisticians in the Pharmaceutical Industry (EFSPI) Statisticians In The Pharmaceutical Industry (PSI) There are also journals including: Statistics in Medicine Pharmaceutical Statistics == Basic concepts == For describing situations Incidence (epidemiology) vs. Prevalence vs. Cumulative incidence Transmission rate vs. Force of infection Mortality rate vs. Standardized mortality ratio vs. Age-standardized mortality rate Pandemic vs. Epidemic vs. Endemic vs. Syndemic Serial interval vs. Incubation period Cancer cluster Sexual network Years of potential life lost Maternal mortality rate Perinatal mortality rate Low Birth weight ratio For assessing the effectiveness of an intervention Absolute risk reduction Control event rate Experimental event rate Number needed to harm Number needed to treat Odds ratio Relative risk reduction Relative risk Relative survival Minimal clinically important difference == Related statistical theory == Survival analysis Proportional hazards models Active control trials: Clinical trials in which a kind of new treatment is compared with some other active agent rather than a placebo. ADLS(Activities of daily living scale): It is a scale designed to measure physical ability/disability that is used in investigations of a variety of chronic disabling conditions, such as arthritis. This scale is based on scoring responses to questions about self-care, grooming, etc. Actuarial statistics: The statistics used by actuaries to calculate liabilities, evaluate risks and plan the financial course of insurance, pensions, etc. == See also == Herd immunity Rare disease Hilda Mary Woods - the first author (with William Russell) of the first British textbook of medical statistics, published in 1931 == References == == Further reading == Altman, D.G. (1991), Practical Statistics for Medical Research, CRC Press, ISBN 978-0-412-27630-9 Armitage, P.; Berry, G.; Matthews, J.N.S. (2002), Statistical Methods in Medical Research, Blackwell, ISBN 978-0-632-05257-8 Bland, J. Martin (2000), An Introduction to Medical Statistics (3rd ed.), Oxford: OUP, ISBN 978-0-19-263269-2 Kirkwood, B.R.; Sterne, J.A.C. (2003), Essential Medical Statistics (2nd ed.), Blackwell, ISBN 978-0-86542-871-3 Petrie, Aviva; Sabin, Caroline (2005), Medical Statistics at a Glance (2nd ed.), WileyBlackwell, ISBN 978-1-4051-2780-6 Onwude, Joseph (2008), Learn Medical Statistics (2nd ed.), DesignsOnline.co.uk == External links == Health-EU Portal EU health statistics 
Methods engineering is a subspecialty of industrial engineering and manufacturing engineering concerned with human integration in industrial production processes. == Overview == Alternatively it can be described as the design of the productive process in which a person is involved. The task of the Methods engineer is to decide where humans will be utilized in the process of converting raw materials to finished products and how workers can most effectively perform their assigned tasks. The terms operation analysis, work design and simplification, and methods engineering and corporate re-engineering are frequently used interchangeably. Lowering costs and increasing reliability and productivity are the objectives of methods engineering. These objectives are met in a five step sequence as follows: Project selection, data acquisition and presentation, data analysis, development of an ideal method based on the data analysis and, finally, presentation and implementation of the method. == Methods engineering topics == === Project selection === Methods engineers typically work on projects involving new product design, products with a high cost of production to profit ratio, and products associated with having poor quality issues. Different methods of project selection include the Pareto analysis, fish diagrams, Gantt charts, PERT charts, and job/work site analysis guides. === Data acquisition and presentation === Data that needs to be collected are specification sheets for the product, design drawings, quantity and delivery requirements, and projections as to how the product will perform or has performed in the market. The Gantt process chart can assist in the analysis of the man to machine interaction and it can aid in establishing the optimum number of workers and machines subject to the financial constraints of the operation. A flow diagram is frequently employed to represent the manufacturing process associated with the product. === Data analysis === Data analysis enables the methods engineer to make decisions about several things, including: purpose of the operation, part design characteristics, specifications and tolerances of parts, materials, manufacturing process design, setup and tooling, working conditions, material handling, plant layout, and workplace design. Knowing the specifics (who, what, when, where, why, and how) of product manufacturing assists in the development of an optimum manufacturing method. === Ideal method development === Equations of synchronous and random servicing as well as line balancing are used to determine the ideal worker to machine ratio for the process or product chosen. Synchronous servicing is defined as the process where a machine is assigned to more than one operator, and the assigned operators and machine are occupied during the whole operating cycle. Random servicing of a facility, as the name indicates, is defined as a servicing process with a random time of occurrence and need of servicing variables. Line balancing equations determine the ideal number of workers needed on a production line to enable it to work at capacity. === Presentation and methods implementation === The industrial process or operation can be optimized using a variety of available methods. Each method design has its advantages and disadvantages. The best overall method is chosen using selection criteria and concepts involving value engineering, cost-benefit analysis, crossover charts, and economic analysis. The outcome of the selection process is then presented to the company for implementation at the plant. This last step involves "selling the idea" to the company brass, a skill the methods engineer must develop in addition to the normal engineering qualifications. == See also == Work design Motion analysis == References == 
A mixed model is a statistical model containing both fixed effects and random effects. These models are useful in a wide variety of disciplines in the physical, biological and social sciences. They are particularly useful in settings where repeated measurements are made on the same statistical units (longitudinal study), or where measurements are made on clusters of related statistical units. Because of their advantage in dealing with missing values, mixed effects models are often preferred over more traditional approaches such as repeated measures ANOVA. == History and current status == Ronald Fisher introduced random effects models to study the correlations of trait values between relatives. In the 1950s, Charles Roy Henderson provided best linear unbiased estimates (BLUE) of fixed effects and best linear unbiased predictions (BLUP) of random effects. Subsequently, mixed modeling has become a major area of statistical research, including work on computation of maximum likelihood estimates, non-linear mixed effect models, missing data in mixed effects models, and Bayesian estimation of mixed effects models. Mixed models are applied in many disciplines where multiple correlated measurements are made on each unit of interest. They are prominently used in research involving human and animal subjects in fields ranging from genetics to marketing, and have also been used in industrial statistics. == Definition == In matrix notation a mixed model can be represented as where is a known vector of observations, with mean ; is an unknown vector of fixed effects; is an unknown vector of random effects, with mean and variance-covariance matrix ; is an unknown vector of random errors, with mean and variance ; and are known design matrices relating the observations to and , respectively. == Estimation == The joint density of and can be written as: . Assuming normality, , and , and maximizing the joint density for and , gives Henderson's "mixed model equations" (MME): The solutions to the MME, and are best linear unbiased estimates (BLUE) and predictors (BLUP) for and , respectively. This is a consequence of the Gauss-Markov theorem when the conditional variance of the outcome is not scalable to the identity matrix. When the conditional variance is known, then the inverse variance weighted least squares estimate is BLUE. However, the conditional variance is rarely, if ever, known. So it is desirable to jointly estimate the variance and weighted parameter estimates when solving MMEs. One method used to fit such mixed models is that of the EM algorithm where the variance components are treated as unobserved nuisance parameters in the joint likelihood. Currently, this is the implemented method for the major statistical software packages R (lme in the nlme library) and SAS (proc mixed). The solution to the mixed model equations is a maximum likelihood estimate when the distribution of the errors is normal. == See also == Fixed effects model Generalized linear mixed model Linear regression Mixed-design analysis of variance Multilevel model Random effects model Repeated measures design == References == == Further reading == Milliken, G. A., & Johnson, D. E. (1992). Analysis of messy data: Vol. I. Designed experiments. New York: Chapman & Hall. West, B. T., Welch, K. B., & Galecki, A. T. (2007). Linear mixed models: A practical guide using statistical software. New York: Chapman & Hall/CRC. == Commercial == NCSS (statistical software) includes longitudinal mixed models analysis. Stata statistical software includes multilevel mixed-effects models analysis. 
National accounts or national account systems (NAS) are the implementation of complete and consistent accounting techniques for measuring the economic activity of a nation. These include detailed underlying measures that rely on double-entry accounting. By design, such accounting makes the totals on both sides of an account equal even though they each measure different characteristics, for example production and the income from it. As a method, the subject is termed national accounting or, more generally, social accounting. Stated otherwise, national accounts as systems may be distinguished from the economic data associated with those systems. While sharing many common principles with business accounting, national accounts are based on economic concepts. One conceptual construct for representing flows of all economic transactions that take place in an economy is a social accounting matrix with accounts in each respective row-column entry. National accounting has developed in tandem with macroeconomics from the 1930s with its relation of aggregate demand to total output through interaction of such broad expenditure categories as consumption and investment. Economic data from national accounts are also used for empirical analysis of economic growth and development. == ScopeEdit == National accounts broadly present output, expenditure, and income activities of the economic actors (households, corporations, government) in an economy, including their relations with other countries' economies, and their wealth (net worth). They present both flows (measured over a period) and stocks (measured at the end of a period), ensuring that the flows are reconciled with the stocks. As to flows, the national income and product accounts (in U.S. terminology) provide estimates for the money value of income and output per year or quarter, including GDP. As to stocks, the 'capital accounts' are a balance-sheet approach that has assets on one side (including values of land, the capital stock, and financial assets) and liabilities and net worth on the other, measured as of the end of the accounting period. National accounts also include measures of the changes in assets, liabilities, and net worth per accounting period. These may refer to flow of funds accounts or, again, capital accounts. There are a number of aggregate measures in the national accounts, notably including gross domestic product or GDP, perhaps the most widely cited measure of aggregate economic activity. Ways of breaking down GDP include as types of income (wages, profits, etc.) or expenditure (consumption, investment/saving, etc.). Measures of these are examples of macro-economic data. Such aggregate measures and their change over time are generally of strongest interest to economic policymakers, although the detailed national accounts contain a source of information for economic analysis, for example in the input-output tables which show how industries interact with each other in the production process. National accounts can be presented in nominal or real amounts, with real amounts adjusted to remove the effects of price changes over time. A corresponding price index can also be derived from national output. Rates of change of the price level and output may also be of interest. An inflation rate (growth rate of the price level) may be calculated for national output or its expenditure components. Economic growth rates (most commonly the growth rate of GDP) are generally measured in real (constant-price) terms. One use of economic-growth data from the national accounts is in growth accounting across longer periods of time for a country or across to estimate different sources of growth, whether from growth of factor inputs or technological change. The accounts are derived from a wide variety of statistical source data including surveys, administrative and census data, and regulatory data, which are integrated and harmonized in the conceptual framework. They are usually compiled by national statistical offices and/or central banks in each country, though this is not always the case, and may be released on both an annual and (less detailed) quarterly frequency. Practical issues include inaccuracies from differences between economic and accounting methodologies, lack of controlled experiments on quality of data from diverse sources, and measurement of intangibles and services of the banking and financial sectors. Two developments relevant to the national accounts since the 1980s include the following. Generational accounting is a method for measuring redistribution of lifetime tax burdens across generations from social insurance, including social security and social health insurance. It has been proposed as a better guide to the sustainability of a fiscal policy than budget deficits, which reflect only taxes minus spending in the current year. Environmental or green national accounting is the method of valuing environmental assets, which are usually not counted in measuring national wealth, in part due to the difficulty of valuing them. The method has been proposed as an alternative to an implied zero valuation of environmental assets and as a way of measuring the sustainability of welfare levels in the presence of environmental degradation. Macroeconomic data not derived from the national accounts are also of wide interest, for example some cost-of-living indexes, the unemployment rate, and the labor force participation rate. In some cases, a national-accounts counterpart of these may be estimated, such as a price index computed from the personal consumption expenditures and the GDP gap (the difference between observed GDP and potential GDP). == Main componentsEdit == The presentation of national accounts data may vary by country (commonly, aggregate measures are given greatest prominence), however the main national accounts include the following accounts for the economy as a whole and its main economic actors. Current accounts: production accounts which record the value of domestic output and the goods and services used up in producing that output. The balancing item of the accounts is value added, which is equal to GDP when expressed for the whole economy at market prices and in gross terms; income accounts, which show primary and secondary income flows - both the income generated in production (e.g. wages and salaries) and distributive income flows (predominantly the redistributive effects of government taxes and social benefit payments). The balancing item of the accounts is disposable income ("National Income" when measured for the whole economy); expenditure accounts, which show how disposable income is either consumed or saved. The balancing item of these accounts is saving. Capital accounts, which record the net accumulation, as the result of transactions, of non-financial assets; and the financing, by way of saving and capital transfers, of the accumulation. Net lending/borrowing is the balancing item for these accounts Financial accounts, which show the net acquisition of financial assets and the net incurrence of liabilities. The balance on these accounts is the net change in financial position. Balance sheets, which record the stock of assets, both financial and non-financial, and liabilities at a particular point in time. Net worth is the balance from the balance sheets (United Nations, 1993). The accounts may be measured as gross or net of consumption of fixed capital (a concept in national accounts similar to depreciation in business accounts). == HistoryEdit == The original motivation for the development of national accounts and the systematic measurement of employment was the need for accurate measures of aggregate economic activity. This was made more pressing by the Great Depression and as a basis for Keynesian macroeconomic stabilisation policy and wartime economic planning. The first efforts to develop such measures were undertaken in the late 1920s and 1930s, notably by Colin Clark and Simon Kuznets. Richard Stone of the U.K. led later contributions during World War II and thereafter. The first formal national accounts were published by the United States in 1947. Many European countries followed shortly thereafter, and the United Nations published A System of National Accounts and Supporting Tables in 1952. International standards for national accounting are defined by the United Nations System of National Accounts, with the most recent version released for 2008. Even before that in early 1920s there were national economic accounts tables. One of such systems was called Balance of national economy and was used in USSR and other socialistic countries to measure the efficiency of socialistic production.Economic theory. In Europe, the worldwide System of National Accounts has been adapted in the European System of Accounts (ESA), which is applied by members of the European Union and many other European countries. Research on the subject continues from its beginnings through today. == See alsoEdit == == ReferencesEdit == 
A natural experiment is an empirical study in which individuals (or clusters of individuals) exposed to the experimental and control conditions are determined by nature or by other factors outside the control of the investigators, yet the process governing the exposures arguably resembles random assignment. Thus, natural experiments are observational studies and are not controlled in the traditional sense of a randomized experiment. Natural experiments are most useful when there has been a clearly defined exposure involving a well defined subpopulation (and the absence of exposure in a similar subpopulation) such that changes in outcomes may be plausibly attributed to the exposure. In this sense the difference between a natural experiment and a non-experimental observational study is that the former includes a comparison of conditions that pave the way for causal inference, while the latter does not. Natural experiments are employed as study designs when controlled experimentation is extremely difficult to implement or unethical, such as in several research areas addressed by epidemiology (e.g., evaluating the health impact of varying degrees of exposure to ionizing radiation in people living near Hiroshima at the time of the atomic blast) and economics (e.g., estimating the economic return on amount of schooling in US adults). == History == One of the most famous early natural experiments was the 1854 Broad Street cholera outbreak in London, England. On 31 August 1854, a major outbreak of cholera struck Soho. Over the next three days 127 people near Broad Street died. By the end of the outbreak 616 people died. The physician John Snow identified the source of the outbreak as the nearest public water pump, using a map of deaths and illness that revealed a cluster of cases around the pump. In this example, Snow discovered a strong association between the use of the water and deaths and illnesses due to cholera. Snow found that the Southwark and Vauxhall Waterworks Company, which supplied water to districts with high attack rates, obtained the water from the Thames downstream from where raw sewage was discharged into the river. By contrast, districts that were supplied water by the Lambeth Waterworks Company, which obtained water upstream from the points of sewage discharge, had low attack rates. Given the near-haphazard patchwork development of the water supply in mid-Nineteenth Century London, Snow viewed the developments as "an experiment...on the grandest scale." Of course, the exposure to the polluted water was not under the control of any scientist. Therefore, this exposure has been recognized as being a natural experiment. == Recent examples == === Family size === In Angrist and Evans (1998), the authors wish to estimate the effect of family size on the labor market outcomes of the mother. The correlations between family size and various outcomes do not tell us how family size causally affects labor market outcomes because both labor market outcomes and family size may be affected by unobserved variables such as preferences and because labor market outcomes may itself affect family size (called "reverse causality," for example, a woman may defer having a child if she gets a raise at work). The study notes that two-child families with either two boys or two girls are substantially more likely to have a third child than two-child families with one boy and one girl. The sex of the first two children, then, forms a natural experiment: it is as if an experimenter has randomly assigned some families to have two children and others to have three or more. The authors are then able to credibly estimate the causal effect of having a third child on labor market outcomes. === Game shows === Within economics, game shows are a frequently studied form of natural experiment. While game shows might seem as artificial contexts, they can be considered as natural experiment due to the fact that the context arises without interference of the scientist. Game shows have been used to study a wide range of different types of economic behavior, such as decision making under risk and cooperative behavior. === Smoking ban === An example of a natural experiment occurred in Helena, Montana during the six-month period from June 2002 to December 2002 when a smoking ban was in effect in all public spaces in Helena including bars and restaurants. Helena is geographically isolated and served by only one hospital. It was observed that the rate of heart attacks dropped by 60% while the smoking ban was in effect. Opponents of the law prevailed in getting the enforcement of the law suspended after six months, after which the rate of heart attacks went back up. Note, however, that while this may have been a good example of a natural experiment (called a case-crossover experiment, where the exposure is removed for a time period and then returned), it is also a good example of how confounding variables can result in faulty conclusions being made. For instance, many smoking ban-heart attack studies fail to indicate that heart attack rates were already on the decline before the smoking ban was in place, or fail to take into account seasonal fluxes in heart attacks (highest in the winter months and lowest in the summer). For the Helena study in particular, the claim that 40% of pre-ban heart attacks were caused by passive smoking is not believable, considering that only 10-15% of coronary heart disease cases are thought to be caused by active smoking. === Nuclear weapons testing === Nuclear weapons testing released large quantities of radioactive isotopes into the atmosphere, some of which could be incorporated into biological tissues. The release stopped after the Partial Nuclear Test Ban Treaty in 1963, which prohibited atmospheric nuclear tests. This resembled a large-scale pulse-chase experiment, but could not have been performed as a regular experiment in humans due to scientific ethics. Several types of observations were made possible (in people born before 1963), such as determination of the rate of replacement for cells in different human tissues. === Vietnam War draft === An important question in economics is what determines earnings. Angrist (1990) was interested to know the effects of military service on lifetime earnings. The study leveraged the approximate random assignment of the Vietnam War draft lottery as an instrumental variable for whether a given individual served in the military. Because many factors might predict whether someone serves in the military, the draft lottery provides a natural experiment whereby those drafted into the military can be compared against those not drafted because the two groups should not differ substantially prior to military service. Angrist finds that the earnings of veterans are significantly lower (approximately 15 percent less) than those of non-veterans. == See also == Experiment Instrumental variable, statistical technique for analysing natural experiments == References == 
Observation is the active acquisition of information from a primary source. In living beings, observation employs the senses. In science, observation can also involve the recording of data via the use of instruments. The term may also refer to any data collected during the scientific activity. Observations can be qualitative, that is, only the absence or presence of a property is noted, or quantitative if a numerical value is attached to the observed phenomenon by counting or measuring. == Observation in science == The scientific method requires observations of nature to formulate and test hypotheses. It consists of these steps: Asking a question about a natural phenomenon Making observations of the phenomenon Hypothesizing an explanation for the phenomenon Predicting a logical consequence of the hypothesis Testing the hypothesis by an experiment, observational study, field study or simulation Creating a conclusion with data gathered in the experiment, or forming a revised/new hypothesis and repeating the process Publishing your results. Peer review of the results of others. Observations play a role in the second and fifth steps of the scientific method. However the need for reproducibility requires that observations by different observers can be comparable. Human sense impressions are subjective and qualitative making them difficult to record or compare. Theed or shared by all observers, and counting how many of the standard units are comparable to the object. Measurement reduces an observation to a number which can be recorded, and two observations which result in the same number are equal within the resolution of the process. Senses are limited, and are subject to errors in perception such as optical illusions. Scientific instruments were developed to magnify human powers of observation, such as weighing scales, clocks, telescopes, microscopes, thermometers, cameras, and tape recorders, and also translate into perceptible form events that are unobservable by human senses, such as indicator dyes, voltmeters, spectrometers, infrared cameras, oscilloscopes, interferometers, geiger counters, x-ray machines, and radio receivers. One problem encountered throughout scientific fields is that the observation may affect the process being observed, resulting in a different outcome than if the process was unobserved. This is called the observer effect. For example, it is not normally possible to check the air pressure in an automobile tire without letting out some of the air, thereby changing the pressure. However, in most fields of science it is possible to reduce the effects of observation to insignificance by using better instruments. Considered as a physical process itself, all forms of observation (human or instrumental) involve amplification and are thus thermodynamically irreversible processes, increasing entropy. == Observational paradoxes == In some specific fields of science the results of observation differ depending on factors which are not important in everyday observation. These are usually illustrated with "paradoxes" in which an event appears different when observed from two different points of view, seeming to violate "common sense". Relativity: In relativistic physics which deals with velocities close to the speed of light, it is found that different observers may observe different values for the length, time rates, mass, and many other properties of an object, depending on the observer's velocity relative to the object. For example, in the twin paradox one twin goes on a trip near the speed of light and comes home younger than the twin who stayed at home. This is not a paradox: time passes at a slower rate when measured from a frame moving with respect to the object. In relativistic physics, an observation must always be qualified by specifying the state of motion of the observer, its reference frame. Quantum mechanics: In quantum mechanics, which deals with the behavior of very small objects, it is not possible to observe a system without changing the system, and the "observer" must be considered part of the system being observed. In isolation, quantum objects are represented by a wave function which often exists in a superposition or mixture of different states. However, when an observation is made to determine the actual location or state of the object, it always finds the object in a single state, not a "mixture". The interaction of the observation process appears to "collapse" the wave function into a single state. So any interaction between an isolated wave function and the external world that results in this wave function collapse is called an observation or measurement, whether or not it is part of a deliberate observation process. == Biases == The human senses do not function like a video camcorder, impartially recording all observations. Human perception occurs by a complex, unconscious process of abstraction, in which certain details of the incoming sense data are noticed and remembered, and the rest forgotten. What is kept and what is thrown away depends on an internal model or representation of the world, called by psychologists a schema, that is built up over our entire lives. The data is fitted into this schema. Later when events are remembered, memory gaps may even be filled by "plausible" data the mind makes up to fit the model; this is called reconstructive memory. How much attention the various perceived data are given depends on an internal value system, which judges how important it is to the individual. Thus two people can view the same event and come away with entirely different perceptions of it, even disagreeing about simple facts. This is why eyewitness testimony is notoriously unreliable. Several of the more important ways observations can be affected by human psychology are given below. === Confirmation bias === Human observations are biased toward confirming the observer's conscious and unconscious expectations and view of the world; we "see what we expect to see". In psychology, this is called confirmation bias. Since the object of scientific research is the discovery of new phenomena, this bias can and has caused new discoveries to be overlooked. One example is the discovery of x-rays. It can also result in erroneous scientific support for widely held cultural myths, for example the scientific racism that supported ideas of racial superiority in the early 20th century. Correct scientific technique emphasizes careful recording of observations, separating experimental observations from the conclusions drawn from them, and techniques such as blind or double blind experiments, to minimize observational bias. === "Cargo cult" science === Another bias, which has become more prevalent with the advent of "big science" and the large rewards of new discoveries, is bias in favor of the researcher's desired hypothesis or outcome; we "see what we want to see". Called pathological science and cargo cult science, this is different from deliberate falsification of results, and can happen to good-faith researchers. Researchers with a great incentive or desire for a given outcome can misinterpret or misjudge results, or even persuade themselves they have seen something they haven't. Possible examples of mistaken discoveries caused by this bias are Martian "canals", N rays, polywater, cold fusion, and perpetual motion machines. Recent decades have seen scientific scandals caused by researchers playing "fast and loose" with observational methods in order to get their pet theories published. This type of bias is rampant in pseudoscience, where correct scientific techniques are not followed. The main defense against this bias, besides correct research techniques, is peer review and repetition of the experiment, or the observation, by other researchers with no incentive to bias. For example, an emerging practice in the competitive field of biotechnology is to require the physical results of experiments, such as serums and tissue cultures, be made available to competing laboratories for independent testing. === Processing bias === Modern scientific instruments can extensively process "observations" before they are presented to the human senses, and particularly with computerized instruments, there is sometimes a question as to where in the data processing chain "observing" ends and "drawing conclusions" begins. This has recently become an issue with digitally enhanced images published as experimental data in papers in scientific journals. The images are enhanced to bring out features that the researcher wants to emphasize, but this also has the effect of supporting the researcher's conclusions. This is a form of bias that is difficult to quantify. Some scientific journals have begun to set detailed standards for what types of image processing are allowed in research results. Computerized instruments often keep a copy of the "raw data" from sensors before processing, which is the ultimate defense against processing bias, and similarly scientific standards require preservation of the original unenhanced "raw" versions of images used as research data. === Observational bias === An observational bias occurs when researchers only look where they think they will find positive results, or where it is easy to record observations. This is called the "streetlight effect". == Observations in philosophy == "Observe always that everything is the result of a change, and get used to thinking that there is nothing Nature loves so well as to change existing forms and to make new ones like them." Observation in philosophical terms is the process of filtering sensory information through the thought process. Input is received via hearing, sight, smell, taste, or touch and then analyzed through either rational or irrational thought. You see a parent beat their child; you observe that such an action is either good or bad. Deductions about what behaviors are good or bad may be based in no way on preferences about building relationships, or study of the consequences resulting from the observed behavior. With the passage of time, impressions stored in the consciousness about many related observations, together with the resulting relationships and consequences, permit the individual to build a construct about the moral implications of behavior. == See also == Introspection List of cognitive biases Naturalistic observation Observational astronomy Observational learning Observational study Observations and Measurements Observatory Observer effect Uncertainty principle == References == 
In statistics, ordinal data is a statistical data type consisting of numerical scores that exist on an ordinal scale, i.e. an arbitrary numerical scale where the exact numerical quantity of a particular value has no significance beyond its ability to establish a ranking over a set of data points. A variable, the data on which are ordinal, is known as an ordinal variable. In regression analysis, outcomes (dependent variables) that are ordinal variables can be predicted using a variant of ordinal regression, such as ordered logit or ordered probit. Examples of ordinal data are often found in questionnaires: for example, the survey question "Is your general health poor, reasonable, good, or excellent?" may have those answers coded respectively as 1, 2, 3, and 4. Sometimes data on an interval scale or ratio scale are grouped onto an ordinal scale: for example, individuals whose income is known might be grouped into the income categories $0-$19,999, $20,000-$39,999, $40,000-$59,999, ..., which then might be coded as 1, 2, 3, 4, .... 
Parametric statistics is a branch of statistics which assumes that sample data comes from a population that follows a probability distribution based on a fixed set of parameters. Most well-known elementary statistical methods are parametric. Conversely a non-parametric model differs precisely in that the parameter set (or feature set in machine learning) is not fixed and can increase, or even decrease if new relevant information is collected. A parametric model as it relies on a fixed parameter set assumes more about a given population than non-parametric methods. When the assumptions are correct, parametric methods will produce more accurate and precise estimates than non-parametric methods, i.e. have more statistical power. As more is assumed when the assumptions are not correct they have a greater chance of failing, and for this reason are not a robust statistical method. On the other hand, parametric formulae are often simpler to write down and faster to compute. For this reason their simplicity can make up for their lack of robustness, especially if care is taken to examine diagnostic statistics. == ExampleEdit == The normal family of distributions all have the same shape and are parameterized by mean and standard deviation. That means if you know the mean and standard deviation, and that the distribution is normal, you know the probability of any future observation. Suppose we have a sample of 99 test scores with a mean of 100 and a standard deviation of 1. If we assume all 99 test scores are random samples from a normal distribution we predict there is a 1% chance that the 100th test score will be higher than 102.365 (that is the mean plus 2.365 standard deviations) assuming that the 100th test score comes from the same distribution as the others. Parametric statistical methods are used to compute the 2.365 value above, given 99 independent observations from the same normal distribution. A non-parametric estimate of the same thing is the maximum of the first 99 scores. We don't need to assume anything about the distribution of test scores to reason that before we gave the test it was equally likely that the highest score would be any of the first 100. Thus there is a 1% chance that the 100th is higher than any of the 99 that preceded it. == HistoryEdit == Parametric functions were mentioned by R. Fisher in his work Statistical Methods for Research Workers in 1925 which created the foundation for modern statistics. == See alsoEdit == Parametric equation Parametric model == ReferencesEdit == 
Probabilistic design is a discipline within engineering design. It deals primarily with the consideration of the effects of random variability upon the performance of an engineering system during the design phase. Typically, these effects are related to quality and reliability. Thus, probabilistic design is a tool that is mostly used in areas that are concerned with quality and reliability. For example, product design, quality control, systems engineering, machine design, civil engineering (particularly useful in limit state design) and manufacturing. It differs from the classical approach to design by assuming a small probability of failure instead of using the safety factor. == Designer's perspective == When using a probabilistic approach to design, the designer no longer thinks of each variable as a single value or number. Instead, each variable is viewed as a probability distribution. From this perspective, probabilistic design predicts the flow of variability (or distributions) through a system. By considering this flow, a designer can make adjustments to reduce the flow of random variability, and improve quality. Proponents of the approach contend that many quality problems can be predicted and rectified during the early design stages and at a much reduced cost. == The objective of probabilistic design == Typically, the goal of probabilistic design is to identify the design that will exhibit the smallest effects of random variability. This could be the one design option out of several that is found to be most robust. Alternatively, it could be the only design option available, but with the optimum combination of input variables and parameters. This second approach is sometimes referred to as robustification, parameter design or design for six sigma == Methods used == Essentially, probabilistic design focuses upon the prediction of the effects of random variability. Some methods that are used to predict the random variability of an output include: the Monte Carlo method (including Latin hypercubes); propagation of error; design of experiments (DOE) the method of moments Statistical interference == See also == Interval finite element == Footnotes == ^ Sundararajan, S (1995). Probabilistic Structural Mechanics Handbook. Springer. ISBN 978-0412054815. ^ Long, M W; Narcico, J D (June 1999), Design Mepthodology for Composite Aircraft Structures, DOT/FAA/AR-99/2, FAA, retrieved 24 January 2015 ^ Sundarth, S; Woeste, Frank E.; Galligan, William (1978), Differential reliability : probabilistic engineering applied to wood members in bending-tension (PDF), Res. Pap. FPL-RP-302., US Forest Products Laboratory, retrieved 21 January 2015 == References == Ang and Tang (2006) Probability Concepts in Engineering: Emphasis on Applications to Civil and Environmental Engineering. John Wiley & Sons. ISBN 0-471-72064-X Ash (1993) The Probability Tutoring Book: An Intuitive Course for Engineers and Scientists (and Everyone Else). Wiley-IEEE Press. ISBN 0-7803-1051-9 Clausing (1994) Total Quality Development: A Step-By-Step Guide to World-Class Concurrent Engineering. American Society of Mechanical Engineers. ISBN 0-7918-0035-0 Haugen (1980) Probabilistic mechanical design. Wiley. ISBN 0-471-05847-5 Papoulis (2002) Probability, Random Variables and Stochastic Process. McGraw-Hill Publishing Co. ISBN 0-07-119981-0 Siddall (1982) Optimal Engineering Design. CRC. ISBN 0-8247-1633-7 == External links == Probabilistic design Non Deterministic Approaches in Engineering 
In arithmetic, the range of a set of data is the difference between the largest and smallest values. However, in descriptive statistics, this concept of range has a more complex meaning. The range is the size of the smallest interval which contains all the data and provides an indication of statistical dispersion. It is measured in the same units as the data. Since it only depends on two of the observations, it is most useful in representing the dispersion of small data sets. == Independent identically distributed continuous random variables == For n independent and identically distributed continuous random variables X1, X2, ..., Xn with cumulative distribution function G(x) and probability density function g(x) the range of the Xi is the range of a sample of size n from a population with distribution function G(x). === Distribution === The range has cumulative distribution function Gumbel notes that the "beauty of this formula is completely marred by the facts that, in general, we cannot express G(x + t) by G(x), and that the numerical integration is lengthy and tiresome." If the distribution of each Xi is limited to the right (or left) then the asymptotic distribution of the range is equal to the asymptotic distribution of the largest (smallest) value. For more general distributions the asymptotic distribution can be expressed as a Bessel function. === Moments === The mean range is given by where x(G) is the inverse function. In the case where each of the Xi has a standard normal distribution, the mean range is given by == Independent nonidentically distributed continuous random variables == For n nonidentically distributed independent continuous random variables X1, X2, ..., Xn with cumulative distribution functions G1(x), G2(x), ..., Gn(x) and probability density functions g1(x), g2(x), ..., gn(x), the range has cumulative distribution function == Independent identically distributed discrete random variables == For n independent and identically distributed discrete random variables X1, X2, ..., Xn with cumulative distribution function G(x) and probability mass function g(x) the range of the Xi is the range of a sample of size n from a population with distribution function G(x). We can assume without loss of generality that the support of each Xi is {1,2,3,...,N} where N is a positive integer or infinity. === Distribution === The range has probability mass function ==== Example ==== If we suppose that g(x)=1/N, the discrete uniform distribution for all x, then we find == Related quantities == The range is a simple function of the sample maximum and minimum and these are specific examples of order statistics. In particular, the range is a linear function of order statistics, which brings it into the scope of L-estimation. == See also == Interquartile range Studentized range == References == 
In engineering, science, and statistics, replication is the repetition of an experimental condition so that the variability associated with the phenomenon can be estimated. ASTM, in standard E1847, defines replication as "the repetition of the set of all the treatment combinations to be compared in an experiment. Each of the repetitions is called a replicate." Replication is not the same as repeated measurements of the same item: they are dealt with differently in statistical experimental design and data analysis. For proper sampling, a process or batch of products should be in reasonable statistical control; inherent random variation is present but variation due to assignable (special) causes is not. Evaluation or testing of a single item does not allow for item-to-item variation and may not represent the batch or process. Replication is needed to account for this variation among items and treatments. == ExampleEdit == As an example, consider a continuous process which produces items. Batches of items are then processed or treated. Finally, tests or measurements are conducted. Several options might be available to obtain ten test values. Some possibilities are: One finished and treated item might be measured repeatedly to obtain ten test results. Only one item was measured so there is no replication. The repeated measurements help identify observational error. Ten finished and treated items might be taken from a batch and each measured once. This is not full replication because the ten samples are not random and not representative of the continuous nor batch processing. Five items are taken from the continuous process based on sound statistical sampling. These are processed in a batch and tested twice each. This includes replication of initial samples but does not allow for batch-to-batch variation in processing. The repeated tests on each provide some measure and control of testing error. Five items are taken from the continuous process based on sound statistical sampling. These are processed in five different batches and tested twice each. This plan includes proper replication of initial samples and also includes batch-to-batch variation. The repeated tests on each provide some measure and control of testing error. Each option would call for different data analysis methods and yield different conclusions. == See alsoEdit == Sample size Test method Design of experiments Degrees of freedom (statistics) Statistical process control Statistical ensemble == BibliographyEdit == ASTM E122-07 Standard Practice for Calculating Sample Size to Estimate, With Specified Precision, the Average for a Characteristic of a Lot or Process "Engineering Statistics Handbook", NIST/SEMATEK Pyzdek, T, "Quality Engineering Handbook", 2003, ISBN 0-8247-4614-7. Godfrey, A. B., "Juran's Quality Handbook", 1999, ISBN 9780070340039. 
A run chart, also known as a run-sequence plot is a graph that displays observed data in a time sequence. Often, the data displayed represent some aspect of the output or performance of a manufacturing or other business process. It is therefore a form of line chart. == OverviewEdit == Run sequence plots are an easy way to graphically summarize a univariate data set. A common assumption of univariate data sets is that they behave like: random drawings; from a fixed distribution; with a common location; and with a common scale. With run sequence plots, shifts in location and scale are typically quite evident. Also, outliers can easily be detected. Examples could include measurements of the fill level of bottles filled at a bottling plant or the water temperature of a dish-washing machine each time it is run. Time is generally represented on the horizontal (x) axis and the property under observation on the vertical (y) axis. Often, some measure of central tendency (mean or median) of the data is indicated by a horizontal reference line. Run charts are analyzed to find anomalies in data that suggest shifts in a process over time or special factors that may be influencing the variability of a process. Typical factors considered include unusually long "runs" of data points above or below the average line, the total number of such runs in the data set, and unusually long series of consecutive increases or decreases. Run charts are similar in some regards to the control charts used in statistical process control, but do not show the control limits of the process. They are therefore simpler to produce, but do not allow for the full range of analytic techniques supported by control charts. == ReferencesEdit == This article incorporates public domain material from websites or documents of the National Institute of Standards and Technology. == Further readingEdit == Pyzdek, Thomas (2003). Quality Engineering Handbook (Second ed.). New York: CRC. ISBN 0-8247-4614-7. == External linksEdit == Run-Sequence Plot 
A scatter plot, scatterplot, or scattergraph is a type of mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are color-coded you can increase the number of displayed variables to three. The data is displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. This kind of plot is also called a scatter chart, scattergram, scatter diagram, or scatter graph. == OverviewEdit == A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a correct solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree with each other. In this case, an identity line, i.e., a y=x line, or an 1:1 line, is often drawn as a reference. The more the two data sets agree, the more the scatters tend to concentrate in the vicinity of the identity line; if the two data sets are numerically identical, the scatters fall on the identity line exactly. One of the most powerful aspects of a scatter plot, however, is its ability to show nonlinear relationships between variables. The ability to do this can be enhanced by adding a smooth line such as loess. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. The scatter diagram is one of the seven basic tools of quality control. Scatter charts can be built in the form of bubble, marker, or/and line charts. == ExampleEdit == For example, to display a link between a person's lung capacity, and how long that person could hold his/her breath, a researcher would choose a group of people to study, then measure each one's lung capacity (first variable) and how long that person could hold his/her breath (second variable). The researcher would then plot the data in a scatter plot, assigning "lung capacity" to the horizontal axis, and "time holding breath" to the vertical axis. A person with a lung capacity of 400 cl who held his/her breath for 21.7 seconds would be represented by a single dot on the scatter plot at the point (400, 21.7) in the Cartesian coordinates. The scatter plot of all the people in the study would enable the researcher to obtain a visual comparison of the two variables in the data set, and will help to determine what kind of relationship there might be between the two variables. == Scatterplot matricesEdit == For a set of data variables (dimensions) X1, X2, ... , Xk, the scatter plot matrix shows all the pairwise scatter plots of the variables on a single view with multiple scatterplots in a matrix format. For k variables, the scatterplot matrix will contain k rows and k columns. A plot located on the intersection of i-th row and j-th column is a plot of variables Xi versus Xj. This means that each row and column is one dimension, and each cell plots a scatterplot of two dimensions. == ReferencesEdit == == External linksEdit == What is a scatterplot? Correlation scatter-plot matrix - for ordered-categorical data - Explanation and R code Tool for visualizing scatter plots Density scatterplot for large datasets (hundreds of millions of points) 
In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the globally best set of labels for the entire sequence at once. As an example of why finding the globally best label sequence might produce better results than labeling one item at a time, consider the part-of-speech tagging task just described. Frequently, many words are members of multiple parts of speech, and the correct label of such a word can often be deduced from the correct label of the word to the immediate left or right. For example, the word "sets" can be either a noun or verb. In a phrase like "he sets the books down", the word "he" is unambiguously a pronoun, and "the" unambiguously a determiner, and using either of these labels, "sets" can be deduced to be a verb, since nouns very rarely follow pronouns and are less likely to precede determiners than verbs are. But in other cases, only one of the adjacent words is similarly helpful. In "he sets and then knocks over the table", only the word "he" to the left is helpful (cf. "...picks up the sets and then knocks over..."). Conversely, in "... and also sets the table" only the word "the" to the right is helpful (cf. "... and also sets of books were ..."). An algorithm that proceeds from left to right, labeling one word at a time, can only use the tags of left-adjacent words and might fail in the second example above; vice versa for an algorithm that proceeds from right to left. Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. The most common statistical models in use for sequence labeling make a Markov assumption, i.e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. This leads naturally to the hidden Markov model (HMM), one of the most common statistical models used for sequence labeling. Other common models in use are the maximum entropy Markov model and conditional random field. == Evaluation == == Application domains == == See also == Artificial intelligence Bayesian networks (of which HMMs are an example) Classification (machine learning) Linear dynamical system, which applies to tasks where the "label" is actually a real number Machine learning Pattern recognition Sequence mining == References == {{Reflist}} == Further reading == Erdogan H., [1]. "Sequence labeling: generative and discriminative approaches, hidden Markov models, conditional random fields and structured SVMs," ICMLA 2010 tutorial, Bethesda, MD (2010) ^ http://mallet.cs.umass.edu/sequences.php 
In statistics, the concept of the shape of the distribution refers to the shape of a probability distribution and it most often arises in questions of finding an appropriate distribution to use to model the statistical properties of a population, given a sample from that population. The shape of a distribution may be considered either descriptively, using terms such as "J-shaped", or numerically, using quantitative measures such as skewness and kurtosis. Considerations of the shape of a distribution arise in statistical data analysis, where simple quantitative descriptive statistics and plotting techniques such as histograms can lead on to the selection of a particular family of distributions for modelling purposes. == Descriptions of shape == The shape of a distribution will fall somewhere in a continuum where a flat distribution might be considered central and where types of departure from this include: mounded (or unimodal), U-shaped, J-shaped, reverse-J shaped and multi-modal. A bimodal distribution would have two high points rather than one. The shape of a distribution is sometimes characterised by the behaviours of the tails (as in a long or short tail). For example, a flat distribution can be said either to have no tails, or to have short tails. A normal distribution is usually regarded as having short tails, while an exponential distribution has exponential tails and a Pareto distribution has long tails. == See also == Shape parameter List of probability distributions == Notes == == References == Yule, G.U., Kendall, M.G. (1950) An Introduction to the Theory of Statistics, 14th Edition (5th Impression, 1968), Griffin, London. den Dekker A. J., Sijbers J., (2014) "Data distributions in magnetic resonance images: a review", Physica Medica 
Social statistics is the use of statistical measurement systems to study human behavior in a social environment. This can be accomplished through polling a group of people, evaluating a subset of data obtained about a group of people, or by observation and statistical analysis of a set of data that relates to people and their behaviors. Social scientists use social statistics for many purposes, including: the evaluation of the quality of services available to a group or organization, analyzing behaviors of groups of people in their environment and special situations, determining the wants of people through statistical sampling. == Statistics in the social sciences == Statistics and statistical analyses have become a key feature of social science. Statistics is employed in economics, psychology, political science, sociology and anthropology. There is a debate regarding the uses and value of statistical methods in social science, especially in political science, with some statisticians questioning the policy conclusions of political partisans who overestimate the interpretive power that non-robust statistical methods such as simple and multiple linear regression allow. Indeed, an important axiom that social scientists cite, but often forget, is that "correlation does not imply causation." The use of statistics has become so widespread in the social sciences that many universities such as Harvard, have developed institutes focusing on "quantitative social science." Harvard's Institute for Quantitative Social Science focuses mainly on fields like political science that incorporate the advanced causal statistical models that Bayesian methods provide. However, some experts in causality feel that these claims of causal statistics are overstated, === Statistical methods in social sciences === Methods, techniques and concepts used in quantitative social sciences include: Structural Equation Modeling and factor analysis Multilevel models Cluster analysis Latent class model Item response theory Survey methodology and survey sampling == See also == List of statistical packages == Further reading == Blalock, H.M., Jr, ed. (1974), Measurement in the Social Sciences, Chicago, Illinois: Aldine Publishing, ISBN 0-202-30272-5, retrieved 10 July 2010 Blalock, Hubert M (1979), Social Statistics, New York: McGraw-Hill, ISBN 0-07-005752-4 Irvine, John, Miles, Ian, Evans, Jeff, (editors), "Demystifying Social Statistics ", London : Pluto Press, 1979. ISBN 0-86104-069-4 Miller, Delbert C., & Salkind, Neil J (2002), Handbook of Research Design and Social Measurement, California: Sage, ISBN 0-7619-2046-3, retrieved 10 July 2010 == References == == External links == Statistics at DMOZ Social science statistics centers Center for Statistics and Social Sciences, University of Washington Center for the Promotion of Research Involving Innovative Statistical Methodology, New York University, NY Centre for Research Methods, Faculty of Social Sciences, University of Helsinki, Finland Cornell Institute for Social and Economic Research Harvard Institute for Quantitative Social Science Inter-University Consortium for Political and Social Research National Centre for Research Methods, UK Odum Institute for Research in Social Sciences, University of North Carolina, Chapel Hill Social Science Statistics Center, University of Missouri, Columbia (see also their Social Statistics Division, School of Social Sciences, University of Southampton, UK links section) Social Statistics Research Group, University of Auckland, New Zealand Statistical databases for social science Inter-University Consortium for Political and Social Research UN Statistics Division- Demographic and Social Statistics Organisation for Economic Co-Operation and Development (OECD) US Bureau of Labor Statistics International Labour Organisation- LABORSTA Labor Research Association- Statistics for Labor Economics Labor and Worklife Program- Labor Stats at Harvard Law School Unionstats.com Social Statistics 2.0 
Statistical graphics, also known as graphical techniques, are graphics in the field of statistics used to visualize quantitative data. == Overview == Whereas statistics and data analysis procedures generally yield their output in numeric or tabular form, graphical techniques allow such results to be displayed in some sort of pictorial form. They include plots such as scatter plots, histograms, probability plots, spaghetti plots, residual plots, box plots, block plots and biplots. Exploratory data analysis (EDA) relies heavily on such techniques. They can also provide insight into a data set to help with testing assumptions, model selection and regression model validation, estimator selection, relationship identification, factor effect determination, and outlier detection. In addition, the choice of appropriate statistical graphics can provide a convincing means of communicating the underlying message that is present in the data to others. Graphical statistical methods have four objectives: The exploration of the content of a data set The use to find structure in data Checking assumptions in statistical models Communicate the results of an analysis. If one is not using statistical graphics, then one is forfeiting insight into one or more aspects of the underlying structure of the data. == History == Statistical graphics have been central to the development of science and date to the earliest attempts to analyse data. Many familiar forms, including bivariate plots, statistical maps, bar charts, and coordinate paper were used in the 18th century. Statistical graphics developed through attention to four problems: Spatial organization in the 17th and 18th century Discrete comparison in the 18th and early 19th century Continuous distribution in the 19th century and Multivariate distribution and correlation in the late 19th and 20th century. Since the 1970s statistical graphics have been re-emerging as an important analytic tool with the revitalisation of computer graphics and related technologies. == Examples == Famous graphics were designed by: William Playfair who produced what could be called the first line, bar, pie, and area charts. For example, in 1786 he published the well known diagram that depicts the evolution of England's imports and exports, Florence Nightingale, who used statistical graphics to persuade the British Government to improve army hygiene, John Snow who plotted deaths from cholera in London in 1854 to detect the source of the disease, and Charles Joseph Minard who designed a large portfolio of maps of which the one depicting Napoleon's campaign in Russia is the best known. See the plots page for many more examples of statistical graphics. == See also == Data Presentation Architecture List of graphical methods Visual inspection Chart List of charting software == References == Attribution This article incorporates public domain material from websites or documents of the National Institute of Standards and Technology. == Further reading == W.S. Cleveland (1993). Visualizing Data. Summit, NJ, USA: Hobart Press. ISBN 0-9634884-0-6. W.S. Cleveland (1994). The Elements of Graphing Data. Summit, NJ, USA: Hobart Press. ISBN 0-9634884-1-4. Paul J. Lewi (2006). Speaking of Graphics. Edward R.Tufte (2001) [1983]. The Visual Display of Quantitative Information (2nd ed.). Cheshire, CT, USA: Graphics Press. ISBN 0-9613921-4-2. Edward R. Tufte (1992) [1990]. Envisioning Information. Cheshire, CT, USA: Graphics Press. ISBN 0-9613921-1-8. == External links == Trend Compass Alphabetic gallery of graphical techniques DataScope a website devoted to data visualization and statistical graphics 
